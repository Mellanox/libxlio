Introduction
============

Accelerated IO SW library (XLIO) boosts the performance of applications written over standard socket API such as
web serving, reverse proxying, caching, load balancing, media streaming, and more. Reduction of latency, increasing
throughput and effective CPU utilization is achieved by full network stack bypass and direct access to
accelerated network hardware.
XLIO dynamically links with these applications at run-time, redirect standard socket API calls allowing them to be
be accelerated without modification.

XLIO execution modes
====================
XLIO supports two execution modes.
1 - Run to completion (R2C)
2 - Worker Threads

Run to completion
-----------------
By default, XLIO works in R2C mode, meaning, the execution context is
provided to XLIO by the application. In particular, XLIO code such as reading/writing packets,
polling CQs, handling sockets, etc, is performed as part of POSIX socket calls or Ultra API calls,
on the same thread which called the API.
In terms of performance this mode is the preferred one, however,
this mode requires from the application to work with sockets in an efficient way,
by avoiding sharing sockets between threads, provide enough execution time to XLIO, use per thread epoll and more.
In case of listen sockets, each thread should have its own listen socket.

Worker Threads
--------------
In this mode XLIO spawns XLIO worker threads. These threads run in the background and perform network operations.
This mode requires minimal network awareness from the aplication,
i.e applications may share sockets between threads, use single listening thread,
use single epoll context or rarely call socket APIs.
While R2C mode provides best performance, Worker Threads mode provides greater flexibility.
The number of XLIO worker threads is controlled by the performance.threading.worker_threads parameter.
XLIO Ultra API is not supported with this mode.
Please see User Manual for additional details and current limitations.

Configuration Subsystem
=======================

On default startup the XLIO library logs to stderr the version, the modified
configuration parameters being used and their values.
Please notice that except monitor.log.level, library logs just those parameters whose value != default.

Example:
 XLIO INFO   : ---------------------------------------------------------------------------
 XLIO INFO   : XLIO_VERSION: 1.0.0-0 Development Snapshot built on May 26 2021 17:00:30
 XLIO INFO   : Git: 46d203af1d322799c8de5789ba4fe0955f8d9942
 XLIO INFO   : Cmd Line: uname -r
 XLIO INFO   : Current Time: Wed May 26 17:02:52 2021
 XLIO INFO   : Pid: 31535
 XLIO INFO   : OFED Version: MLNX_OFED_LINUX-5.2-0.4.8.0:
 XLIO DEBUG  : System: 4.18.0-80.el8.x86_64
 XLIO INFO   : Architecture: x86_64
 XLIO INFO   : Node: r-aa-zorro006
 XLIO INFO   : ---------------------------------------------------------------------------
 XLIO INFO   : Log Level                      DEBUG                      [monitor.log.level]
 XLIO DETAILS: Log Details                    0                          [monitor.log.details]
 XLIO DETAILS: Log Colors                     Enabled                    [monitor.log.colors]
 XLIO DETAILS: Log File                                                  [monitor.log.file_path]
 XLIO DETAILS: Stats File                                                [monitor.stats.file_path]
 XLIO DETAILS: Stats shared memory directory  /tmp/xlio                  [monitor.stats.shmem_dir]
 XLIO DETAILS: SERVICE output directory       /tmp/xlio                  [core.daemon.dir]
 XLIO DETAILS: Stats FD Num (max)             0                          [monitor.stats.fd_num]
 XLIO DETAILS: Application ID                 XLIO_DEFAULT_APPLICATION_ID [acceleration_control.app_id]
 XLIO DETAILS: Polling CPU idle usage         Disabled                   [monitor.stats.cpu_usage]
 XLIO DETAILS: SigIntr Ctrl-C Handle          Enabled                    [core.signals.sigint.exit]
 XLIO DETAILS: SegFault Backtrace             Disabled                   [core.signals.sigsegv.backtrace]
 XLIO DETAILS: Print a report                 Disabled                   [monitor.exit_report]
 XLIO DETAILS: Quick start                    Disabled                   [core.quick_init]
 XLIO DETAILS: Ring allocation logic TX       0 (Ring per interface)     [performance.rings.tx.allocation_logic]
 XLIO DETAILS: Ring allocation logic RX       0 (Ring per interface)     [performance.rings.rx.allocation_logic]
 XLIO INFO   : Ring migration ratio TX        -1                         [performance.rings.tx.migration_ratio]
 XLIO DETAILS: Ring migration ratio RX        -1                         [performance.rings.rx.migration_ratio]
 XLIO DETAILS: Ring limit per interface       0 (no limit)               [performance.rings.max_per_interface]
 XLIO DETAILS: Ring On Device Memory TX       0                          [performance.rings.tx.max_on_device_memory]
 XLIO INFO   : TCP max syn rate               0 (no limit)               [network.protocols.tcp.max_syn_rate]
 XLIO DETAILS: Zerocopy Mem Bufs              200000                     [performance.buffers.tx.global_array_size]
 XLIO DETAILS: Zerocopy Cache Threshold       10 GB                      [core.syscall.sendfile_cache_limit]
 XLIO DETAILS: Tx Mem Buf size                0                          [performance.buffers.tx.buf_size]
 XLIO DETAILS: Tx QP WRE                      32768                      [performance.rings.tx.ring_elements_count]
 XLIO DETAILS: Tx QP WRE Batching             64                         [performance.rings.tx.completion_batch_size]
 XLIO DETAILS: Tx Max QP INLINE               204                        [performance.rings.tx.max_inline_size]
 XLIO DETAILS: Tx MC Loopback                 Enabled                    [network.multicast.mc_loopback]
 XLIO DETAILS: Tx non-blocked eagains         Disabled                   [performance.polling.nonblocking_eagain]
 XLIO DETAILS: Tx Prefetch Bytes              256                        [performance.buffers.tx.prefetch_size]
 XLIO DETAILS: Tx Bufs Batch TCP              16                         [performance.rings.tx.tcp_buffer_batch]
 XLIO DETAILS: Tx Segs Batch TCP              64                         [performance.buffers.tcp_segments.socket_batch_size]
 XLIO DETAILS: TCP Send Buffer size           1 MB                       [network.protocols.tcp.wmem]
 XLIO DETAILS: Rx Mem Buf size                0                          [performance.buffers.rx.buf_size]
 XLIO DETAILS: Rx QP WRE                      16000                      [performance.rings.rx.ring_elements_count]
 XLIO DETAILS: Rx QP WRE Batching             1024                       [performance.rings.rx.post_batch_size]
 XLIO DETAILS: Rx Byte Min Limit              65536                      [performance.override_rcvbuf_limit]
 XLIO DETAILS: Rx Poll Loops                  100000                     [performance.polling.blocking_rx_poll_usec]
 XLIO DETAILS: Rx Poll Init Loops             0                          [performance.polling.offload_transition_poll_count]
 XLIO DETAILS: Rx UDP Poll OS Ratio           100                        [performance.polling.rx_kernel_fd_attention_level]
 XLIO DETAILS: HW TS Conversion               3                          [network.timing.ts_conversion]
 XLIO DETAILS: Rx Poll Yield                  Disabled                   [performance.polling.yield_on_poll]
 XLIO DETAILS: Rx Prefetch Bytes              256                        [performance.buffers.rx.prefetch_size]
 XLIO DETAILS: Rx Prefetch Bytes Before Poll  0                          [performance.buffers.rx.prefetch_before_poll]
 XLIO DETAILS: Rx CQ Drain Rate               Disabled                   [performance.completion_queue.rx_drain_rate_nsec]
 XLIO DETAILS: GRO max streams                32                         [performance.max_gro_streams]
 XLIO DETAILS: TCP 2T rules                   Disabled                   [performance.steering_rules.tcp.2t_rules]
 XLIO DETAILS: TCP 3T rules                   Disabled                   [performance.steering_rules.tcp.3t_rules]
 XLIO DETAILS: UDP 3T rules                   Enabled                    [performance.steering_rules.udp.3t_rules]
 XLIO DETAILS: ETH MC L2 only rules           Disabled                   [performance.steering_rules.udp.only_mc_l2_rules]
 XLIO DETAILS: Force Flowtag for MC           Disabled                   [network.multicast.mc_flowtag_acceleration]
 XLIO DETAILS: Select Poll (usec)             100000                     [performance.polling.iomux.poll_usec]
 XLIO DETAILS: Select Poll OS Ratio           10                         [performance.polling.iomux.poll_os_ratio]
 XLIO DETAILS: Select Skip OS                 4                          [performance.polling.iomux.skip_os]
 XLIO DETAILS: CQ Drain Interval (msec)       10                         [performance.completion_queue.periodic_drain_msec]
 XLIO DETAILS: CQ Drain WCE (max)             10000                      [performance.completion_queue.periodic_drain_max_cqes]
 XLIO DETAILS: CQ Interrupts Moderation       Enabled                    [performance.completion_queue.interrupt_moderation.enable]
 XLIO DETAILS: CQ Moderation Count            48                         [performance.completion_queue.interrupt_moderation.packet_count]
 XLIO DETAILS: CQ Moderation Period (usec)    50                         [performance.completion_queue.interrupt_moderation.period_usec]
 XLIO DETAILS: CQ AIM Max Count               560                        [performance.completion_queue.interrupt_moderation.adaptive_count]
 XLIO DETAILS: CQ AIM Max Period (usec)       250                        [performance.completion_queue.interrupt_moderation.adaptive_period_usec]
 XLIO DETAILS: CQ AIM Interval (msec)         250                        [performance.completion_queue.interrupt_moderation.adaptive_change_frequency_msec]
 XLIO DETAILS: CQ AIM Interrupts Rate (per sec) 10000                       [performance.completion_queue.interrupt_moderation.adaptive_interrupt_per_sec]
 XLIO DETAILS: CQ Poll Batch (max)            16                         [performance.polling.max_rx_poll_batch]
 XLIO DETAILS: CQ Keeps QP Full               Enabled                    [performance.completion_queue.keep_full]
 XLIO DETAILS: QP Compensation Level          256                        [performance.rings.rx.spare_buffers]
 XLIO DETAILS: Offloaded Sockets              Enabled                    [acceleration_control.default_acceleration]
 XLIO DETAILS: Timer Resolution (msec)        10                         [performance.threading.internal_handler.timer_msec]
 XLIO DETAILS: TCP Timer Resolution (msec)    100                        [network.protocols.tcp.timer_msec]
 XLIO DETAILS: TCP control thread             Disabled                   [performance.threading.internal_handler.behavior]
 XLIO DETAILS: TCP timestamp option           0                          [network.protocols.tcp.timestamps]
 XLIO DETAILS: TCP nodelay                    0                          [network.protocols.tcp.nodelay.enable]
 XLIO DETAILS: TCP quickack                   0                          [network.protocols.tcp.quickack]
 XLIO DETAILS: Exception handling mode        -1(just log debug message) [core.exception_handling.mode]
 XLIO DETAILS: Avoid sys-calls on tcp fd      Disabled                   [core.syscall.avoid_ctl_syscalls]
 XLIO DETAILS: Allow privileged sock opt      Enabled                    [core.syscall.allow_privileged_sockopt]
 XLIO DETAILS: Delay after join (msec)        0                          [network.multicast.wait_after_join_msec]
 XLIO DETAILS: Internal Thread Affinity       -1                         [performance.threading.cpu_affinity]
 XLIO DETAILS: Internal Thread Cpuset                                    [performance.threading.cpuset]
 XLIO DETAILS: Buffer batching mode           1 (Batch and reclaim buffers) [performance.buffers.batching_mode]
 XLIO DETAILS: Mem Allocation type            Huge pages                 [core.resources.hugepages.enable]
 XLIO DETAILS: Memory limit                   2 GB                       [core.resources.memory_limit]
 XLIO DETAILS: Memory limit (user allocator)  0                          [core.resources.external_memory_limit]
 XLIO DETAILS: Hugepage size                  0                          [core.resources.hugepages.size]
 XLIO DETAILS: Num of UC ARPs                 3                          [network.neighbor.arp.uc_retries]
 XLIO DETAILS: UC ARP delay (msec)            10000                      [network.neighbor.arp.uc_delay_msec]
 XLIO DETAILS: Num of neigh restart retries   1                          [network.neighbor.errors_before_reset]
 XLIO DETAILS: TSO support                    auto                       [hardware_features.tcp.tso.enable]
 XLIO DETAILS: UTLS RX support                Disabled                   [hardware_features.tcp.tls_offload.rx_enable]
 XLIO DETAILS: UTLS TX support                Enabled                    [hardware_features.tcp.tls_offload.tx_enable]
 XLIO DETAILS: LRO support                    auto                       [hardware_features.tcp.lro]
 XLIO DETAILS: Src port stirde                2                          [applications.nginx.src_port_stride]
 XLIO DETAILS: Size of UDP socket pool        0                          [applications.nginx.udp_pool_size]
 XLIO DETAILS: Number of Nginx workers        0                          [applications.nginx.workers_num]
 XLIO DETAILS: fork() support                 Enabled                    [core.syscall.fork_support]
 XLIO DETAILS: close on dup2()                Enabled                    [core.syscall.dup2_close_fd]
 XLIO DETAILS: MTU                            0 (follow actual MTU)      [network.protocols.ip.mtu]
 XLIO DETAILS: MSS                            0 (follow network.protocols.ip.mtu)        [network.protocols.tcp.mss]
 XLIO DETAILS: TCP CC Algorithm               0 (LWIP)                   [network.protocols.tcp.congestion_control]
 XLIO DETAILS: TCP abort on close             Disabled                   [network.protocols.tcp.linger_0]
 XLIO DETAILS: Polling Rx on Tx TCP           Disabled                   [performance.polling.rx_poll_on_tx_tcp]
 XLIO DETAILS: Skip CQ polling in rx          Disabled                   [performance.polling.skip_cq_on_rx]
 XLIO DETAILS: Lock Type                      Spin                       [performance.threading.mutex_over_spinlock]
 XLIO DETAILS: Worker Threads                 0                          [performance.threading.worker_threads]
 XLIO INFO   : ---------------------------------------------------------------------------

Configuration Values
====================
ACCELERATION_CONTROL
--------------------

acceleration_control.app_id
Maps to **XLIO_APPLICATION_ID** environment variable.
Specify a group of rules from libxlio.conf for XLIO to apply.
Example: 'XLIO_APPLICATION_ID=iperf_server'.
Default value is "XLIO_DEFAULT_APPLICATION_ID" (match only the '*' group rule)

acceleration_control.default_acceleration
Maps to **XLIO_OFFLOADED_SOCKETS** environment variable.
Create all sockets as offloaded/not-offloaded by default.
Value of true is for offloaded, false for not-offloaded.
Default value is true

acceleration_control.rules
Maps to configuration in libxlio.conf file.
Rules defining transport protocol and offload settings for
specific applications or processes.
Default value is []


================================================================================

APPLICATIONS
------------

applications.nginx.distribute_cq
Maps to **XLIO_DISTRIBUTE_CQ** environment variable.
Controls whether Completion Queue (CQ) interrupts are distributed across different CPU cores based on worker ID.

**What is a Completion Vector?**
In RDMA, each CQ can be associated with a "completion vector" - an index that maps to a specific MSI-X interrupt line on the NIC. Each MSI-X interrupt is typically serviced by a specific CPU core. The number of available completion vectors (context->num_comp_vectors) usually equals the number of CPU cores or MSI-X vectors supported by the NIC.

**How This Parameter Works:**
When enabled and running as a worker process (not master), CQs are created with:
   comp_vector = worker_id % num_comp_vectors

When disabled (default), all CQs use comp_vector=0, meaning all CQ completion interrupts are handled by the same CPU (typically CPU 0).

**When Does This Matter?**
This parameter affects interrupt-driven I/O paths. When applications use epoll/blocking I/O and wait for CQ completion events (via completion channels), the kernel delivers interrupts to wake up the waiting process. The completion vector determines which CPU handles these interrupts.

In pure polling mode (where applications continuously poll CQs without waiting for interrupts), this parameter has no effect since interrupts are not used.

**Value Tradeoffs:**

*false (disabled, default):*
- All CQ interrupts go to completion vector 0 (same CPU)
- Simple, predictable interrupt handling
- May create CPU hotspotting under high connection rates (CPS)
- CPU 0 becomes a bottleneck for interrupt processing in multi-worker setups
- Best for: Single-worker applications, polling-only workloads, low CPS scenarios

*true (enabled):*
- CQ interrupts distributed across CPUs based on worker ID
- Spreads interrupt processing load across multiple CPU cores
- Improves CPS (Connections Per Second) in multi-process scenarios
- Each worker's CQs use a different completion vector, avoiding interrupt contention
- Reduces CPU hotspotting on the default interrupt handler CPU
- Best for: Multi-worker Nginx/Envoy, interrupt-driven I/O, high CPS workloads

**Profile Behavior:**
This parameter is automatically set to true when using the nginx profile (via applications.nginx.workers_num > 0). The nginx profile assumes multi-process operation where interrupt distribution is beneficial.

**NUMA Considerations:**
For optimal performance with distribute_cq enabled, consider:
- Aligning worker CPU affinity with NIC NUMA node
- Ensuring completion vectors map to CPUs on the same NUMA node as the NIC
- Use irqbalance or manual smp_affinity configuration to control MSI-X to CPU mapping

**Example Scenario:**
With 8 Nginx workers and a NIC with 16 completion vectors:
- Worker 0: CQs use comp_vector=0 (interrupts on CPU 0)
- Worker 1: CQs use comp_vector=1 (interrupts on CPU 1)
- ...
- Worker 7: CQs use comp_vector=7 (interrupts on CPU 7)

This spreads interrupt handling across 8 CPUs instead of concentrating all on CPU 0.

**When to Enable:**
- Running Nginx/Envoy with multiple workers
- High CPS (connections per second) workloads
- Observing CPU hotspotting on a single core during interrupt-heavy operations
- Using interrupt-driven receive paths (epoll waiting on CQ completion channels)

**When to Keep Disabled:**
- Single-worker applications
- Pure polling mode (applications never wait for CQ interrupts)
- Simple deployments where interrupt distribution adds no benefit
- Low connection rate workloads where interrupt overhead is negligible
Default value is false

applications.nginx.src_port_stride
Maps to **XLIO_NGINX_SRC_PORT_STRIDE** environment variable.
Controls how incoming connections are distributed across Nginx worker processes using hardware flow steering rules based on source port patterns.

**How It Works:**
XLIO creates hardware flow steering rules that match incoming packets based on their source port.
Each Nginx worker receives a unique "slot" in the source port space, determined by:
- mask = (workers_pow2 * src_port_stride) - 2
- value = worker_id * src_port_stride

Where workers_pow2 is the number of workers rounded up to the next power of 2.
Packets are routed to a worker when: (packet_src_port & mask) == value

**Why Minimum is 2:**
The minimum value of 2 is critical because the formula subtracts 2 from the mask,
which excludes bit 0 (the least significant bit) from flow steering decisions.
This is essential because client ephemeral source ports have no predictable pattern
in their LSB - including it would cause uneven distribution.

**Example with 4 workers and stride=2 (default):**
- workers_pow2 = 4
- mask = (4 * 2) - 2 = 6 (binary: 110, checking bits 1-2)
- Worker 0: value=0, receives packets where (src_port & 6) == 0
- Worker 1: value=2, receives packets where (src_port & 6) == 2
- Worker 2: value=4, receives packets where (src_port & 6) == 4
- Worker 3: value=6, receives packets where (src_port & 6) == 6

**Low Values (e.g., 2 - default):**
Benefits:
- Supports more workers with the same hardware flow steering capacity
- Compact match values use fewer bits in the 16-bit port mask
- Works well with high worker counts (8, 16, 32+ Nginx workers)
- Lower hardware resource usage per steering rule

Drawbacks:
- Connection distribution depends more heavily on client source port patterns
- Adjacent workers may receive similar traffic volumes if client ports cluster

**High Values (e.g., 4, 8, 16):**
Benefits:
- May provide more uniform distribution for certain client traffic patterns
- Greater separation between worker match values reduces steering conflicts
- Can help when clients generate ports with specific bit patterns

Drawbacks:
- Limits maximum supported workers: (workers_pow2 * stride) must fit reasonably
  in the 16-bit port space to allow meaningful distribution
- Uses more bits in the port mask, potentially overlapping with meaningful port ranges
- Diminishing returns for most real-world traffic patterns

**Non-Power-of-2 Workers:**
When workers_num is not a power of 2 (e.g., 3, 5, 6, 7 workers), XLIO automatically:
1. Rounds up to the next power of 2 for flow steering rules
2. Creates secondary rules for some workers to balance the load
3. Workers with lower IDs receive additional connection slots

For example, with 3 workers (rounded to 4 slots):
- Worker 0: receives 2 slots (primary + secondary rule)
- Worker 1: receives 1 slot
- Worker 2: receives 1 slot

**Recommended Settings:**
- For most deployments: Use default value of 2
- For power-of-2 worker counts (2, 4, 8, 16): Default works optimally
- For high worker counts (32+): Stick with 2 to maximize supported workers
- Only increase if you observe uneven distribution with default settings
  and have analyzed your client source port patterns
Default value is 2

applications.nginx.udp_pool_size
Maps to **XLIO_NGINX_UDP_POOL_SIZE** environment variable.
Maximum number of UDP sockets to keep in a per-worker pool for reuse.

**What This Does:**
When a UDP socket is closed, instead of destroying it and releasing all resources,
XLIO places the socket into a pool. When a new UDP socket is requested,
XLIO first checks if a socket is available in the pool and reuses it,
avoiding the expensive socket creation overhead.

**Socket Lifecycle with Pooling:**
1. On close(): Socket state set to SOCKINFO_DESTROYING, RX buffers dropped,
   socket pushed to pool stack (no close() syscall)
2. On socket(): If pool not empty, pop socket, reset state to SOCKINFO_OPENED,
   return immediately (no socket() syscall, no object construction)
3. Bonus: connect() calls to the same destination are skipped entirely for pooled sockets

**What Gets Saved Per Reuse:**
- socket() system call (kernel FD allocation)
- sockinfo_udp object construction (~400+ bytes + internal structures)
- Internal epoll_create() and epoll_ctl() calls
- Socket statistics allocation
- Ring allocation logic setup

**Value Tradeoffs:**

*Value of 0 (Disabled - Default):*
Standard behavior. Each close() destroys the socket, each socket() creates new.
No additional memory held. Use when UDP socket churn is low.

*Low Values (1-10):*
+ Minimal memory overhead
+ Still provides optimization for short-lived connections
- May exhaust pool quickly under high load, falling back to normal creation
Recommended for: Light UDP load balancing, moderate connection rates

*Medium Values (20-50):*
+ Good balance between memory usage and hit rate
+ Handles typical NGINX worker load patterns well
- Memory for ~20-50 sockinfo objects per worker
Recommended for: Standard NGINX UDP deployments

*High Values (100+):*
+ Near 100% pool hit rate for burst workloads
+ Eliminates syscall overhead almost entirely
- Significant memory consumption per worker
- Each pooled socket holds: kernel FD, sockinfo structure, internal epoll FD
- Total memory per worker: pool_size × ~2-4KB (depending on configuration)
- With N workers: total = N × pool_size × 2-4KB
Recommended for: High-throughput UDP proxying with very high connection churn

**Memory Impact Example:**
With 8 NGINX workers and udp_pool_size=100:
- ~800 pre-allocated sockets system-wide
- ~1.6-3.2 MB of additional memory held
- ~800 kernel file descriptors reserved

**Monitoring:**
Use xlio_stats to monitor pool effectiveness through socket creation/destruction counters.

**Related Parameter:**
Use with applications.nginx.udp_socket_pool_reuse to control RX buffer recycling
thresholds for pooled sockets, further optimizing memory behavior.
Default value is 0

applications.nginx.udp_socket_pool_reuse
Maps to **XLIO_NGINX_UDP_POOL_RX_NUM_BUFFS_REUSE** environment variable.
Controls when RX buffers are reclaimed for sockets in the UDP socket pool.

**What This Does:**
When a socket is marked for the pool (via applications.nginx.udp_pool_size > 0),
this parameter sets the m_rx_num_buffs_reuse threshold - controlling how many
RX buffers accumulate before being returned to the ring's buffer pool.

**How Buffer Reclamation Works:**
1. As packets are received, used RX buffers are held in a local reuse queue
2. When the queue reaches this threshold, buffers are batch-returned to the ring
3. Lower thresholds = more frequent reclamation, less memory held per socket
4. Higher thresholds = less frequent reclamation, more memory held per socket

**Value Tradeoffs:**

*Value of 0 (Default):*
Uses the global default (performance.memory.rx.batch_size, typically 64).
No special behavior for pooled sockets.

*Low Values (16-32):*
+ Lower memory footprint per pooled socket
+ Faster buffer recycling when socket enters pool
- More frequent lock acquisitions for buffer reclamation
- Slightly higher CPU overhead per packet
Recommended for: Memory-constrained environments, many pooled sockets

*Higher Values (128-256+):*
+ Fewer lock acquisitions for buffer reclamation
+ Better batching efficiency
- More memory held per socket while active
- Longer delay before buffers return to global pool
Recommended for: High-throughput scenarios, fewer pooled sockets

**Interaction with udp_pool_size:**
This parameter only takes effect when applications.nginx.udp_pool_size > 0.
The combination affects total memory footprint:
- Total pooled buffer memory ≈ udp_pool_size × udp_socket_pool_reuse × buffer_size

**Example:**
With udp_pool_size=50 and udp_socket_pool_reuse=64:
- Up to 50 × 64 = 3200 RX buffers may be held across pooled sockets
- At ~2KB per buffer, this is ~6.4MB per worker
Default value is 0

applications.nginx.workers_num
Maps to **XLIO_NGINX_WORKERS_NUM** environment variable.
Number of Nginx worker processes to optimize for.
**This parameter must be set to a value > 0 to enable XLIO offloading for Nginx.**

**How It Works:**
When workers_num > 0, XLIO:
1. Enables the nginx profile with optimized settings (ring per interface, TSO, 3-tuple rules, etc.)
2. Creates hardware flow steering rules to distribute incoming connections across workers
3. Allocates memory resources scaled to the number of workers
4. Assigns each worker a unique ID (0 to workers_num-1) for flow steering

**Flow Steering Mechanism:**
Each Nginx worker receives its own hardware flow steering rule based on source port masking.
The formula uses:
- workers_pow2 = workers_num rounded up to the next power of 2
- mask = (workers_pow2 * src_port_stride) - 2
- value = worker_id * src_port_stride

Incoming packets are routed to a worker when: (packet_src_port & mask) == value

**Value = 0 (Disabled - Default):**
- XLIO does NOT offload Nginx traffic
- Nginx uses standard kernel networking
- No hardware flow steering rules are created
- Use this when running non-Nginx applications or when XLIO offloading is not desired

**Low Values (1-4 workers):**
- Suitable for light workloads or testing
- Each worker gets dedicated flow steering resources
- Minimal memory footprint
- 4GB per-worker memory allocation baseline
- Best with power-of-2 values (1, 2, 4) for optimal flow steering efficiency

**Medium Values (4-16 workers):**
- Optimal for most production deployments
- Good balance between parallelism and resource utilization
- 4GB per-worker memory allocation baseline
- Power-of-2 values (4, 8, 16) provide the most efficient flow steering
  because hardware masks naturally align with worker IDs

**High Values (17-32+ workers):**
- For high-throughput, high-concurrency workloads
- Memory baseline reduced to 3GB per worker (to manage total memory consumption)
- More hardware flow steering rules are created
- Non-power-of-2 values create additional secondary steering rules to balance load
- Higher memory and hardware resource consumption

**Power-of-2 vs Non-Power-of-2 Values:**

Power-of-2 values (2, 4, 8, 16, 32, ...) are most efficient because:
- All workers receive exactly equal traffic distribution
- No secondary steering rules are needed
- Simpler hardware flow table usage

Non-power-of-2 values (3, 5, 6, 7, 9, ...) work but have overhead:
- XLIO rounds up to the next power of 2 (workers_pow2) for mask calculation
- Some workers receive secondary rules to capture "orphan" traffic slots
- Workers with lower IDs (ID < workers_pow2 % workers_num) get additional steering rules
- Example with 3 workers (workers_pow2=4):
  - Worker 0: handles 2 port slots (primary + secondary rule)
  - Worker 1: handles 1 port slot
  - Worker 2: handles 1 port slot
  This creates slightly uneven distribution until traffic volume normalizes it.

**Memory Allocation:**
Total memory_limit calculation:
- workers_num <= 16: 4GB × workers_num total (4GB per worker base)
- workers_num > 16: 3GB × workers_num total (3GB per worker base)

The total is then divided among workers, so each Nginx worker process receives:
memory_limit / workers_num

The master Nginx process (before forking workers) uses minimal memory.

**Performance Tradeoffs Summary:**

| workers_num | Flow Steering | Memory/Worker | Best For |
|-------------|---------------|---------------|----------|
| 0           | None (kernel) | N/A           | Non-Nginx apps |
| 1-4         | Simple        | 4GB base      | Light workloads, testing |
| 4-16        | Optimal       | 4GB base      | Production deployments |
| 17-32       | Complex       | 3GB base      | High-throughput servers |
| 33+         | Very Complex  | 3GB base      | Extreme scale (use power-of-2) |

**Recommendations:**
- Match workers_num exactly to your Nginx worker_processes configuration
- Prefer power-of-2 values when possible for optimal hardware efficiency
- For most deployments, 4-16 workers provides the best balance
- Monitor xlio_stats to verify even distribution across workers
- When in doubt, start with a power-of-2 value and benchmark
Default value is 0


================================================================================

CORE
----

core.daemon.dir
Maps to **XLIO_SERVICE_NOTIFY_DIR** environment variable.
Set the directory path for XLIO to write files used by xliod.
Note: when used xliod must be run with --notify-dir directing the same folder.
Default value is /tmp/xlio

core.daemon.enable
Maps to **XLIO_SERVICE_ENABLE** environment variable.
Enable the XLIO daemon service for additional monitoring capabilities.
Default value is false

core.exception_handling.mode
Maps to **XLIO_EXCEPTION_HANDLING** environment variable.
Mode for handling missing support or error cases in Socket API or functionality by XLIO.
Useful for quickly identifying XLIO unsupported Socket API or features.
Use:
   - "exit" or -2 - to exit() on XLIO startup failure.
   - "handle_debug" or -1 - for handling at DEBUG severity.
   - "log_debug_undo_offload" or 0 - to log DEBUG message and
      try recovering via Kernel network stack (un-offloading the socket).
   - "log_error_undo_offload" or 1 - to log ERROR message and
      try recovering via Kernel network stack (un-offloading the socket).
   - "log_error_return_error" or 2 - to log ERROR message and
      return API respectful error code.
   - "log_error_abort" or 3 - to log ERROR message and
      abort application (throw xlio_error exception).
Default value is -1

core.quick_init
Maps to **XLIO_QUICK_START** environment variable.
Avoid expensive extra checks to reduce the initialization time.
This may result in failures in case of a system misconfiguration.
For example, if the parameter is enabled and hugepages are requested
beyond the cgroup limit, XLIO crashes due to an access to an unmapped page.
Default value is false

core.resources.external_memory_limit
Maps to **XLIO_MEMORY_LIMIT_USER** environment variable.
Memory limit for external user allocator.
The user allocator can optionally be provided with XLIO extra API.
0 makes XLIO use the core.resources.memory_limit value for user allocations.
Supports suffixes: B, KB, MB, GB.
Default value is 0

core.resources.heap_metadata_block_size
Maps to **XLIO_HEAP_METADATA_BLOCK** environment variable.
Size of metadata block added to every heap allocation.
Supports suffixes: B, KB, MB, GB.
Default value is 32MB

core.resources.hugepages.enable
Maps to **XLIO_MEM_ALLOC_TYPE** environment variable.
Use huge pages for data buffers when available to improve performance
by reducing TLB misses.
XLIO will try to allocate data buffers as configured:
when false, using malloc.
when true, using huge pages.
XLIO also overrides accordingly these rdma-core parameters:
MLX_QP_ALLOC_TYPE and MLX_CQ_ALLOC_TYPE.
Default value is true

core.resources.hugepages.size
Maps to **XLIO_HUGEPAGE_SIZE** environment variable.
Force specific hugepage size for XLIO internal memory allocations.
0 allows to use any supported and available hugepages.
Must be a power of 2, or 0.
The size may be specified with suffixes such as KB, MB, GB.
Supports suffixes: B, KB, MB, GB.
Default value is 0

core.resources.memory_limit
Maps to **XLIO_MEMORY_LIMIT** environment variable.
Pre-allocated memory limit for buffers.
Note that the limit does not include dynamic memory allocation
and XLIO memory consumption can exceed the limit.
0 means unlimited memory allocation.
Supports suffixes: B, KB, MB, GB.
Default value is 2GB

core.signals.sigint.exit
Maps to **XLIO_HANDLE_SIGINTR** environment variable.
When enabled, the library handler will be called when interrupt signal
is sent to the process.
XLIO will also call the application handler if it exists.
Default value is true

core.signals.sigsegv.backtrace
Maps to **XLIO_HANDLE_SIGSEGV** environment variable.
When enabled, print backtrace if segmentation fault happens.
Default value is false

core.syscall.allow_privileged_sockopt
Maps to **XLIO_ALLOW_PRIVILEGED_SOCK_OPT** environment variable.
Permit the use of privileged socket options that might require special permissions.
Default value is true

core.syscall.avoid_ctl_syscalls
Maps to **XLIO_AVOID_SYS_CALLS_ON_TCP_FD** environment variable.
For TCP fd, avoid system calls for the supported options of:
ioctl, fcntl, getsockopt, setsockopt.
Non-supported options will go to OS.
Default value is false

core.syscall.deferred_close
Maps to **XLIO_DEFERRED_CLOSE** environment variable.
Defers closing of file descriptors until the socket is actually closed,
useful for multi-threaded applications.
Default value is false

core.syscall.dup2_close_fd
Maps to XLIO_CLOSE_ON_DUP2 environment variable.
When this parameter is enabled, XLIO will handle the duplicate fd (oldfd)
as if it was closed (clear internal data structures) and only then,
will forward the call to the OS.
This is, in practice, a very rudimentary dup2 support.
It only supports the case where dup2 is used to close file descriptors.
Default value is true

core.syscall.fork_support
Maps to **XLIO_FORK** environment variable.
Control whether XLIO should support fork.
Setting this flag on will cause XLIO to call ibv_fork_init() function.
ibv_fork_init() initializes libibverbs data structures to handle fork()
function calls correctly and avoid data corruption.
If ibv_fork_init() is not called or returns a non-zero status, then libibverbs
data structures are not fork()-safe and
the effect of an application calling fork() is undefined.
Default value is true

core.syscall.sendfile_cache_limit
Maps to **XLIO_ZC_CACHE_THRESHOLD** environment variable.
Memory limit for the mapping cache which is used by sendfile().
Supports suffixes: B, KB, MB, GB.
Default value is 10GB


================================================================================

HARDWARE_FEATURES
-----------------

hardware_features.striding_rq.enable
Maps to **XLIO_STRQ** environment variable.
Enable/Disable Striding Receive Queues (STRQ).

Striding RQ vs Regular RQ:
With regular RQ, each Work Queue Element (WQE) receives exactly one packet.
With Striding RQ, each WQE is divided into multiple strides (slots),
allowing a single WQE to receive many packets. This significantly reduces
WQE replenishment overhead under high packet rates.

Benefits of Striding RQ (enabled):
• Higher packet rates with lower CPU overhead for WQE management.
• Better batching - fewer doorbell writes to the NIC.
• Dynamic stride allocation from an expandable pool.
• Recommended for most workloads, especially high packet-rate scenarios.

When to disable:
• Legacy compatibility requirements.
• Specific debugging or troubleshooting scenarios.
• Environments where Striding RQ is not supported by hardware.

WQE buffer size = strides_num × stride_size.
With defaults (2048 × 64 bytes), each WQE can hold up to 2048 small packets
or fewer large packets that span multiple strides.
Default value is true

hardware_features.striding_rq.stride_size
Maps to **XLIO_STRQ_STRIDE_SIZE_BYTES** environment variable.
The size, in bytes, of each stride in a receive WQE.
Must be power of two and in range [64 - 8192].

Packets larger than stride_size span multiple strides.
For example, a 1500-byte packet with stride_size=64 uses 24 strides.

Lower values (e.g., 64 - default):
• Optimal memory efficiency for small packets (typical UDP, TCP ACKs).
• Minimal internal fragmentation - less wasted space per packet.
• More packets fit within each WQE.
• Large packets (jumbo frames, LRO segments) consume many strides,
  increasing metadata overhead per packet.

Higher values (e.g., 2048-8192):
• Better suited for jumbo frames (MTU 9000) or LRO-aggregated segments.
• Fewer strides per large packet reduces per-packet overhead.
• Fewer stride descriptor objects are allocated.
• Significant memory waste when receiving small packets.
  A 64-byte packet in an 8192-byte stride wastes 99% of the stride.

Recommendations:
• For typical mixed traffic with MTU 1500: use default (64 bytes).
• For jumbo frames (MTU 9000): consider 256-512 bytes.
• For NVMe-oF or storage workloads with large transfers: consider 2048-8192 bytes.

Note: strides_num × stride_size must be at least MTU + 18 bytes (Ethernet + VLAN headers).
Default value is 64

hardware_features.striding_rq.strides_num
Maps to **XLIO_STRQ_NUM_STRIDES** environment variable.
The number of strides (packet slots) in each receive Work Queue Element (WQE).
Must be power of two and in range [512 - 65536].

Memory Impact:
WQE buffer size = strides_num × stride_size.
With default values (2048 × 64 bytes), each WQE uses 128 KB.
Total RX memory ≈ strides_num × stride_size × rx_num_wr.

Lower values (e.g., 512):
• Smaller WQE buffers reduce memory footprint per ring.
• Faster WQE buffer recycling since fewer packets need to complete.
• Fewer packets per WQE means more frequent WQE replenishment,
  which may become a bottleneck under very high packet rates.

Higher values (e.g., 65536):
• More packets can be received per WQE, reducing WQE management overhead.
• Better for high packet-rate workloads (millions of packets/sec).
• Larger WQE buffers increase memory consumption.
• WQE buffer is held until all strides are consumed, potentially
  increasing memory pressure under bursty traffic.

Constraint: strides_num × rx_num_wr must not exceed 4,194,304 (CQE limit).
If exceeded, rx_num_wr is automatically reduced.
Default value is 2048

hardware_features.tcp.lro
Maps to **XLIO_LRO** environment variable.
Large Receive Offload (LRO) is a hardware technique for increasing inbound TCP throughput by reducing CPU overhead. The NIC aggregates multiple incoming TCP segments from the same stream into a single larger buffer before delivering it to the application, reducing the number of packets that must be processed.

Values:
   - "auto" or -1 (default)
      Automatically enabled based on ethtool setting and adapter capability.
      Check with: ethtool -k <interface> | grep large-receive-offload
   - "disable" or 0
      LRO is disabled. Each TCP segment is delivered individually.
   - "enable" or 1
      Force LRO enabled if the adapter supports it.

Performance Tradeoffs:

Enabled (higher throughput, lower CPU):
• Multiple TCP segments are coalesced into larger chunks (up to 64KB).
• Dramatically reduces per-packet CPU overhead for bulk data transfers.
• Ideal for high-bandwidth workloads: file transfers, backup, streaming, NVMe-oF.
• Can achieve near line-rate throughput with lower CPU utilization.
• Coalesced segments span multiple strides when using Striding RQ.
• Memory efficiency: fewer buffer descriptors and less metadata per byte transferred.

Disabled (lower latency, per-packet processing):
• Each TCP segment is delivered immediately without waiting for aggregation.
• Lower latency - critical for latency-sensitive applications.
• Required when applications need per-packet timestamps or ordering guarantees.
• Better for interactive protocols, trading, or real-time communication.
• Higher CPU overhead at high packet rates (millions of pps).
• Each packet consumes at least one stride in Striding RQ mode.

Interaction with Striding RQ:
• LRO max payload size = min(strides_num × stride_size, 64KB), rounded to 256 bytes.
• With defaults (2048 × 64 = 128KB), LRO can coalesce up to 64KB segments.
• Lower stride configurations may limit LRO aggregation size.
• LRO-aggregated packets consume multiple strides proportional to their size.
• For maximum LRO benefit, ensure strides_num × stride_size >= 64KB.

Recommendations:
• Bulk data workloads (file/object storage, backup): enable LRO.
• Latency-sensitive workloads (trading, real-time): disable LRO.
• Mixed workloads: use auto and let the system decide.
• NVMe-oF with large I/O: enable LRO with larger stride_size (2048+).
Default value is -1

hardware_features.tcp.tls_offload.dek_cache_max_size
Maps to **XLIO_HIGH_WMARK_DEK_CACHE_SIZE** environment variable.
Maximum number of Data Encryption Key (DEK) objects to cache for TLS offload.

HOW DEK CACHING WORKS:
DEK objects contain TLS encryption keys loaded into the NIC's crypto engine.
Creating new DEKs requires NIC firmware operations which have latency cost.
XLIO maintains a cache of reusable DEK objects to amortize this cost.

When a TLS connection closes, its DEK is returned to the 'put cache'.
When a new TLS connection needs a DEK, XLIO first tries the 'get cache'.
If the get cache is empty, a crypto-sync operation swaps the caches.
This high watermark limits the maximum put cache size.

HIGH VALUES (2048+):
   + Better performance with many short-lived TLS connections.
   + Fewer crypto-sync operations (less NIC firmware latency).
   + Smoother latency distribution for connection setup.
   - Higher NIC memory consumption for DEK storage.
   - May exhaust system DEK resources with very high values.

LOW VALUES (256-512):
   + Lower NIC resource consumption.
   + Suitable for long-lived connections (fewer DEK recycles).
   - More frequent crypto-sync operations with many connections.
   - Higher latency variance for connection setup.

RECOMMENDATIONS:
   - High connection churn (HTTP/1.1, many short requests): 2048-4096
   - Moderate connection count with reuse (HTTP/2, gRPC): 1024 (default)
   - Few long-lived connections: 256-512
   - Very high connection count (10K+): Consider 4096+

NOTE: This parameter only affects TLS hardware offload. Has no effect
when tx_enable and rx_enable are both false.
Default value is 1024

hardware_features.tcp.tls_offload.dek_cache_min_size
Maps to **XLIO_LOW_WMARK_DEK_CACHE_SIZE** environment variable.
Minimum DEK cache threshold before creating new DEK objects.

HOW IT WORKS:
When the available DEK cache (get cache) is empty and the put cache size
is below this low watermark, XLIO creates new DEK objects instead of
performing a crypto-sync operation to recycle existing DEKs.

This avoids throttling behavior where a single DEK is returned and
immediately fetched, causing frequent crypto-sync operations.

HIGH VALUES (relative to max_size):
   + More aggressive new DEK creation.
   + Fewer crypto-sync operations.
   + Better latency for connection setup bursts.
   - More total DEK objects created over time.

LOW VALUES:
   + More DEK reuse via crypto-sync.
   + Lower total DEK object count.
   - More frequent crypto-sync operations.
   - Higher latency variance during cache transitions.

CONSTRAINT:
Must be less than dek_cache_max_size. If set >= max_size, XLIO
automatically adjusts to max_size / 2.

RECOMMENDATION:
Keep at ~50% of dek_cache_max_size (default ratio). Adjust max_size
first; min_size rarely needs manual tuning.
Default value is 512

hardware_features.tcp.tls_offload.rx_enable
Maps to **XLIO_UTLS_RX** environment variable.
Controls whether TLS decryption is offloaded to the NIC hardware or performed by the CPU.

HOW IT WORKS:
When enabled, XLIO leverages the Linux kTLS API to offload TLS decryption to
NVIDIA ConnectX NICs. The NIC's crypto engine performs AES-GCM decryption
directly on incoming packets. XLIO manages TIR (Transport Interface Receive)
contexts and handles resync scenarios when packets arrive out of order.

IMPORTANT: RX offload is more complex than TX due to packet reordering.
The NIC must handle out-of-order packets which may require software fallback
for authentication/decryption of certain records.

ENABLED (true) - HARDWARE TLS RX OFFLOAD:
   + Lower CPU utilization - NIC crypto engine handles decryption.
   + Higher receive throughput ceiling (under ideal conditions).
   + Lower receive latency - fewer CPU cycles in the data path.
   + Better scalability for receive-heavy TLS workloads (web servers).
   - Requires compatible NIC (ConnectX-6 DX or later with TLS RX offload).
   - Requires TLS library with kTLS RX support.
   - Consumes NIC resources (TIR contexts, DEK entries).
   - Resync overhead can degrade throughput under load (see below).
   - More complex failure modes than TX offload.

DISABLED (false) - SOFTWARE TLS DECRYPTION:
   + Works on any hardware without TLS offload capability.
   + Simpler packet handling - no hardware state to manage.
   + Easier debugging - all decryption visible in CPU profiling.
   + No NIC resource consumption for RX TLS.
   + More predictable throughput under high load or packet loss.
   + No resync contention with high connection counts.
   - Higher CPU utilization - CPU performs all AES-GCM decryption.
   - Lower receive throughput ceiling.
   - Higher receive latency.

CRITICAL: RESYNC OVERHEAD AND THROUGHPUT DEGRADATION

RX TLS offload can experience significant throughput degradation under
certain conditions due to expensive resync operations:

1. WHAT TRIGGERS RESYNC:
   - Out-of-order packet delivery
   - TCP retransmissions
   - Packet loss and recovery
   - Network congestion causing reordering

2. WHY RESYNC IS EXPENSIVE:
   - Resync requires Send Queue (SQ) credits from the TX path
   - Each resync posts UMR WQEs to reprogram NIC TLS state
   - Resyncs compete with TX traffic for SQ resources

3. UNDER HIGH LOAD:
   - When SQ is congested, resyncs are SKIPPED and retried later
   - Skipped resyncs cause packets to fall back to software decryption
   - Software fallback (OpenSSL EVP_Decrypt) is CPU-intensive
   - This creates a negative feedback loop under load

4. HIGH CONNECTION COUNT AMPLIFIES THE PROBLEM:
   - Each connection can independently trigger resyncs
   - All connections share limited SQ credits
   - More connections = more resync contention = more SW fallback
   - Throughput can degrade significantly vs software-only path

RESYNC SCENARIOS (tracked in statistics):
   - full_enc: Entire record encrypted, needs full SW decrypt
   - head_enc: First buffers encrypted, partial SW decrypt
   - tail_enc: Tail encrypted, needs SW re-encrypt + decrypt
   - mix_enc: Mixed state, most expensive SW processing

PERFORMANCE IMPACT:
   - IDEAL CONDITIONS (low loss, few connections, light TX load):
     Significant CPU savings, higher throughput.
   - HIGH LOAD / MANY CONNECTIONS / PACKET LOSS:
     May perform WORSE than software decryption due to resync overhead
     and SQ contention. Throughput can drop 20-50% in extreme cases.

WHY DEFAULT IS FALSE:
RX offload is disabled by default because:
   1. Resync overhead can cause unexpected throughput degradation.
   2. Performance depends heavily on network conditions and load.
   3. TX offload alone provides significant benefit for most servers.
   4. Software decryption has more predictable performance.
   5. Some TLS libraries have limited kTLS RX support.

WHEN TO ENABLE:
   - Low packet loss, stable network environment.
   - Moderate connection count (not thousands of connections).
   - High-throughput receive workloads with large payloads.
   - TX path has spare SQ capacity (not heavily loaded).
   - Testing confirms benefit for your specific workload.

WHEN TO KEEP DISABLED:
   - High connection count (1000+ TLS connections).
   - Lossy network with frequent retransmissions.
   - Heavy bidirectional traffic (TX competes for SQ credits).
   - Workload shows high resync counts in XLIO statistics.
   - Throughput testing shows degradation vs software path.

MONITORING:
Use XLIO statistics to monitor RX TLS health:
   - n_tls_rx_resync: Total resync operations (should be low)
   - n_tls_rx_records_full_enc: Records fully SW-decrypted
   - n_tls_rx_records_*_enc: Partial SW decryption events
   - n_rx_tls_auth_fail: Authentication failures
High values indicate resync pressure; consider disabling RX offload.

PREREQUISITES:
   - NVIDIA ConnectX-6 DX, ConnectX-7, or BlueField-2+ with TLS RX offload.
   - TLS library compiled with kTLS RX support.
   - Application must use kTLS-compatible TLS API.
   - XLIO built with --enable-utls configure option.

INTERACTION WITH OTHER PARAMETERS:
   - hardware_features.tcp.tls_offload.tx_enable: TX and RX offload share
     SQ resources. Heavy TX load reduces RX resync capacity.
   - hardware_features.tcp.tls_offload.dek_cache_max_size: Shared DEK cache
     for both TX and RX operations.

RECOMMENDATION:
Start with TX offload only (default). Enable RX offload only after:
   1. Confirming low packet loss in your environment.
   2. Testing with realistic connection counts.
   3. Monitoring resync statistics under production load.
   4. Verifying throughput improvement vs software path.
Default value is false

hardware_features.tcp.tls_offload.tx_enable
Maps to **XLIO_UTLS_TX** environment variable.
Controls whether TLS encryption is offloaded to the NIC hardware or performed by the CPU.

HOW IT WORKS:
When enabled, XLIO leverages the Linux kTLS API to offload TLS encryption to
NVIDIA ConnectX NICs. The NIC's dedicated crypto engine performs AES-GCM
encryption directly on outgoing packets, freeing the CPU from encryption work.
XLIO manages hardware resources including TIS (Transport Interface Send)
contexts and DEK (Data Encryption Key) caches on the NIC.

ENABLED (true) - HARDWARE TLS OFFLOAD:
   + Lower CPU utilization - NIC crypto engine handles encryption.
   + Higher throughput ceiling - dedicated hardware accelerator.
   + Lower latency - fewer CPU cycles in the data path.
   + Better scalability - CPU freed for application logic.
   + Zero-copy friendly - encrypted data sent directly without extra copies.
   - Requires compatible NIC (ConnectX-6 DX or later with TLS offload).
   - Requires TLS library with kTLS support (OpenSSL 3.0+, or patched versions).
   - Consumes NIC resources (DEK cache entries, TIS contexts).
   - Resync overhead when TCP retransmissions occur (NIC must replay state).

DISABLED (false) - SOFTWARE TLS ENCRYPTION:
   + Works on any hardware without TLS offload capability.
   + No NIC resource consumption (DEK cache, TIS contexts freed).
   + Simpler debugging - all encryption visible in CPU profiling.
   + Useful when NIC resources are scarce with many TLS connections.
   - Higher CPU utilization - CPU performs all AES-GCM encryption.
   - Lower throughput ceiling - CPU becomes the encryption bottleneck.
   - Higher latency - additional CPU cycles per packet.
   - More memory bandwidth - data copied through CPU for encryption.

PERFORMANCE IMPACT:
   - With hardware offload: Expect 2-5x lower CPU usage for TLS workloads
     and higher sustainable throughput (100Gbps+ achievable).
   - Without hardware offload: CPU encryption at ~10-20 Gbps per core
     (cipher-dependent), limiting total throughput.

WHEN TO DISABLE:
   - Hardware doesn't support TLS offload (pre-ConnectX-6 DX NICs).
   - TLS library lacks kTLS support.
   - Debugging TLS-related issues (software path is easier to trace).
   - Very high connection count exhausting NIC DEK cache.

PREREQUISITES:
   - NVIDIA ConnectX-6 DX, ConnectX-7, or BlueField-2+ with TLS offload.
   - TLS library compiled with kTLS support.
   - Application must use kTLS-compatible TLS API (setsockopt SOL_TLS).
   - XLIO built with --enable-utls configure option.

INTERACTION WITH OTHER PARAMETERS:
   - hardware_features.tcp.tls_offload.dek_cache_max_size: Controls DEK cache
     size. Increase for high connection counts with frequent key rotations.
   - hardware_features.tcp.tls_offload.rx_enable: Independent control for
     receive-side TLS offload.

DEFAULT RECOMMENDATION:
Keep enabled (true) for best performance on supported hardware. XLIO
automatically falls back to software encryption if hardware is unavailable.
Default value is true

hardware_features.tcp.tso.enable
Maps to **XLIO_TSO** environment variable.
TCP Segmentation Offload (TSO), also known as Large Send Offload (LSO),
allows the TCP stack to pass buffers larger than the MTU to the network
adapter. The NIC hardware then segments these large buffers into
MTU-sized packets, calculating TCP sequence numbers, checksums, and
headers for each segment.

HOW IT WORKS:
Without TSO, the CPU must create individual TCP segments (typically
1460 bytes each for standard Ethernet). With TSO enabled, the application
can submit much larger buffers (up to hardware_features.tcp.tso.max_size),
and the NIC handles segmentation in hardware.

VALUE OPTIONS:
   - "auto" or -1 (default):
      Automatically enables TSO based on:
      1. Adapter hardware capability (queried via device attributes)
      2. ethtool settings: ethtool -k <interface> | grep tcp-segmentation-offload
      Recommended for most deployments.
   - "disable" or 0:
      Forces TSO off regardless of hardware capability.
   - "enable" or 1:
      Forces TSO on if the adapter supports it.
      Fails gracefully if hardware lacks TSO capability.

PERFORMANCE BENEFITS OF TSO (ENABLED):
   + Dramatically higher throughput - can achieve line-rate on 100Gbps+
     links by reducing per-packet processing overhead.
   + Significant CPU offload - segmentation work moves from CPU to NIC,
     freeing CPU cycles for application processing.
   + Fewer system calls per byte transferred - one send() can transmit
     hundreds of KB instead of individual 1460-byte segments.
   + Reduced memory bandwidth - fewer packet headers to construct.
   + Better cache utilization - less per-packet metadata.
   + Lower interrupt rate - fewer completion events per byte transferred.

WHEN TO DISABLE TSO:
   - Debugging network issues (TSO can mask certain problems).
   - When using network monitoring tools that need to see actual packets.
   - Extremely latency-sensitive applications where even tiny buffering
     adds unacceptable delay (though this is rare - TSO latency impact
     is typically negligible).
   - Compatibility with certain virtualization or tunneling setups that
     don't properly handle TSO packets.

CRITICAL INTERACTION - CONGESTION WINDOW:
TSO segment aggregation is limited by the TCP congestion window (cwnd):
   - TSO max_payload = MIN(hardware_features.tcp.tso.max_size,
                           available_window_space)
   - If cwnd is small (e.g., during slow start), TSO cannot aggregate
     large segments regardless of max_size setting.
   - This is normal TCP behavior - TSO respects congestion control.

RELATED PARAMETERS:
   - hardware_features.tcp.tso.max_size: Caps aggregated segment size.
   - network.protocols.tcp.wmem: TCP send buffer affects TSO efficiency.
   - network.protocols.tcp.mss: Segment granularity for TSO aggregation.

RECOMMENDATIONS:
   - High-throughput bulk transfers: Enable (default "auto" is fine).
   - Data center / cloud workloads: Enable for efficiency.
   - Most production deployments: Use "auto" (default).
   - Network debugging: Temporarily disable.

VERIFICATION:
Check if TSO is active: ethtool -k <interface> | grep tcp-segmentation-offload
XLIO logs TSO status at startup: "ring attributes: m_tso = ..."
Default value is -1

hardware_features.tcp.tso.max_size
Maps to **XLIO_MAX_TSO_SIZE** environment variable.
Maximum size in bytes of data that can be aggregated into a single TSO
"super-packet" before being handed to the NIC for hardware segmentation.

HOW MAX_SIZE AFFECTS TSO:
When the application sends data, XLIO aggregates multiple TCP segments
into a single large buffer (up to max_size bytes) and submits it to the
NIC as one work request. The NIC then segments this into individual
MTU-sized packets for transmission.

Effective TSO payload = MIN(max_size, hardware_capability, congestion_window)

COMMON VALUES:
   - 64KB (65536):   Conservative, lower memory per connection.
   - 256KB (262144): Default, balanced throughput and resource usage.
   - 512KB-1MB:      Aggressive, maximum throughput optimization.

HIGH VALUES (512KB - 1MB) - THROUGHPUT-OPTIMIZED:
   + Maximum throughput potential - fewer NIC work requests per byte,
     achieving line-rate on 100Gbps+ links more easily.
   + Lowest CPU overhead per byte - amortizes send() syscall and
     work request posting costs over more data.
   + Fewest interrupts - one completion event for larger data chunks.
   + Best efficiency for bulk/streaming workloads (file transfer,
     backup, video streaming, database replication).
   - Higher per-connection memory consumption - larger aggregation
     buffers are allocated.
   - Increased head-of-line blocking potential - if one large TSO
     segment is delayed, all data within it waits.
   - May interact poorly with very small congestion windows during
     slow start phase (TSO aggregation capped by cwnd).
   - Bursty traffic pattern - sending large chunks can cause
     micro-bursts that may trigger switch buffer drops.

LOW VALUES (32KB - 64KB) - LATENCY-OPTIMIZED:
   + Lower per-connection memory footprint - important for servers
     with thousands of concurrent connections.
   + More granular congestion control response - smaller chunks
     react faster to network feedback.
   + Reduced head-of-line blocking - data sent in smaller units.
   + Smoother traffic pattern - less bursty, potentially better for
     shared network environments.
   + Faster first-byte latency for new data - less buffering before
     transmission.
   - Lower peak throughput - more NIC work requests needed per byte.
   - Higher CPU overhead - more frequent send operations.
   - May not fully utilize available bandwidth on very high-speed
     links (100Gbps+) where aggregation efficiency matters most.

VERY LOW VALUES (<32KB) - SPECIAL CASES:
   - Generally not recommended for bulk transfers.
   - May be useful for latency-critical applications with small messages.
   - Significantly increases CPU overhead for high-throughput workloads.
   - Consider whether TSO provides benefit at all for such small sizes.

HARDWARE CONSTRAINTS:
The actual TSO payload size is capped by the NIC's hardware capability:
   - Queried from device attributes (xlio_ibv_tso_caps.max_tso).
   - If max_size exceeds hardware capability, the hardware limit wins.
   - XLIO logs a warning if hardware cap exceeds default 256KB,
     suggesting you increase max_size to utilize full NIC potential.

CONGESTION WINDOW INTERACTION:
TSO segment aggregation is dynamically limited by TCP congestion control:
   - Actual aggregation = MIN(max_size, available_cwnd_space)
   - During slow start (small cwnd), TSO may not reach max_size.
   - This is correct behavior - TSO respects congestion control.
   - As cwnd grows, TSO aggregation automatically increases.

INTERACTION WITH OTHER PARAMETERS:
   - network.protocols.tcp.wmem: Send buffer should be >= max_size
     for optimal TSO efficiency. If wmem < max_size, TSO is limited
     by available buffer space.
   - network.protocols.tcp.mss: TSO aggregates MSS-sized segments.
     Higher MSS = fewer segments per TSO super-packet.
   - performance.buffers.tx_buf_size: Per-buffer memory allocation;
     TSO buffer size is MIN(tx_buf_size, max_size).

MEMORY IMPACT:
Larger max_size increases memory usage approximately proportional to:
   - Number of concurrent TCP connections
   - Amount of data in flight per connection
   - Send buffer (wmem) sizing

For high-connection-count servers (10K+ connections), consider lower
max_size (64KB-128KB) to reduce total memory footprint.

RECOMMENDATIONS:
   - Bulk transfer / streaming (few connections, max throughput):
     512KB - 1MB
   - Balanced workloads (moderate connections, good throughput):
     256KB (default)
   - High connection count servers (10K+ connections):
     64KB - 128KB
   - Latency-sensitive applications: 32KB - 64KB
   - Memory-constrained environments: 64KB

TUNING APPROACH:
1. Start with default 256KB.
2. Monitor throughput with iperf3 or application benchmarks.
3. If throughput is below line-rate, try increasing to 512KB or 1MB.
4. If memory is constrained, reduce to 128KB or 64KB.
5. Check XLIO startup logs for hardware capability warnings.

Supports suffixes: B, KB, MB (e.g., "512KB", "1MB").
Default value is 262144


================================================================================

MONITOR
-------

monitor.exit_report
Maps to **XLIO_PRINT_REPORT** environment variable.
Print a human readable report of resources usage at exit.
The report is printed during termination phase.
Therefore, it can be missed if the process is killed with the SIGKILL signal.
Use:
   - "auto" or -1
      Print report only if anomaly is detected on process exit.
   - "disable" or 0
      Never print report.
   - "enable" or 1
      Always print report.
Default value is -1

monitor.log.colors
Maps to **XLIO_LOG_COLORS** environment variable.
Use color scheme when logging.
Red for errors, purple for warnings and dim for low level debugs.
monitor.log.colors is automatically disabled when logging is directed
to a non terminal device (e.g. monitor.log.file_path is configured).
Default value is true

monitor.log.details
Maps to **XLIO_LOG_DETAILS** environment variable.
Add details on each log line:
   - 0=Basic log line
   - 1=ThreadId
   - 2=ProcessId+ThreadId
   - 3=Time + ProcessId + ThreadId [Time is in milli-seconds from start of process].
Default value is 0

monitor.log.file_path
Maps to **XLIO_LOG_FILE** environment variable.
Redirect all logging to a specific user defined file.
This is very useful when raising the monitor.log.level.
Library will replace a single '%d' appearing in the log file name
with the pid of the process loaded with XLIO.
This can help in running multiple instances of XLIO each with its own log file name.
Example: "/tmp/xlio.log"
Default value is ""

monitor.log.level
Maps to **XLIO_TRACELEVEL** environment variable.
Logging level the library will be using.
   - "none" or -2
      Print no log at all
   - "panic" or -1
      Panic level logging, this would generally cause fatal behavior and an exception
      will be thrown by the library. Typically, this is caused by memory
      allocation problems. This level is rarely used.
   - "error" or 0
      Runtime ERRORs in the library.
      Typically, these can provide insight for the developer of wrong internal
      logic like: Errors from underlying OS or Infiniband verbs calls. internal
      double mapping/unmapping of objects.
   - "warn" or 2
      Runtime warning that do not disrupt the workflow of the application but
      might warn of a problem in the setup or the overall setup configuration.
      Typically, these can be address resolution failure (due to wrong routing
      setup configuration), corrupted ip packets in the receive path or
      unsupported functions requested by the user application
   - "info" or 3
      General information passed to the user of the application. Bring up
      configuration logging or some general info to help the user better
      use the library
   - "details" or 4
      Complete XLIO configuration information.
      Very high level insight of some of the critical decisions done in library.
   - "debug" or 5
      High level insight to the operations done in the library. All socket API calls
      are logged and internal high level control channels log there activity.
   - "fine" or 6
      Low level run time logging of activity. This logging level includes basic
      Tx and Rx logging in the fast path and it will lower application
      performance.
      It is recommended to use this level with monitor.log.file_path parameter.
   - "finer" or 7
      Very low level run time logging of activity!
      This logging level will DRASTICALLY lower application performance.
      It is recommended to use this level with monitor.log.file_path parameter.
   - "all" or 8
      today this level is identical to finer.
Example: monitor.log.level="debug"
Default value is 3

monitor.stats.cpu_usage
Maps to **XLIO_CPU_USAGE_STATS** environment variable.
Calculate XLIO CPU usage during polling HW loops.
This information is available through XLIO stats utility.
Default value is false

monitor.stats.fd_num
Maps to **XLIO_STATS_FD_NUM** environment variable.
Maximum number of sockets monitored by XLIO statistic mechanism.
This affects the number of sockets that xlio_stats and
monitor.stats.file_path can report simultaneously.
xlio_stats tool is additionally limited by 1024 sockets.
Default value is 0

monitor.stats.file_path
Maps to **XLIO_STATS_FILE** environment variable.
Redirect socket statistics to a specific user defined file.
Library will dump each socket statistics into a file when closing the socket.
Example: "/tmp/xlio_stats.log"
Default value is ""

monitor.stats.shmem_dir
Maps to **XLIO_STATS_SHMEM_DIR** environment variable.
Set the directory path for the library to create the shared memory files for xlio_stats.
No files will be created when setting this value to empty string "".
Default value is /tmp/xlio


================================================================================

NETWORK
-------

network.multicast.mc_flowtag_acceleration
Maps to **XLIO_MC_FORCE_FLOWTAG** environment variable.
Forces the use of flow tag acceleration for multicast flows where
(SO_REUSEADDR) is set.
Applicable if there are no other sockets opened for the same flow in system.
Default value is false

network.multicast.mc_loopback
Maps to **XLIO_TX_MC_LOOPBACK** environment variable.
This parameter sets the initial value used by XLIO internally
to control the multicast loopback packets behavior during transmission.
An application that calls setsockopt() with IP_MULTICAST_LOOP will
run over the initial value set by this parameter.
Read more in 'Multicast loopback behavior' in notes section below.
Default value is true

network.multicast.wait_after_join_msec
Maps to **XLIO_WAIT_AFTER_JOIN_MSEC** environment variable.
This parameter indicates the time of delay in milliseconds for the first packet
sent after receiving the multicast JOINED event from the SM.
This is helpful to overcome loss of first few packets of an outgoing stream due to
SM lengthy handling of MFT configuration on the switch chips.
Default value is 0

network.neighbor.arp.uc_delay_msec
Maps to **XLIO_NEIGH_UC_ARP_DELAY_MSEC** environment variable.
Time in milliseconds to wait between unicast ARP attempts.
Default value is 10000

network.neighbor.arp.uc_retries
Maps to **XLIO_NEIGH_UC_ARP_QUATA** environment variable.
Number of unicast ARP retries before sending
broadcast ARP when neigh state is NUD_STALE.
Default value is 3

network.neighbor.errors_before_reset
Maps to **XLIO_NEIGH_NUM_ERR_RETRIES** environment variable.
Number of retries to restart the neighbor state machine after receiving an ERROR event.
Default value is 1

network.neighbor.update_interval_msec
Maps to **XLIO_NETLINK_TIMER** environment variable.
Sets the interval in milliseconds between neighbor table updates.
Default value is 10000

network.protocols.ip.mtu
Maps to **XLIO_MTU** environment variable.
Size of each Rx and Tx data buffer (Maximum Transfer Unit).
This value sets the fragmentation size of the packets sent by the library.
If network.protocols.ip.mtu is 0 then for each interface
XLIO will follow the actual MTU.
If network.protocols.ip.mtu is greater than 0 then this MTU value is
applicable to all interfaces regardless of their actual MTU.
Default value is 0

network.protocols.tcp.congestion_control
Maps to **XLIO_TCP_CC_ALGO** environment variable.
TCP congestion control algorithm.
The default algorithm coming with LWIP is a variation of Reno/New-Reno.
The new Cubic algorithm was adapted from FreeBSD implementation.
Use:
   - "lwip" or 0 for LWIP algorithm.
   - "cubic" or 1 for Cubic algorithm.
   - "disable" or 2 to disable the congestion algorithm.
Default value is 0

network.protocols.tcp.linger_0
Maps to **XLIO_TCP_ABORT_ON_CLOSE** environment variable.
Controls how XLIO terminates TCP connections when close() is called.

HOW TCP CONNECTION TERMINATION WORKS:
TCP defines two ways to close a connection:
1. Graceful close (FIN): Four-way handshake ensuring all data is delivered.
2. Abortive close (RST): Immediate termination, discarding pending data.

This parameter selects which method XLIO uses by default.

linger_0=true (RST/ABORTIVE CLOSE - fast teardown mode):
   XLIO immediately sends a RST (Reset) segment and discards all TCP state.
   The connection is terminated instantly without waiting for acknowledgments.

   + Immediate connection teardown - no handshake delay.
   + No TIME_WAIT state - socket resources freed instantly.
   + Faster port reuse - ephemeral ports available immediately.
   + Lower memory footprint - no lingering socket state.
   + Prevents TIME_WAIT accumulation on high-churn servers.
   + Better for servers handling thousands of short-lived connections.
   + Used by nvme_bf3 profile for maximum storage throughput.
   - Pending send buffer data is DISCARDED (potential data loss).
   - Unacknowledged data is NOT retransmitted.
   - Peer receives RST and may see ECONNRESET error.
   - Non-standard TCP behavior (violates graceful shutdown RFC).
   - Peer application may not process final data.
   - Can cause issues with protocols requiring complete delivery.
   - Not suitable when data integrity is critical.

linger_0=false (FIN/GRACEFUL CLOSE - reliable mode, default):
   XLIO performs standard TCP graceful shutdown:
   1. Sends any pending data in the send buffer.
   2. Sends FIN segment to signal end of transmission.
   3. Waits for peer's FIN+ACK (enters FIN_WAIT_1, FIN_WAIT_2 states).
   4. Enters TIME_WAIT state for 2*MSL (typically 60-120 seconds).

   + All pending data is transmitted and acknowledged.
   + Reliable delivery - peer receives all data.
   + Standard TCP behavior - follows RFC specifications.
   + No data loss risk - graceful handshake guarantees delivery.
   + Better debugging - clean connection state transitions.
   + Required for protocols needing complete data delivery.
   - TIME_WAIT state consumes socket resources for 60-120 seconds.
   - High connection churn can exhaust ephemeral ports.
   - Slower teardown - requires multiple round-trips.
   - Memory overhead for TIME_WAIT socket tracking.
   - May limit maximum connection rate on busy servers.

ADDITIONAL ABORT TRIGGERS:
Even when linger_0=false, XLIO sends RST (abortive close) if:
   - Application has unread data in the receive buffer when close() is called.
   - SO_LINGER socket option is set with l_linger=0.
   - Process is terminating (shutdown scenario).

PERFORMANCE IMPACT:

High-connection-rate servers (e.g., reverse proxies, load balancers):
   - TIME_WAIT accumulation can become a bottleneck.
   - With 10,000 connections/second and 60s TIME_WAIT: 600,000 sockets in TIME_WAIT.
   - Each TIME_WAIT socket consumes ~300-400 bytes of memory.
   - Enable linger_0=true if connection rate is limiting throughput.

Latency-sensitive applications:
   - linger_0=true eliminates FIN handshake latency (1-2 RTT savings).
   - Useful when connection setup/teardown latency matters.

Data-critical applications:
   - Keep linger_0=false to ensure all data is delivered.
   - Required for transactional systems, databases, file transfers.

RECOMMENDATIONS:
   - High-throughput web servers (Nginx, HAProxy): consider enabling (true).
   - Storage/NVMe applications: enable (true) for maximum throughput.
   - Short-lived connection servers: enable (true) to prevent port exhaustion.
   - Financial/trading systems: keep disabled (false) for data integrity.
   - Database connections: keep disabled (false).
   - File transfer applications: keep disabled (false).
   - When unsure: keep disabled (false) - it's the safer default.
   - If experiencing TIME_WAIT buildup: enable (true) or tune kernel settings.
Default value is false

network.protocols.tcp.mss
Maps to **XLIO_MSS** environment variable.
Defines the maximum TCP payload size (in bytes) that can be sent in a single
TCP segment without requiring IP fragmentation.

VALUE INTERPRETATION:
   - 0 (default): Auto-calculate MSS based on network.protocols.ip.mtu,
     leaving 40 bytes for IP (20) + TCP (20) headers:
     "TCP MSS = network.protocols.ip.mtu - 40"
   - Non-zero: Forces MSS to that specific value regardless of MTU.

COMMON MSS VALUES:
   - 1460 bytes: Standard Ethernet (MTU 1500 - 40 = 1460)
   - 8960 bytes: Jumbo frames (MTU 9000 - 40 = 8960, maximum supported)
   - 536 bytes: Conservative minimum for unknown network paths

IMPACT ON CONGESTION CONTROL:
MSS directly affects TCP congestion window (cwnd) growth, which determines
how much data can be in flight:

   - Slow Start phase: cwnd increases by 1 MSS per ACK received.
     Higher MSS = faster throughput ramp-up in bytes/second.
   - Congestion Avoidance phase: cwnd increases by ~MSS²/cwnd per ACK,
     roughly 1 MSS per RTT. Higher MSS = larger increments.
   - ssthresh (slow start threshold) minimum is 2×MSS.
   - After timeout: cwnd resets to 1 MSS.

Example: To reach 10MB cwnd from 14KB ssthresh at 1ms RTT:
   - With MSS=1460: ~7000 RTTs = ~7 seconds
   - With MSS=8960: ~1100 RTTs = ~1.1 seconds

PERFORMANCE TRADEOFFS:

High MSS values (4000-8960 bytes, Jumbo frames):
   + Faster congestion window growth - quicker throughput ramp-up.
   + Higher throughput efficiency - more payload per packet.
   + Better for bulk data transfers - fewer packets, less header overhead.
   + Reduced CPU overhead - fewer packets to process per byte transferred.
   + Enhanced TSO (TCP Segmentation Offload) efficiency - NIC aggregates
     more data per super-packet.
   - Requires network infrastructure supporting Jumbo frames end-to-end.
   - Higher latency per packet - larger packets take longer to transmit.
   - Larger retransmission penalty - losing one packet means losing more data.
   - May cause IP fragmentation if path MTU is smaller (severely degrades
     performance - fragments are reassembled in software).

Low MSS values (536-1460 bytes, standard Ethernet):
   + Compatible with all networks - standard 1500-byte MTU is universal.
   + Lower per-packet latency - smaller packets transmit faster.
   + Smaller retransmission cost - less data lost per dropped packet.
   + Better for lossy networks - faster recovery from packet loss.
   - Slower congestion window growth in bytes - longer throughput ramp-up.
   - Higher overhead - more headers per byte of payload.
   - More packets to process - higher CPU utilization per byte transferred.
   - Less efficient TSO aggregation.

Very low MSS values (<536 bytes):
   - WARNING: Generally not recommended.
   - Extremely high header overhead (40 bytes header for <536 payload).
   - Very slow cwnd growth - may never reach line rate on high-BDP paths.
   - Increased packet processing overhead.
   - Only useful for highly specialized latency-critical applications
     with tiny messages.

TSO (TCP SEGMENTATION OFFLOAD) INTERACTION:
When TSO is enabled (hardware_features.tcp.tso.enable), the NIC aggregates
multiple MSS-sized segments into larger super-packets for transmission.
   - MSS defines the segment granularity for TSO aggregation.
   - Higher MSS with TSO = fewer segments to manage, better efficiency.
   - TSO max payload (hardware_features.tcp.tso.max_size) caps the total
     aggregated size regardless of MSS.

CRITICAL CONSTRAINT - FRAGMENTATION:
MSS must not exceed (Path MTU - 40). If MSS is larger than what the network
path can handle:
   - Packets will be IP-fragmented (very bad for performance).
   - Fragments require CPU reassembly, defeating hardware offload benefits.
   - Packet loss of any fragment requires retransmitting entire original packet.
   - Always ensure MSS ≤ min(MTU along path) - 40.

RECOMMENDATIONS:
   - Default (0): Best for most deployments - auto-adapts to interface MTU.
   - Standard Ethernet: Use 0 or 1460 (for MTU 1500).
   - Jumbo frame networks: Use 0 with network.protocols.ip.mtu=9000,
     or explicitly set to 8960.
   - Mixed environments: Use 0 to auto-adapt per interface.
   - High-throughput bulk transfers: Maximize MSS with Jumbo frames if
     network supports it.
   - Latency-sensitive (trading, gaming): Standard 1460 is usually optimal;
     avoid very low values as they slow cwnd growth.

MAXIMUM VALUE: 8960 bytes (Jumbo MTU 9000 - 40 byte headers).
Default value is 0

network.protocols.tcp.nodelay.byte_threshold
Maps to **XLIO_TCP_NODELAY_TRESHOLD** environment variable.
Effective only if network.protocols.tcp.nodelay.enable is true.
Sets a minimum data size threshold before triggering immediate transmission.

HOW IT WORKS:
When nodelay.enable is true, this threshold adds a condition:
data is sent immediately only if unsent_data_bytes >= byte_threshold.
Smaller writes are still batched until they exceed the threshold.

PERFORMANCE TRADEOFFS:

byte_threshold=0 (default - pure TCP_NODELAY):
   + Lowest possible latency - every write triggers immediate send.
   + Standard TCP_NODELAY behavior expected by most applications.
   - Maximum packet overhead - each tiny write becomes its own packet.
   - Highest CPU and network overhead.

Low values (1-100 bytes):
   + Still very low latency for most practical message sizes.
   + Filters out accidental tiny writes (single bytes, partial headers).
   - Minimal practical benefit over threshold=0.

Medium values (100-500 bytes):
   + Good balance for mixed workloads with small and medium messages.
   + Reduces packet rate while maintaining low latency for real messages.
   + Useful when application sends small control messages mixed with data.
   - May add slight latency if messages are smaller than threshold.

High values (500-1460 bytes, approaching MSS):
   + Approaches Nagle-like batching while still having nodelay flag.
   + Useful for applications that want nodelay semantics but send
     many small writes that should be coalesced.
   - Defeats much of the purpose of enabling nodelay.
   - Values >= MSS (~1460) make nodelay nearly ineffective.

RECOMMENDATIONS:
   - For classic TCP_NODELAY behavior, use 0 (default).
   - For request-response with known message sizes, set slightly below
     your typical message size to avoid fragmenting single messages
     into multiple packets while still getting low latency.
   - Typical values: 0 for trading/gaming, 64-256 for mixed workloads.
   - Values above MSS (~1460 bytes) are rarely useful.
Default value is 0

network.protocols.tcp.nodelay.enable
Maps to **XLIO_TCP_NODELAY** environment variable.
When true, disables Nagle's algorithm for each TCP socket during initialization.
This means TCP segments are sent as soon as possible, even for small data.

HOW NAGLE'S ALGORITHM WORKS:
With Nagle enabled (enable=false), TCP buffers small writes and waits to send until:
   - All previously sent data has been acknowledged, OR
   - Enough data accumulates to fill a Maximum Segment Size (MSS, typically ~1460 bytes).
This reduces the number of small packets ('tinygrams') sent over the network.

PERFORMANCE TRADEOFFS:

enable=true (Nagle DISABLED - low latency mode):
   + Minimizes transmission latency - data sent immediately.
   + Critical for latency-sensitive applications: financial trading, gaming,
     real-time telemetry, interactive protocols (SSH, Telnet).
   + Better for request-response patterns where you send a small request
     and wait for a reply before sending more.
   - Increases packet rate and network overhead (more TCP/IP headers per byte).
   - May reduce throughput on high-latency links due to more round-trips.
   - Higher CPU usage from processing more packets.

enable=false (Nagle ENABLED - default, high throughput mode):
   + Better throughput for bulk data transfers and streaming.
   + Fewer packets means less network overhead and CPU usage.
   + More efficient use of network bandwidth.
   - Adds latency (up to ~200ms worst case) waiting to batch small writes.
   - Can cause 'write-write-read' deadlocks in poorly designed protocols
     where receiver waits for complete message while sender waits to batch.

RECOMMENDATIONS:
   - Bulk transfers, streaming, file copies: keep disabled (enable=false).
   - Low-latency trading, gaming, interactive apps: enable (enable=true).
   - If enabling, consider also enabling tcp.quickack for symmetric low latency.
   - Can be overridden per-socket using setsockopt(TCP_NODELAY).
Default value is false

network.protocols.tcp.push
Maps to **XLIO_TCP_PUSH_FLAG** environment variable.
Controls whether the TCP PUSH (PSH) flag is set on the last segment of each write operation.

HOW THE PUSH FLAG WORKS:
The TCP PUSH flag is a signal from the sender to the receiver that this segment
contains data that should be delivered to the application immediately, rather
than waiting for more data to arrive. When enabled, XLIO sets the PSH flag on
the last segment of every tcp_write operation.

Sender behavior:
   - When enabled (push=true): PSH flag is set on the last segment of each write.
   - When disabled (push=false): No PSH flags are set; data is sent without urgency hint.

Receiver behavior:
   - PSH flag signals the TCP stack to deliver buffered data to the application.
   - Without PSH, the receiver may buffer data waiting for more segments.

PERFORMANCE TRADEOFFS:

push=true (PUSH flag ENABLED - default, low latency mode):
   + Receiver delivers data immediately upon receiving the segment.
   + Lower end-to-end latency for request-response patterns.
   + Essential for interactive protocols (telnet, SSH, real-time messaging).
   + Prevents receiver from buffering when waiting for more data.
   + Better for applications sending discrete messages/records.
   - May slightly increase receiver CPU usage due to more frequent delivery events.
   - Receiver cannot batch multiple segments before delivery.

push=false (PUSH flag DISABLED - throughput mode):
   + Receiver can buffer data more efficiently.
   + Better for high-throughput bulk transfers (file copies, streaming).
   + Reduces delivery events on the receiver, lowering CPU overhead.
   + Used by NGINX profile for maximum throughput scenarios.
   - Higher latency: receiver may wait for more data before delivering.
   - Can cause delays in request-response patterns.
   - May cause application-level timeouts if receiver waits too long.
   - Not recommended for interactive or latency-sensitive applications.

TSO (TCP Segmentation Offload) INTERACTION:
   - The PSH flag does NOT prevent TSO segment merging.
   - Segments with PSH+ACK flags can still be merged for TSO.
   - TSO efficiency is preserved regardless of this setting.

RECOMMENDATIONS:
   - Interactive apps (trading, gaming, messaging): keep enabled (true).
   - Request-response APIs (HTTP, RPC): keep enabled (true).
   - Bulk transfers (file copy, backup, streaming): consider disabling (false).
   - High-throughput servers (NGINX, web servers): consider disabling (false).
   - When disabled, ensure application protocol handles message boundaries.
   - If experiencing latency issues with push=false, re-enable this option.
   - For mixed workloads, keep enabled (true) as latency impact is usually more
     noticeable than the throughput benefit from disabling.
Default value is true

network.protocols.tcp.quickack
Maps to **XLIO_TCP_QUICKACK** environment variable.
Controls TCP's delayed acknowledgement behavior. When true, disables delayed ACKs,
causing TCP to send an acknowledgement immediately after receiving each packet.

HOW DELAYED ACKs WORK:
By default (quickack=false), TCP may delay sending ACKs:
   - TCP waits up to tcp.timer_msec/2 (default ~50ms) before sending an ACK.
   - This allows batching multiple ACKs together and piggybacking ACKs on data.
   - Reduces ACK packet count by up to 50% compared to immediate ACKs.

PERFORMANCE TRADEOFFS:

quickack=true (Delayed ACKs DISABLED - low latency mode):
   + Lower round-trip latency - sender receives ACKs immediately.
   + Faster TCP congestion window growth during connection ramp-up,
     since CWND increases with each ACK received.
   + Better for latency-sensitive request-response protocols.
   + Avoids Nagle-delayed ACK interaction: when sender uses Nagle algorithm
     and receiver uses delayed ACKs, both sides wait for each other,
     causing latency spikes of 40-200ms ('ACK starvation').
   + Recommended when tcp.nodelay.enable=true for symmetric low latency.
   - Increases packet rate (more ACK packets on the network).
   - Higher CPU overhead processing more ACKs on sender side.
   - More network bandwidth consumed by ACK traffic.

quickack=false (Delayed ACKs ENABLED - default, efficient mode):
   + Reduced ACK traffic - fewer packets means less network overhead.
   + More efficient for bulk transfers and streaming workloads.
   + Lower CPU usage from processing fewer ACK packets.
   + ACKs can be piggybacked on response data, saving packets.
   - Adds latency for the sender (up to tcp.timer_msec/2, default ~50ms).
   - Slower congestion window growth during connection startup.
   - Can interact poorly with Nagle algorithm on sender, causing delays.

RECOMMENDATIONS:
   - Financial trading, gaming, real-time apps: enable (true) with nodelay.
   - Bulk transfers, streaming, high-throughput apps: keep disabled (false).
   - If latency spikes occur with small messages, try enabling quickack.
   - For asymmetric patterns (e.g., server sends data, client ACKs),
     enable quickack on the ACK-heavy side (client).
   - Can be overridden per-socket using setsockopt(TCP_QUICKACK).
Default value is false

network.protocols.tcp.timer_msec
Maps to **XLIO_TCP_TIMER_RESOLUTION_MSEC** environment variable.
Controls the resolution of TCP's internal timers in milliseconds.

THIS TIMER DRIVES TWO CRITICAL TCP MECHANISMS:

1. FAST TIMER (fires every timer_msec):
   - Sends delayed ACKs when data has been received but not yet acknowledged.
   - Maximum delayed ACK latency is timer_msec (e.g., 100ms default).

2. SLOW TIMER (fires every timer_msec * 2):
   - Retransmission timeout (RTO) detection - determines when lost packets
     are retransmitted.
   - Persist timer - sends zero-window probes when receiver advertises
     zero window.
   - Keepalive probes - checks if idle connections are still alive.
   - Connection state timeouts - cleans up connections in FIN_WAIT,
     SYN_RCVD, TIME_WAIT, and LAST_ACK states.
   - Out-of-sequence data cleanup.

PERFORMANCE TRADEOFFS:

Low values (10-50ms) - LATENCY-OPTIMIZED:
   + Faster detection of packet loss - retransmissions triggered sooner.
   + Lower delayed ACK latency - sender receives ACKs faster, improving
     throughput ramp-up and RTT measurements.
   + More responsive connection state machine.
   + Better for latency-sensitive applications (trading, gaming, real-time).
   - Higher CPU overhead - timers checked more frequently.
   - Increased context switching and power consumption.
   - May trigger premature retransmissions on high-latency networks.

High values (200-500ms) - CPU-OPTIMIZED:
   + Lower CPU overhead - less frequent timer processing.
   + Better batching of timer events.
   + Reduced power consumption.
   + More efficient for high-connection-count servers.
   - Slower packet loss detection - longer time before retransmission.
   - Higher delayed ACK latency - can slow down sender's congestion
     window growth.
   - Less responsive to network changes.
   - Connection state transitions (close, timeout) take longer.

RECOMMENDATIONS:
   - Latency-critical applications: 10-50ms
   - General-purpose / balanced: 100ms (default)
   - High connection count with CPU constraints: 200-500ms
   - Never exceed 500ms (RFC 1122 requires delayed ACK < 500ms)

CONSTRAINTS:
   - Minimum effective value: performance.threading.internal_handler.timer_msec
     (if set lower, it will be clamped to the internal thread timer).
   - Maximum recommended: 500ms per RFC 1122 Section 4.2.3.2.
Default value is 100

network.protocols.tcp.timestamps
Maps to **XLIO_TCP_TIMESTAMP_OPTION** environment variable.
If set, enable TCP timestamp option.
Currently, LWIP is not supporting RTTM and PAWS mechanisms.
See RFC1323 for info.
Use:
   - "disable" or 0 to disable.
   - "enable" or 1 to enable.
   - "os" or 2 for OS follow up.
Note that enabling causes a slight performance degradation.
Default value is 0

network.protocols.tcp.wmem
Maps to **XLIO_TCP_SEND_BUFFER_SIZE** environment variable.
TCP send buffer size of LWIP. This controls the maximum amount of data
that can be queued for transmission before the sender receives ACKs.
Supports suffixes: B, KB, MB, GB.

PERFORMANCE TRADEOFFS:

Higher values (e.g., 2MB+):
   - Improves throughput on high bandwidth-delay product (BDP) networks
     (e.g., high-speed links with latency > 1ms).
   - Allows the application to queue more data without blocking or receiving
     EAGAIN, reducing context switches.
   - Better handles bursty workloads.
   - CAUTION: Increases memory consumption per socket. With many concurrent
     connections, this can add up significantly.
   - CAUTION: May increase latency (bufferbloat) since more data is queued
     before TCP flow control kicks in.

Lower values (e.g., 64KB-256KB):
   - Reduces memory footprint per socket, beneficial for servers with
     thousands of concurrent connections.
   - Provides faster flow control feedback to the application.
   - Better latency characteristics for latency-sensitive applications.
   - CAUTION: May limit throughput on high-latency or high-bandwidth networks,
     as the pipe cannot be kept full.
   - CAUTION: Non-blocking sockets may receive more frequent EAGAIN errors.

SIZING GUIDANCE:
   - For optimal throughput, size >= bandwidth * round-trip-time (BDP).
     Example: 10 Gbps link with 1ms RTT = 10 Gbps * 0.001s = 1.25 MB.
   - For latency-sensitive applications, smaller is generally better.
   - Can be overridden per-socket using SO_SNDBUF setsockopt.
   - Default of 1MB is suitable for most datacenter and LAN environments.
Default value is 1MB

network.timing.hw_ts_conversion
Maps to **XLIO_HW_TS_CONVERSION** environment variable.
Defines how hardware timestamps are converted to a comparable format.
The value of network.timing.hw_ts_conversion is determined by all devices -
i.e if the hardware of one device does not support the conversion,
then it will be disabled for the other devices.
Use:
   - "disable" or 0 to disable
   - "raw_hw" or 1
      only convert the time stamp to seconds.nano_seconds time units
      (or disable if hardware does not supports).
   - "best_possible" or 2
      uses the best possible - raw hw or system time
      Sync to system time, then Raw hardware time
      disable if none of them are supported by hardware.
   - "system" or 3
      Sync to system time - convert the time stamp to seconds.nano_seconds
      time units comparable to receive software timestamp.
      disable if hardware does not support.
   - "ptp" or 4 - PTP Sync
      convert the time stamp to seconds.nano_seconds time units.
      in case it is not supported -
      will apply option "system" (or disable if hardware does not supports).
   - "rtc" or 5 - RTC Sync
      convert the time stamp to seconds.nano_seconds time units.
      in case it is not supported -
      will apply option "system" (or disable if hardware does not support).
Default value is 3


================================================================================

PERFORMANCE
-----------

performance.buffers.batching_mode
Maps to **XLIO_BUFFER_BATCHING_MODE** environment variable.

**What This Does:**
Controls how sockets manage RX and TX buffers - whether they cache buffers locally
for reuse or return them immediately to the shared buffer pool. This is a master
switch that affects both receive and transmit buffer lifecycle management.

**Understanding the Buffer Flow:**

*Without Batching (immediate return):*
```
Packet arrives → Buffer allocated from ring pool → App processes data → Buffer returned to ring pool immediately
```
Each operation potentially requires acquiring a lock on the shared pool.

*With Batching (cached locally):*
```
Packet arrives → Buffer from ring pool → App processes → Buffer cached in socket's local queue
                                                           ↓
                                          When cache reaches threshold → Batch return to ring pool
```
Lock acquisition is amortized across many buffers.

**The Three Modes Explained:**

---

**"disable" or 0 - No Buffer Batching**

Forces all buffer batch sizes to 1:
- performance.rings.tx.tcp_buffer_batch → 1
- performance.rings.tx.udp_buffer_batch → 1
- performance.buffers.rx.batch_size → 1

*Behavior:*
- Every RX buffer is returned to the ring/pool immediately after the application
  consumes the data (after recv/read completes)
- Every TX buffer is fetched one at a time from the pool for each send operation
- No local caching of buffers at the socket level

*Performance Characteristics:*

| Aspect              | Impact                                              |
|---------------------|-----------------------------------------------------|
| Memory usage        | LOWEST - no per-socket buffer caching               |
| Memory predictability | BEST - buffers never held by idle sockets         |
| Pool lock contention | HIGHEST - every buffer operation acquires lock    |
| Latency average     | Higher - pool access overhead on every operation    |
| Latency variance    | Higher - susceptible to lock contention spikes      |
| Throughput (PPS)    | Lower - serialization at pool locks                 |
| CPU overhead        | Higher - more frequent lock acquire/release         |

*When to Use:*
- Memory-constrained environments where buffer memory must be strictly controlled
- Systems with thousands of mostly-idle sockets where cached buffers would waste memory
- NGINX DPU deployments (this mode is auto-selected for NGINX_DPU profile)
- Debugging scenarios to simplify buffer lifecycle tracking
- When strict buffer accounting is required

---

**"enable_and_reuse" or 1 - Batching with Periodic Reclaim (DEFAULT)**

Enables local buffer caching with automatic cleanup of unused buffers.

*RX Buffer Behavior:*
- Processed buffers are queued in the socket's local reuse list (m_rx_reuse_buff)
- When queue size reaches threshold (rx_bufs_batch, default ~64), batch return is triggered
- At 2× threshold, forced immediate return occurs
- During TCP timer callbacks (~100ms intervals), `return_pending_rx_buffs()` checks
  for idle cached buffers and returns them if unused since last check
- Uses a two-phase "pending" mechanism: first call marks buffers pending, second call
  returns them if still unused - this prevents returning actively-used buffers

*TX Buffer Behavior:*
- Sockets fetch multiple TX buffers at once (tcp_buffer_batch/udp_buffer_batch)
- Unused fetched buffers stay in socket's local cache (dst_entry.m_p_tx_mem_buf_desc_list)
- During TCP timer callbacks, `return_pending_tx_buffs()` returns excess cached buffers
  to the ring pool via return_buffers_pool()

*The Reclaim Mechanism:*
Every TCP timer tick (~100ms, controlled by network.protocols.tcp.timer_msec),
XLIO calls return_pending_rx_buffs() and return_pending_tx_buffs(). This ensures:
- Idle sockets don't hoard buffers indefinitely
- Buffer pool doesn't get starved by inactive connections
- Critical for avoiding deadlocks where all buffers are held by idle sockets,
  preventing new FIN packets from being received to close connections

*Performance Characteristics:*

| Aspect              | Impact                                              |
|---------------------|-----------------------------------------------------|
| Memory usage        | MODERATE - caching with periodic cleanup            |
| Memory predictability | GOOD - idle buffers eventually returned           |
| Pool lock contention | LOW - batched operations amortize locking         |
| Latency average     | Better - most operations use local cache            |
| Latency variance    | Lower - fewer pool access interruptions             |
| Throughput (PPS)    | Good - efficient batch processing                   |
| CPU overhead        | Lower - amortized locking overhead                  |
| Deadlock safety     | BEST - reclaim prevents buffer starvation           |

*When to Use:*
- Most production deployments (this is the default for good reason)
- Servers with mixed workloads (active and idle connections)
- When you need balance between performance and resource efficiency
- Applications where connection lifetimes vary significantly
- Any scenario where buffer starvation/deadlock must be avoided

---

**"enable" or 2 - Batching without Reclaim**

Enables local buffer caching but DISABLES the periodic reclaim mechanism.

*Key Difference from Mode 1:*
- `return_pending_rx_buffs()` returns immediately without doing anything
- `return_pending_tx_buffs()` returns immediately without doing anything
- Cached buffers STAY in socket's local cache until the socket is destroyed
  or the cache naturally fills and triggers a batch return

*Behavior:*
- Same batching/caching as mode 1 during active send/receive
- BUT: idle sockets keep their cached buffers indefinitely
- Buffers only return to pool when:
  a) Cache exceeds threshold and batch return triggers, OR
  b) Socket is closed/destroyed

*Performance Characteristics:*

| Aspect              | Impact                                              |
|---------------------|-----------------------------------------------------|
| Memory usage        | HIGHEST - buffers held even when idle               |
| Memory predictability | LOWEST - hard to predict buffer distribution      |
| Pool lock contention | LOWEST - minimal pool interaction                 |
| Latency average     | Best - buffers always ready in local cache          |
| Latency variance    | LOWEST - no reclaim overhead during operation       |
| Throughput (PPS)    | Highest - maximum caching efficiency                |
| CPU overhead        | Lowest - no periodic reclaim processing             |
| Deadlock risk       | ELEVATED - possible buffer starvation               |

*When to Use:*
- High-frequency trading or ultra-low-latency applications where every
  microsecond of variance matters
- Applications with predictable, steady traffic patterns on all sockets
- Systems with abundant memory where buffer efficiency is less important
- Workloads where all sockets are continuously active (no idle connections)
- When you're certain total buffer demand won't exceed pool capacity

*CAUTION - Buffer Starvation Risk:*
Without reclaim, if many sockets cache buffers and then go idle, the global
buffer pool can be depleted. New connections may fail to allocate buffers,
and critically, FIN packets for connection teardown may not be processable,
potentially causing connection leaks or hangs.

---

**Interaction with Other Parameters:**

- **performance.rings.tx.tcp_buffer_batch:** When batching_mode=0 (disable),
  this is forced to 1 regardless of configured value.
- **performance.rings.tx.udp_buffer_batch:** Same as above - forced to 1 when disabled.
- **performance.buffers.rx.batch_size:** Controls the reuse threshold for RX buffers.
  With batching enabled, buffers accumulate until this threshold before batch return.
- **network.protocols.tcp.timer_msec:** Controls how often the reclaim mechanism runs
  in mode 1 (enable_and_reuse). Lower values = more frequent reclaim checks.

**Tuning Recommendations:**

1. **Start with default (enable_and_reuse / 1)** - safe for most workloads

2. **Switch to disable (0) if:**
   - Running thousands of connections with sporadic traffic
   - Memory is severely constrained
   - Seeing "unable to allocate buffer" errors with many idle sockets
   - Need deterministic memory usage for capacity planning

3. **Switch to enable (2) only if:**
   - All sockets are continuously active (no idle connections)
   - Latency consistency is paramount (HFT, real-time control)
   - You have profiled and confirmed no buffer starvation risk
   - Memory is abundant and not a concern
   - You've tested under peak load and verified buffer availability

**Monitoring:**
- xlio_stats shows buffer pool levels and allocation failures
- Watch for n_rx_ready_byte_drop and buffer allocation errors
- Monitor connection close failures that might indicate buffer starvation
Default value is 1

performance.buffers.rx.buf_size
Maps to **XLIO_RX_BUF_SIZE** environment variable.

**What This Does:**
Controls the size of each receive buffer element in the RX buffer pool (g_buffer_pool_rx_rwqe). These buffers hold incoming packet data received from the network interface. The buffer size determines the maximum packet payload that can be received without fragmentation or truncation.

**IMPORTANT: Striding RQ Interaction (Default Mode)**
When Striding RQ is enabled (hardware_features.striding_rq.enable=true, the DEFAULT), this parameter has LIMITED effect:
- Buffer size is calculated from stride configuration: `strides_num × stride_size` (default: 2048 × 64 = 128KB)
- The rx_buf_size value is used ONLY for LRO max_payload_sz calculation (see below)
- To change actual buffer sizes with Striding RQ, adjust hardware_features.striding_rq.strides_num and stride_size instead

This parameter primarily affects scenarios where **Striding RQ is DISABLED**.

**Value Semantics:**
- **0 (Default):** Auto-calculate based on maximum MTU across all network interfaces
  - With Striding RQ enabled: uses stride configuration for buffers
  - With Striding RQ disabled: uses max_mtu directly (e.g., 1500 bytes for standard Ethernet, ~9000 for jumbo frames)
- **Non-zero value:** Force specific buffer size (must be > TCP MSS, otherwise reset to 0)
- **Maximum:** 65280 bytes (0xFF00)
- **Alignment:** Automatically aligned to 64-byte boundary for cache efficiency

**How Buffer Size Affects LRO (Large Receive Offload):**
When LRO is enabled, multiple incoming TCP segments are aggregated into larger buffers before delivery to the application. The rx_buf_size limits this aggregation:
```
LRO max_payload_sz = min(actual_buf_size, 64KB) rounded down to 256-byte boundary
```
- With default (0) and Striding RQ: max_payload ≈ 64KB (limited by XLIO_MLX5_PARAMS_LRO_PAYLOAD_SIZE)
- With explicit rx_buf_size=8192: max_payload = 8192 bytes
- Larger max_payload = more segments aggregated = higher throughput, fewer packets to process
- Smaller max_payload = less aggregation = lower latency per delivery, more frequent packet events

**Memory Impact:**
Total RX buffer pool memory = buf_size × number_of_buffers
- Initial pool: 2 × ring_elements_count buffers per ring
- Pool expands dynamically as needed (up to XLIO_MEMORY_LIMIT)

With Striding RQ disabled:
- Standard MTU (1500): ~3MB per ring (2 × 1024 buffers × 1500 bytes)
- Jumbo frames (9000): ~18MB per ring
- Large rx_buf_size (32KB): ~64MB per ring

**Performance Tradeoffs:**

*High Values (8KB - 64KB) - Best for Throughput (when Striding RQ disabled):*

Benefits:
- Larger LRO aggregation: More TCP segments coalesced into fewer packets
- Fewer buffer descriptor operations: Each buffer holds more data
- Reduced per-packet overhead: Fewer completions to process for same data volume
- Better for bulk data transfer, streaming, and high-bandwidth applications
- Enables receiving jumbo frames when MTU > 1500

Drawbacks:
- Higher memory consumption: Each buffer consumes more memory
- Memory waste for small packets: UDP packets or small TCP segments leave buffer mostly empty
- Longer time-to-first-byte: LRO holds packets longer waiting for more segments
- May increase latency variance due to larger aggregation batches

Recommended for:
- Bulk data transfer (file transfer, backup, replication)
- Streaming media servers
- High-throughput applications where latency variance is acceptable
- Jumbo frame environments (MTU 9000)

*Low Values (MTU-sized, ~1500-2048) - Best for Latency (when Striding RQ disabled):*

Benefits:
- Lower memory footprint: Efficient memory utilization
- Minimal LRO aggregation delay: Packets delivered sooner
- Better memory efficiency: Buffers sized appropriately for typical packets
- More predictable latency: Less time spent in aggregation
- Suitable for mixed workloads with varying packet sizes

Drawbacks:
- Limited LRO benefit: Less segment aggregation reduces throughput optimization
- Higher per-packet overhead: More completions to process
- More buffer management operations: Smaller buffers cycle more frequently

Recommended for:
- Latency-sensitive applications (trading, gaming, real-time control)
- Request-response protocols (HTTP, database queries)
- Applications with small message sizes
- Memory-constrained environments

*Value of 0 (Auto-calculation) - Recommended for Most Users:*

Benefits:
- Automatically adapts to network configuration
- Optimal for the configured MTU
- No manual tuning required
- Works correctly with both standard and jumbo frames

**Interaction with Other Parameters:**

- **hardware_features.striding_rq.enable:** When true (default), stride configuration overrides rx_buf_size for buffer allocation. rx_buf_size only affects LRO max_payload calculation.

- **hardware_features.striding_rq.strides_num/stride_size:** With Striding RQ enabled, these control actual buffer sizes. Effective buffer = strides_num × stride_size.

- **hardware_features.lro:** LRO aggregation is limited by rx_buf_size (or stride buffer size). Larger buffers allow more aggressive aggregation.

- **network.protocols.ip.mtu:** rx_buf_size should be >= MTU to receive full packets without fragmentation. Auto-calculation (value 0) ensures this.

- **performance.rings.rx.ring_elements_count:** Determines how many buffers are allocated per ring. Total memory = buf_size × ring_elements_count × 2.

- **performance.rings.rx.spare_buffers:** Local buffer cache per ring. Must be compatible with buffer size for efficient recycling.

**Validation Rules:**
- If rx_buf_size <= TCP MSS (Maximum Segment Size), it's automatically reset to 0 (auto-calculate)
- TCP MSS = MTU - 40 (IP header + TCP header)
- Value is clamped to maximum 65280 bytes (0xFF00)

**Tuning Recommendations:**

1. **For most deployments:** Use default (0) - automatically optimal for your MTU

2. **For maximum throughput (Striding RQ disabled):**
   - Set to 32KB-64KB
   - Ensure XLIO_MEMORY_LIMIT is sufficient for larger buffer pool
   - Combine with hardware_features.lro enabled

3. **For latency-sensitive applications (Striding RQ disabled):**
   - Use default (0) or MTU-sized value
   - Consider disabling LRO for minimum latency

4. **For jumbo frames:**
   - Use default (0) - auto-detects MTU
   - Or explicitly set to match your jumbo MTU (e.g., 9000)

5. **For Striding RQ environments (default):**
   - This parameter has minimal effect on buffer allocation
   - Tune hardware_features.striding_rq.strides_num and stride_size instead
   - rx_buf_size only affects LRO max_payload calculation

**Monitoring:**
- Check xlio_stats for buffer pool statistics (n_buffer_pool_len, n_buffer_pool_created)
- Monitor n_rx_sw_pkt_drops for buffer exhaustion issues
- If seeing drops, consider increasing XLIO_MEMORY_LIMIT rather than buffer size

Supports suffixes: B, KB, MB, GB (e.g., "8KB", "32KB").
Default value is 0

performance.buffers.rx.prefetch_before_poll
Maps to **XLIO_RX_PREFETCH_BYTES_BEFORE_POLL** environment variable.

**What This Controls:**
Number of bytes to speculatively prefetch into CPU L1 cache BEFORE polling the Completion Queue (CQ) for new packets. Unlike the regular `prefetch_size` parameter (which prefetches AFTER a packet is received), this parameter enables predictive prefetching of the NEXT expected packet's buffer location.

**How It Works - The Mechanism:**

XLIO maintains knowledge of where the next packet will arrive based on the order buffers were posted to the receive queue:

1. **Buffer Posting Phase (Background):**
   - When XLIO posts RX buffers to the hardware queue, it chains them together via an internal linked list (`p_prev_desc`)
   - This creates a predictable order: buffer posted first will receive a packet first (FIFO)

2. **Prefetch Before Poll (when enabled):**
   - At the START of each `poll_and_process_element_rx()` call, BEFORE checking for completions
   - XLIO prefetches the predicted next buffer location into L1 cache
   - For Regular RQ: prefetches `m_p_next_rx_desc_poll->p_buffer`
   - For Striding RQ: prefetches `m_rx_hot_buffer->p_buffer + consumed_offset`

3. **Regular Prefetch (happens later, uses `prefetch_size`):**
   - AFTER a CQE is polled and validated
   - Prefetches the CURRENT packet's data (already in hand)

**The Key Difference: Timing**

```
                     Regular prefetch (prefetch_size)
                              |
                              v
[Poll CQ] --> [Get CQE] --> [Prefetch packet data] --> [Process packet]
    ^
    |
prefetch_before_poll happens HERE, speculatively
```

With `prefetch_before_poll`, the prefetch happens BEFORE we even know if a packet is ready. This is speculative optimization.

**Why This Specifically Benefits LOW PPS (Packets Per Second) Traffic:**

At low packet rates (e.g., <100K PPS), there are significant idle gaps between packets:

*Without prefetch_before_poll (default):*
```
Packet 1 arrives --> [Cache warm] --> processed
   |--- 10ms idle gap (CPU does other work, cache contents change) ---|
Packet 2 arrives --> [Buffer is COLD in cache] --> L1 cache miss --> higher latency
```

During the idle gap:
- CPU executes application code, other threads, OS tasks
- L1 cache contents are evicted/replaced with other data
- The next RX buffer becomes "cold" (not in L1 cache)
- When packet 2 arrives, reading its buffer causes expensive L1 cache misses (~4-10 cycles penalty per miss)

*With prefetch_before_poll enabled:*
```
Application calls recv() --> poll starts
  ^
  |-- Speculatively prefetch next RX buffer location (even if no packet yet)
  |-- Returns with no data (EAGAIN or timeout)

[Buffer now warm in L1 cache]

Next recv() call --> poll starts
  ^
  |-- Packet 2 is now ready
  |-- Buffer already in L1 cache from previous speculative prefetch!
  |-- Lower latency to process and copy to user space
```

Even if the poll returns empty, the prefetch keeps the expected buffer location warm. When a packet does arrive, cache hits replace cache misses.

**Why This Does NOT Benefit HIGH PPS Traffic:**

At high packet rates (e.g., >1M PPS), packets arrive back-to-back with minimal gaps:

```
Packet 1 --> Packet 2 --> Packet 3 --> Packet 4 (continuous stream)
        ~1µs      ~1µs      ~1µs
```

- Regular prefetch (after poll) is already effective because:
  - Prefetch for packet N happens while processing packet N
  - By the time packet N+1 is polled, the prefetched data is still warm
  - Very short inter-packet gaps mean L1 cache hasn't been evicted

- The "before poll" prefetch becomes redundant:
  - We're already about to poll and find a packet immediately
  - Extra prefetch instruction overhead accumulates at millions of PPS
  - Each poll iteration adds unnecessary prefetch CPU cycles

**Value Range:** 0 to 2044 bytes (0 = disabled, 32-2044 when enabled)

**Performance Impact - VALUE OF 0 (Disabled, Default):**

| Aspect | Impact |
|--------|--------|
| CPU overhead per poll | LOWEST - no speculative prefetch |
| Low PPS latency | Higher (buffer may be cold when packet arrives) |
| High PPS throughput | OPTIMAL - no redundant prefetch overhead |
| Recommended for | High throughput applications, >500K PPS workloads |

**Performance Impact - LOW Values (32-128 bytes):**

| Aspect | Impact |
|--------|--------|
| Prefetch overhead | Minimal - 1-2 cache lines per poll |
| Cache warming | Partial - warms headers + small payload portion |
| L1 pollution risk | Very low |
| Best for | Small packet workloads, minimal latency improvement needed |

With 64 bytes, you prefetch one cache line - enough to warm the IP/UDP headers and ~20-30 bytes of payload.

**Performance Impact - MEDIUM Values (256-512 bytes):**

| Aspect | Impact |
|--------|--------|
| Prefetch overhead | Moderate - 4-8 cache lines per poll |
| Cache warming | Good - headers + substantial payload |
| L1 pollution risk | Low to moderate |
| Best for | General low-latency applications, medium packet sizes |

256 bytes (matching the default `prefetch_size`) provides a good balance for typical packet sizes.

**Performance Impact - HIGH Values (1024-2044 bytes):**

| Aspect | Impact |
|--------|--------|
| Prefetch overhead | Higher - 16-32 cache lines per poll |
| Cache warming | Excellent - nearly entire MTU-sized packet |
| L1 pollution risk | Significant - ~1-2KB of L1 per poll (L1 is typically 32KB) |
| Best for | Large packets near MTU, latency-critical applications that can afford the overhead |

**Critical Tradeoff: Prefetch Overhead vs. Cache Warming**

Every poll operation pays the prefetch cost, even if:
- No packet is waiting (poll returns empty)
- Application is polling frequently in a tight loop
- Multiple sockets share the same ring (prefetch executed once per ring poll)

```
Cost per poll = (prefetch_before_poll / 64) prefetch instructions

At 1M polls/sec with 256 bytes:
  = 4 prefetch instructions × 1M = 4M extra instructions/sec
  = Negligible on modern CPUs

At 10M polls/sec with 1024 bytes:
  = 16 prefetch instructions × 10M = 160M extra instructions/sec
  = Potentially measurable overhead
```

**When to Enable (Non-zero Value):**

1. **Latency-sensitive, low PPS applications:**
   - Financial trading systems
   - Real-time control systems
   - Interactive gaming servers
   - Any application where sub-microsecond latency improvements matter

2. **Bursty traffic patterns:**
   - Request-response with idle gaps (HTTP, database queries)
   - Sporadic sensor data
   - Event-driven systems with variable inter-arrival times

3. **When you've profiled and see L1 cache misses:**
   - Use `perf stat -e L1-dcache-load-misses` to measure
   - If misses are high in RX path, this parameter may help

**When to Keep Disabled (Value 0, Default):**

1. **High throughput, high PPS workloads:**
   - Streaming applications
   - Bulk data transfer
   - Packet forwarding/routing

2. **When regular prefetch (`prefetch_size`) is sufficient:**
   - Continuous traffic with minimal idle gaps
   - Multi-threaded applications where cache is always warm

3. **CPU-constrained environments:**
   - Where every instruction matters
   - When CPU utilization is already near 100%

**Tuning Recommendations:**

*For ultra-low latency (trading, HFT):*
- Start with 256 bytes
- Profile baseline latency
- Try 128 and 512, measure p99 latency
- Choose value that minimizes latency variance (std-dev)

*For mixed workloads (web servers, databases):*
- Try 256 bytes
- Monitor both latency and throughput
- If throughput degrades, disable or reduce value

*For request-response patterns:*
- Enable with value matching typical response size + 40 bytes (headers)
- Example: 512-byte responses → set to 512 or 576

**Interaction with Other Parameters:**

- **performance.buffers.rx.prefetch_size:** Complementary, not exclusive. `prefetch_size` handles the current packet; `prefetch_before_poll` speculatively warms the next. Both can be enabled simultaneously for maximum cache warming.

- **performance.polling.blocking_rx_poll_usec:** Long blocking polls (high values) benefit more from `prefetch_before_poll` since the idle wait time means buffers go cold.

- **hardware_features.striding_rq:** With Striding RQ, the prefetch target is the next stride position within the current WQE buffer, calculated as `current_buffer + consumed_bytes`. Value is automatically clamped to stride_size.

- **performance.polling.max_rx_poll_batch:** With large batch sizes, prefetch happens once per poll call, not per packet. This reduces overhead for high PPS.

**Profiling Tips:**

1. Measure baseline without prefetch_before_poll:
   ```bash
   XLIO_RX_PREFETCH_BYTES_BEFORE_POLL=0 ./your_app
   ```

2. Enable with default prefetch_size value:
   ```bash
   XLIO_RX_PREFETCH_BYTES_BEFORE_POLL=256 ./your_app
   ```

3. Compare latency distributions:
   - Focus on p50, p99, p99.9 latencies
   - Low PPS workloads should show improvement in tail latencies
   - High PPS workloads may show no improvement or slight degradation

4. Monitor cache effectiveness:
   ```bash
   perf stat -e L1-dcache-load-misses,L1-dcache-loads ./your_app
   ```
   Look for reduced L1 miss rate with prefetch enabled.

Disable with 0.
Default value is 0

performance.buffers.rx.prefetch_size
Maps to **XLIO_RX_PREFETCH_BYTES** environment variable.

**What This Controls:**
Number of bytes to prefetch into CPU L1 cache when processing received packets. This optimization targets the receive path where packet data is copied from XLIO's RX buffers to the application's user-space buffers during recv()/recvfrom()/recvmsg() calls.

**How It Works - The Complete Flow:**

1. **Packet arrives at NIC** → Hardware posts a Completion Queue Entry (CQE)
2. **XLIO polls CQ** → Retrieves the CQE indicating packet is ready
3. **CQE Processing (prefetch happens here):**
   - `prefetch_range()` is called on the packet buffer
   - Prefetch starts AFTER the L2 (Ethernet) header (~14-18 bytes into buffer)
   - Issues CPU prefetch instructions to bring data into L1 cache
4. **Packet added to socket's Ready Queue** → Awaits application retrieval
5. **Application calls recv()** → Data is memcpy'd from buffer to user space
   - If prefetch was effective, memcpy hits warm L1 cache!

**Critical Detail: What Actually Gets Prefetched**

The prefetch starts after the L2 header (m_sz_transport_header ≈ 14-18 bytes), so the prefetched region includes:

```
Buffer Layout:
|-- L2 Header (14-18B) --|-- IP Header (20B) --|-- UDP/TCP (8-20B) --|-- PAYLOAD --|
                         ^                                            ^
                         |-- Prefetch starts here                     |-- App reads from here
```

- **Prefetched but NOT read by app:** IP + UDP/TCP headers (~28-40 bytes)
- **Effective payload prefetch:** prefetch_size minus ~30-40 bytes of headers

With default value (256 bytes):
- Total prefetched: 256 bytes starting at offset ~14
- Headers within prefetch: ~30-40 bytes (not useful to app)
- **Actual payload prefetched: ~216-226 bytes**

With minimum value (32 bytes):
- Total prefetched: 32 bytes starting at offset ~14
- Almost entirely IP+UDP headers!
- **Actual payload prefetched: ~0-4 bytes** (nearly useless)

**CPU Cache Mechanics:**

- Uses hardware prefetch instructions: `prefetcht0` (x86), `dcbt` (PPC64), `prfm pldl1keep` (ARM64)
- One prefetch instruction per L1 cache line:
  - x86/ARM64: 64-byte cache lines
  - PPC64: 128-byte cache lines
- Example (x86): 256 bytes = 4 prefetch instructions (256 ÷ 64)
- Actual bytes prefetched = min(prefetch_size, packet_size - L2_header_size)

**Value Range:** 32 to 2044 bytes (MCE_MIN_RX_PREFETCH_BYTES to MCE_MAX_RX_PREFETCH_BYTES)

**Performance Impact - LOW Values (32-128 bytes):**

| Aspect | Impact |
|--------|--------|
| Prefetch overhead | MINIMAL - 1-2 prefetch instructions |
| Effective payload prefetch | POOR - mostly prefetches IP/UDP headers, minimal payload |
| Cache pollution risk | LOWEST - minimal data brought into L1 |
| Best for | Extremely small payloads (<64 bytes), or when prefetch overhead must be minimized |

With 32-64 bytes, you're mostly prefetching protocol headers that the application doesn't read. This setting provides little benefit for typical workloads.

**Performance Impact - DEFAULT Value (256 bytes):**

| Aspect | Impact |
|--------|--------|
| Prefetch overhead | LOW - 4 prefetch instructions (x86) |
| Effective payload prefetch | GOOD - ~216 bytes of actual payload |
| Cache pollution risk | LOW - fits comfortably in L1 (32KB typical) |
| Best for | Small to medium packets, mixed workloads, general-purpose applications |

The default 256 bytes is well-balanced: 4 cache lines cover typical small message payloads while keeping overhead minimal.

**Performance Impact - HIGH Values (512-1024 bytes):**

| Aspect | Impact |
|--------|--------|
| Prefetch overhead | MODERATE - 8-16 prefetch instructions |
| Effective payload prefetch | EXCELLENT - ~470-990 bytes of payload |
| Cache pollution risk | MODERATE - consumes 8-16 cache lines |
| Write efficiency | HIGH - large portion of payload pre-warmed |
| Best for | Medium to large packets, throughput-focused applications |

**Performance Impact - MAXIMUM Values (1500-2044 bytes, near MTU):**

| Aspect | Impact |
|--------|--------|
| Prefetch overhead | HIGH - 24-32 prefetch instructions |
| Effective payload prefetch | MAXIMUM - entire payload pre-warmed |
| Cache pollution risk | HIGHER - may evict other useful data from 32KB L1 |
| Instruction overhead | SIGNIFICANT - prefetch loop adds CPU cycles per packet |
| Best for | Large packets near MTU size, applications that always read full payloads |

**When Prefetch Helps Most:**

1. **Cold buffers:** Buffers recently allocated from pool (not in cache from previous use)
2. **Large payloads:** Where multiple cache lines need to be read by application
3. **High memory latency systems:** Multi-socket NUMA where buffer may be on remote node
4. **Bursty traffic patterns:** After idle periods when L1 cache has been repurposed
5. **Run-to-completion mode:** Tight timing between prefetch and application read

**When Prefetch May Hurt:**

1. **Very small packets (<64 bytes payload):** Overhead exceeds benefit; headers dominate prefetch
2. **Extremely high PPS (millions/sec):** Prefetch instruction overhead accumulates significantly
3. **Partial reads:** Application uses MSG_PEEK or reads only first few bytes
4. **Long ready queue:** If many packets queue before app reads, prefetched data evicts from L1
5. **Worker threads mode:** Timing gap between prefetch (worker thread) and read (app thread) may cause L1 eviction
6. **L1 cache contention:** Other hot data being accessed may evict prefetched packet data

**Timing Window Consideration:**

Prefetch effectiveness depends on the time between prefetch (CQ processing) and consumption (app recv()):

- **Ideal:** < 1-10 microseconds - data stays warm in L1
- **Acceptable:** 10-100 microseconds - data likely still in L2/L3
- **Poor:** > 100 microseconds - data may be evicted, prefetch wasted

In run-to-completion mode (default), timing is usually tight. With worker threads or if application is slow to call recv(), the gap widens and prefetch benefit decreases.

**Architecture-Specific Behavior:**

*x86/x86_64:*
- L1 cache: typically 32KB per core
- Cache line: 64 bytes
- prefetcht0 brings data to all cache levels (L1, L2, L3)
- Modern CPUs have hardware prefetchers that may already optimize sequential access

*ARM64 (aarch64):*
- L1 cache: typically 32-64KB per core
- Cache line: 64 bytes (most implementations)
- prfm pldl1keep instruction with L1 keep hint
- Explicit prefetch often more beneficial than x86 due to less aggressive hardware prefetch

*PPC64:*
- L1 cache: typically 32-64KB per core
- Cache line: 128 bytes (larger than x86/ARM)
- dcbt instruction for data cache block touch
- Fewer prefetch instructions needed: 256 bytes = only 2 instructions

**Tuning Recommendations:**

*For latency-sensitive applications (trading, real-time, gaming):*
- Start with default (256 bytes) - proven good balance
- If typical message size < 200 bytes, consider reducing to 128
- If typical message size > 512 bytes, consider increasing to match payload size + 40 (for headers)
- Profile with `perf stat -e cache-misses` to measure actual L1 miss rates

*For throughput-oriented applications (streaming, bulk transfer):*
- Match prefetch_size to your typical payload size + ~40 bytes for headers
- For MTU-sized packets (1500B): use 1024-1500 bytes
- For jumbo frames (9000B MTU): use 2044 (maximum) - covers first 2KB of payload
- Monitor CPU utilization for prefetch overhead at very high PPS

*For high PPS applications (packet processing, forwarding):*
- Consider reducing to 128-256 bytes or using minimum (32)
- At millions of PPS, even small per-packet overhead multiplies significantly
- Profile to determine if prefetch actually improves throughput

*For mixed workloads:*
- Default (256 bytes) is usually optimal
- Covers headers + ~216 bytes payload - good for most common packet sizes
- Low overhead with meaningful benefit

*For NUMA systems:*
- Prefetch more valuable when buffers may reside on remote NUMA node
- Consider higher values (512-1024 bytes)
- Combine with proper NUMA pinning (numactl) for best results

**Interaction with Other Parameters:**

- **performance.buffers.rx.prefetch_before_poll:** Alternative prefetch timing - prefetches BEFORE polling CQ. Use one or the other based on your latency needs.

- **performance.polling.max_rx_poll_batch:** If batch size is large, prefetch is done per-packet within the batch. Higher batch = more prefetch instructions per poll cycle.

- **hardware_features.striding_rq:** With Striding RQ enabled, buffer layout differs slightly but prefetch mechanics remain the same.

- **network.protocols.ip.mtu:** Maximum useful prefetch_size is approximately MTU - 14 (L2 header). Prefetching beyond packet size is harmless but wasteful.

**Profiling Tips:**

1. Measure L1 cache miss rate:
   ```bash
   perf stat -e L1-dcache-load-misses,L1-dcache-loads ./your_app
   ```

2. Compare different values:
   - Run benchmark with prefetch_size=32 (baseline, minimal prefetch)
   - Run with 256 (default)
   - Run with value matching your payload size
   - Measure both latency and throughput

3. Look for:
   - Reduced L1 cache miss percentage with appropriate prefetch size
   - But watch for increased CPU utilization from prefetch overhead
   - Sweet spot is where miss rate improves without significant CPU increase
Default value is 256

performance.buffers.tcp_segments.pool_batch_size
Maps to **XLIO_TX_SEGS_POOL_BATCH_TCP** environment variable.

**What This Does:**
Controls the allocation granularity of the global TCP segment pool (g_tcp_seg_pool).
When the pool needs more segments, it allocates this many tcp_seg structures
in a single memory allocation from the heap. This is the bottom tier of XLIO's
three-level segment caching architecture.

**Three-Level Caching Architecture Position:**
This parameter controls the bottom tier - the global pool:
1. **Global Pool** (g_tcp_seg_pool): THIS LEVEL - Central pool protected by a spinlock.
   When exhausted, allocates 'pool_batch_size' new segments from heap memory.
   Segments are pre-linked into a free list for O(1) allocation.
2. **Ring Cache** (per-ring): Middle tier, fetches from global pool in
   ring_batch_size batches (default 1024).
3. **Socket Cache** (per-connection): Top tier, fetches from ring in
   socket_batch_size batches (default 64).

**How Global Pool Allocation Works:**
1. Application starts, global pool is initialized with 'pool_batch_size' segments
2. Rings fetch segments from pool as connections are established
3. When pool is empty and a ring needs segments, pool calls expand():
   - Allocates pool_batch_size × sizeof(tcp_seg) bytes (~1.6MB with default)
   - Creates a linked list of tcp_seg structures
   - Prepends to existing free list
   - Statistics tracked: total_objs, allocations, expands
4. Segments return to pool when rings' caches overflow
5. Segments are NEVER deallocated - pool only grows (no shrinking)

**Memory Calculation:**
Each tcp_seg structure is approximately 100 bytes (including the l2_l3_tcphdr_zc
array for zerocopy header storage). With default pool_batch_size of 16384:
- Initial allocation: 16384 × ~100 bytes ≈ 1.6MB
- Each expansion adds another ~1.6MB
- Memory is never returned to OS until process exit

**Value Tradeoffs:**

*High Values (16384, 32768, 65536) - Optimize for fewer allocations:*
+ Fewer heap allocations over application lifetime
+ Less memory fragmentation (larger contiguous blocks)
+ Lower allocation overhead during traffic spikes
+ Better for long-running servers that reach steady state
+ Pre-allocates enough for thousands of concurrent segments
+ Segments are contiguous in memory - better for CPU prefetching
- Higher initial memory footprint (even if not all segments are used)
- Memory committed upfront that might not be needed
- Each expansion is a larger memory commitment
- Overkill for applications with few connections

*Low Values (256, 512, 1024) - Optimize for memory efficiency:*
+ Lower initial memory footprint
+ Memory grows more gradually with actual demand
+ Better for memory-constrained environments
+ More appropriate for applications with predictable, low connection counts
+ Segments still contiguous within each allocation batch
- More frequent heap allocations during warmup phase
- More memory fragmentation (many small allocations)
- Brief latency spikes during allocation events
- With many concurrent segments, triggers many expand() calls

**Impact on Performance:**

*During Warmup Phase:*
When application starts and connections are established:
- Lower values: More expand() calls, each holding global pool spinlock
- Higher values: Fewer expand() calls, but each is larger
The expand() operation includes heap allocation and linked list setup,
so minimizing calls reduces warmup latency variance.

*During Steady State:*
Once enough segments are allocated, pool_batch_size has minimal impact:
- Segments cycle between socket → ring → pool → ring → socket
- expand() is rarely called
- Pool acts as a reservoir for bursty traffic across rings

*During Traffic Spikes:*
If traffic spike exceeds pooled segments:
- Lower values: Multiple expand() calls, lock contention during each
- Higher values: Single larger expand(), one-time latency hit

**Relationship with Other Batch Parameters:**
The three batch sizes form a hierarchy:
- socket_batch_size (64): Fastest path, no lock
- ring_batch_size (1024): Per-ring lock, 16× socket batches
- pool_batch_size (16384): Global spinlock, 16× ring batches

The 16× ratios mean:
- Pool can satisfy ~16 ring refills before needing to expand
- Ring can satisfy ~16 socket refills before needing pool access
- This reduces frequency of more expensive (locked) operations

**Tuning Recommendations:**

- **High-performance servers (long-running, high traffic):**
  Default 16384 or higher. Pre-allocates generously for sustained load.
  Memory cost (~1.6MB per batch) is negligible for such deployments.

- **Latency-sensitive applications:**
  Consider 32768 or 65536 to minimize expand() calls during operation.
  Trading memory for predictability - avoid allocation during trading hours.

- **Memory-constrained systems:**
  Use 2048-4096. More expand() calls but lower memory ceiling.
  Each batch is ~200-400KB instead of ~1.6MB.

- **Applications with few connections (<100):**
  Lower to 1024-2048. Don't pre-allocate for scale you won't need.
  100 connections × ~10 segments each = ~1000 segments typical max.

- **Containerized/serverless workloads:**
  Lower values (1024-4096) match resource limits.
  Short-lived instances don't benefit from large pools.

- **High connection count servers (10K+):**
  Default 16384 or higher. Many connections = many concurrent segments.
  With 10K connections, peak might need 50K+ segments across all queues.

**Monitoring and Debugging:**
Use xlio_stats to monitor pool behavior:
- n_tcp_seg_pool_size: Current number of segments in global pool
- n_tcp_seg_pool_no_segs: Count of allocation failures (pool exhausted)
  High values indicate pool_batch_size or total memory is insufficient.

At shutdown, XLIO logs pool statistics:
  "TCP segments pool statistics:"
  "  allocations=N expands=M total_segs=P"
Many 'expands' suggests pool_batch_size could be increased.

**Related Parameters:**
- tcp_segments.socket_batch_size: How many segments socket fetches from ring
- tcp_segments.ring_batch_size: How many segments ring fetches from global pool
- performance.rings.rx.allocation_logic: Determines ring count (affects total segment demand)
Default value is 16384

performance.buffers.tcp_segments.ring_batch_size
Maps to **XLIO_TX_SEGS_RING_BATCH_TCP** environment variable.

**What This Does:**
Controls how many TCP segment structures (tcp_seg) a ring fetches at once
from the global segment pool when its local cache is empty. Each ring
maintains a local cache to reduce contention on the global pool's spinlock.

**Three-Level Caching Architecture Position:**
This parameter controls the middle tier of XLIO's hierarchical caching:
1. **Global Pool** (g_tcp_seg_pool): Central pool protected by a spinlock.
   Allocates new segments in large batches (pool_batch_size, default 16384).
2. **Ring Cache** (per-ring): THIS LEVEL - Each ring maintains a local cache,
   fetching from the global pool in ring_batch_size batches.
   Protected by a per-ring lock (m_tcp_seg_lock).
3. **Socket Cache** (per-connection): Each TCP connection has its own cache,
   fetching from the ring in socket_batch_size batches (default 64).

**How Ring-Level Caching Works:**
1. When a socket's cache is empty, it calls ring->get_tcp_segs(socket_batch_size)
2. Ring acquires its own lock (m_tcp_seg_lock) - separate from global pool lock
3. If ring cache has enough segments, they're returned immediately
4. If ring cache is empty or insufficient, ring acquires global pool spinlock
   and fetches 'ring_batch_size' segments in one operation
5. Requested segments go to socket, remainder stays in ring cache
6. When sockets return segments, they go back to ring cache first
7. Ring returns segments to global pool when:
   - Ring cache size exceeds 2 × ring_batch_size (the return threshold)
   - At that point, half of the cached segments are returned to global pool

**The Return Threshold Mechanism:**
The return threshold = 2 × ring_batch_size (e.g., 2048 with default 1024).
This creates a hysteresis effect that prevents thrashing:
- Ring cache grows as sockets return segments
- Once it exceeds 2 × ring_batch_size, half are returned to global pool
- This keeps ring cache between ring_batch_size and 2 × ring_batch_size
- Prevents constant fetching/returning cycles

**Value Tradeoffs:**

*High Values (1024, 2048, 4096) - Optimize for reduced global lock contention:*
+ Fewer global pool lock acquisitions per ring
+ Better performance when many rings are active simultaneously
+ More segments stay 'local' to each ring
+ Higher return threshold means more buffer against spikes
+ Good for per_thread or per_socket ring allocation with many threads
- Higher memory consumption per ring (N segments × ~48 bytes per ring)
- More segments 'stuck' in ring caches during idle periods
- May starve other rings if one ring accumulates many segments
- With per_interface allocation (single ring), benefits are minimal

*Low Values (128, 256, 512) - Optimize for memory and segment distribution:*
+ Lower memory overhead per ring
+ Better segment sharing across rings via global pool
+ Lower return threshold - segments return to global pool sooner
+ More responsive to workload changes across rings
+ Better for memory-constrained systems
- More frequent global pool lock acquisitions
- Higher latency variance during segment fetch operations
- Global pool spinlock becomes bottleneck with many active rings

**Interaction with Ring Allocation Logic:**
The impact of this parameter depends heavily on how rings are allocated:

- **per_interface** (all connections share one ring):
  Ring batch size matters less - single ring rarely needs global pool refills
  after initial warmup. Consider lower values to save memory.

- **per_thread** (default - ring per thread):
  Moderate impact. With N threads, you have N rings each maintaining up to
  2 × ring_batch_size segments. Total memory: N × 2 × 1024 × 48 ≈ 100KB × N.
  Higher values reduce global lock contention between threads.

- **per_socket** (ring per socket):
  Significant impact! With thousands of sockets, you could have thousands
  of rings. Consider lower values (256-512) to avoid memory explosion.
  Example: 1000 sockets × 2048 segments × 48 bytes = ~94MB just for segments.

- **per_cpuid/per_core**:
  Similar to per_thread. Ring count bounded by CPU count.

**Default Value (1024) - Balanced for typical deployments:**
- Return threshold: 2048 segments per ring
- Memory overhead: ~98KB per ring (2048 × 48 bytes)
- Fetches from global pool roughly every 16 socket refills (1024/64)
- Good balance between lock reduction and memory efficiency

**Tuning Recommendations:**

- **High-throughput, few threads (1-4):** Default 1024 works well.
  Low ring count means memory isn't a concern; focus on throughput.

- **Many threads (32+) with per_thread allocation:**
  Consider reducing to 512 to limit total memory.
  32 threads × 2 × 1024 × 48 = ~3MB vs ~1.5MB with 512.

- **Per-socket allocation with many connections:**
  Use 256-512. Memory scales linearly with connection count.
  With 10K connections: 10K × 2 × 256 × 48 = ~235MB.

- **Per-interface allocation:**
  Can use lower values (256-512) since there's only one ring.
  Memory savings are minimal but no downside.

- **Worker thread mode:**
  Rings are shared more broadly; higher values may help reduce
  global pool contention between worker threads.

- **Memory-constrained systems:**
  Lower to 256-512. Tradeoff is more global lock contention.

**Related Parameters:**
- tcp_segments.socket_batch_size: How many segments socket fetches from ring
- tcp_segments.pool_batch_size: Allocation granularity in global pool
- performance.rings.rx.allocation_logic: Determines how many rings exist
Default value is 1024

performance.buffers.tcp_segments.socket_batch_size
Maps to **XLIO_TX_SEGS_BATCH_TCP** environment variable.

**What This Does:**
Controls how many TCP segment structures (tcp_seg) a socket fetches at once
from the ring's segment cache. Each TCP connection maintains a local cache
of pre-allocated segment structures to reduce lock contention when sending data.

**What is a tcp_seg?**
A tcp_seg is a lightweight control structure (~48 bytes) used by the TCP stack
to track outgoing data segments. Each tcp_seg contains:
- Pointers to the TCP header and packet buffer (pbuf)
- Sequence number and segment length
- Flags for TCP options (MSS, timestamps, window scaling, TSO, zerocopy)
Note: tcp_seg is NOT the data buffer itself - it's metadata that describes
a TCP segment for transmission, retransmission tracking, and acknowledgment handling.

**Three-Level Caching Architecture:**
XLIO uses a hierarchical caching system for tcp_seg allocation:
1. **Global Pool** (g_tcp_seg_pool): Central pool protected by a spinlock.
   Allocates new segments in large batches (pool_batch_size, default 16384).
2. **Ring Cache** (per-ring): Each ring maintains a local cache, fetching
   from the global pool in ring_batch_size batches (default 1024).
   Protected by a per-ring lock.
3. **Socket Cache** (per-connection): Each TCP connection has its own cache,
   fetching from the ring in socket_batch_size batches (this parameter).
   NO LOCK NEEDED - single-threaded access within a connection.

**How Socket-Level Batching Works:**
1. When sending TCP data, the socket's tcp_write() needs a tcp_seg structure
2. Socket first checks its local cache (m_tcp_seg_list)
3. If empty, it acquires the ring lock and fetches 'socket_batch_size' segments
4. One segment is used immediately, the rest remain in the socket's local cache
5. After transmission completes, segments are returned to the socket's cache
6. Excess segments are returned to the ring when:
   - Total cached > 2x socket_batch_size, AND
   - Less than half are in active use
7. When connection closes, all cached segments return to the ring

**Special Mode: Value of 1 (Direct Allocation)**
When socket_batch_size=1, XLIO uses a different code path:
- Segments are fetched one at a time directly from the ring
- No socket-level caching occurs
- Creates 'hot' segments that stay in CPU cache
- Each allocation/free touches the ring lock
- Beneficial for latency-sensitive single-stream scenarios

**Value Tradeoffs:**

*High Values (64, 128, 256) - Optimize for throughput and multi-stream:*
+ Dramatically fewer ring lock acquisitions per connection
+ Better sustained throughput for streaming workloads
+ Lower CPU overhead from lock contention with many concurrent connections
+ Amortized allocation cost over many send operations
+ Segments stay 'warm' in the socket's working set
- Higher memory consumption per connection (N segments × ~48 bytes)
- Segments stay in connection's cache even during idle periods
- May delay segment availability to other connections
- Excess segment return logic adds minor overhead

*Low Values (1, 2, 4) - Optimize for memory and CPU cache efficiency:*
+ Lower memory footprint per connection
+ Better segment sharing across connections
+ Segments more likely to be CPU cache-hot (recently used)
+ More predictable memory usage
+ Better for servers with thousands of mostly-idle connections
- More frequent ring lock acquisitions
- Higher latency variance due to more frequent fetching
- Lock contention becomes significant with many active connections

*Value of 1 - Direct allocation mode:*
Fetches one segment per operation directly from ring.
Benefits:
+ Maximum CPU cache efficiency - same segments reused rapidly
+ Minimal per-connection memory overhead
+ Predictable allocation timing
Drawbacks:
- Every send/free operation acquires ring lock
- Higher lock contention with multiple connections on same ring
- Not suitable for high-throughput multi-connection scenarios

**Default Value (64) - Balanced for typical workloads:**
With 64 segments cached per socket:
- Memory overhead: ~3KB per connection (64 × 48 bytes)
- Ring lock acquired roughly once per 64 send operations
- Good balance between throughput and memory efficiency
- Handles burst traffic without frequent refills

**Tuning Recommendations:**
- High-throughput streaming (file transfer, video): Consider 128-256
- Web servers (many connections, bursty traffic): 32-64 works well
- High-connection servers (10K+ connections): Use 8-16 to save memory
- Single high-rate stream (trading, telemetry): Try 1 for cache efficiency
- Many concurrent streams on same ring: Higher values reduce lock contention
- Memory-constrained systems: Lower values (8-32)
- Worker thread mode: Lower values may help since threads share rings

**Interaction with Ring Allocation Logic:**
- With per_thread ring allocation: each thread's connections share a ring,
  so higher socket_batch_size reduces ring lock contention between connections
- With per_socket ring allocation: each connection has its own ring,
  so socket_batch_size mainly affects memory usage
- With per_interface ring allocation: all connections share one ring,
  so higher values significantly reduce lock contention

**Related Parameters:**
- tcp_segments.ring_batch_size: How many segments ring fetches from global pool
- tcp_segments.pool_batch_size: Allocation granularity in global pool
- performance.rings.rx.allocation_logic: Determines ring sharing patterns
Default value is 64

performance.buffers.tx.buf_size
Maps to **XLIO_TX_BUF_SIZE** environment variable.

**What This Controls:**
Size of individual transmit data buffer elements in the global TX buffer pool (g_buffer_pool_tx).
Each buffer holds outgoing packet payload data. The actual allocated size per buffer is
`buf_size + 92 bytes` (92 bytes reserved for L2/L3/L4 headers: Ethernet, IP, TCP/UDP).

**Value Range:**
- Minimum: Must be greater than TCP MSS (Maximum Segment Size), otherwise reset to 0 (auto)
- Maximum: 256KB (262144 bytes)
- Default: 0 (auto-calculated)

**Auto-Calculation (value = 0):**
When set to 0, XLIO calculates the optimal buffer size as:
`buf_size = TCP_MSS` (derived from MTU - 40 bytes for IP+TCP headers)

For standard 1500-byte MTU: buf_size ≈ 1460 bytes
For jumbo frames (9000-byte MTU): buf_size ≈ 8960 bytes

**Memory Consumption:**
Total TX buffer pool memory ≈ `num_buffers × (buf_size + 92 + ~128 bytes descriptor overhead)`

Initial pool size = `performance.rings.tx.tcp_buffer_batch × 1024` buffers

Examples:
- Default (1460 bytes), batch=16: ~27 MB initial pool
- 64KB buffers, batch=16: ~1.1 GB initial pool
- 256KB buffers, batch=1: ~262 MB initial pool (worker threads mode)

**TSO (TCP Segmentation Offload) Interaction:**
This parameter directly affects TSO efficiency. When TSO is enabled:
- `tso.max_buf_sz = min(buf_size, max_tso_size)`
- LWIP uses `max_buf_sz` to determine how much data to pack into a single TCP segment
- Larger buffers allow TSO to create larger segments (up to the hardware's max_payload_sz)
- The hardware NIC then splits these large segments into MTU-sized packets

**Performance Impact - LOW Values (0/auto, ~1.5KB for standard MTU):**

| Aspect | Impact |
|--------|--------|
| Memory footprint | LOWEST - minimal per-buffer overhead |
| TSO efficiency | LIMITED - segments capped at ~MSS size |
| Buffer allocations | MORE FREQUENT - many small buffers needed for large sends |
| CPU overhead | HIGHER - more segments to create, more syscalls |
| Latency (small msgs) | GOOD - buffers match typical packet sizes |
| Throughput (bulk) | LOWER - TSO cannot aggregate effectively |
| Best for | Latency-sensitive apps with small messages, memory-constrained environments |

**Performance Impact - HIGH Values (64KB - 256KB):**

| Aspect | Impact |
|--------|--------|
| Memory footprint | HIGHER - large per-buffer allocation |
| TSO efficiency | MAXIMUM - full hardware TSO utilization |
| Buffer allocations | FEWER - one buffer can hold large payload |
| CPU overhead | LOWER - fewer segments, less processing |
| Latency (small msgs) | SLIGHTLY HIGHER - potential memory waste for small data |
| Throughput (bulk) | HIGHEST - TSO creates optimal large segments |
| Best for | Streaming, bulk transfers, high-bandwidth applications |

**Special Behaviors:**

1. **Worker Threads Mode:** When `performance.threading.worker_threads > 0`,
   XLIO automatically sets `buf_size = 256KB` for optimal throughput in
   the asynchronous processing model.

2. **TSO Disabled:** When TSO is disabled, large buffer sizes provide less benefit
   since segments are still limited to MSS. However, fewer buffer allocations
   may still reduce overhead for applications sending large payloads.

3. **Zero-Copy TX:** For zero-copy transmissions (PBUF_ZEROCOPY), this parameter
   does not affect the data buffers since data is sent directly from application memory.
   It only affects the descriptor pool sizing.

**Tuning Recommendations:**

*For latency-sensitive applications (trading, real-time):*
- Use default (0) or small values matching your typical message sizes
- Smaller buffers = faster allocation, less memory waste

*For throughput-oriented applications (streaming, file transfer):*
- Use 64KB-256KB with TSO enabled
- Enables hardware to batch many packets efficiently
- Reduces CPU overhead significantly at high bandwidth

*For mixed workloads:*
- Consider 16KB-32KB as a balanced middle ground
- Provides reasonable TSO benefit without excessive memory

*For memory-constrained environments:*
- Keep default (0) to minimize buffer pool memory
- Ensure core.resources.memory_limit is appropriately sized

**Interaction with Other Parameters:**
- `hardware_features.offloads.tso.enable`: TSO must be enabled to benefit from large buffers
- `hardware_features.offloads.tso.max_size`: Upper bound on TSO segment size
- `performance.rings.tx.tcp_buffer_batch`: Affects how many buffers are fetched at once
- `core.resources.memory_limit`: Large buf_size increases memory consumption
- `performance.threading.worker_threads`: Automatically overrides to 256KB when > 0

Supports suffixes: B, KB, MB, GB.
Default value is 0

performance.buffers.tx.prefetch_size
Maps to **XLIO_TX_PREFETCH_BYTES** environment variable.

**What This Controls:**
Number of bytes to prefetch into CPU L1 cache before writing packet data to TX buffers.
This optimization targets the UDP fast path where packet headers and payload are copied
to transmit buffers allocated from XLIO's buffer pool.

**How It Works:**
When XLIO prepares a UDP packet for transmission:
1. A TX buffer is obtained from the buffer pool (may be "cold" - not in CPU cache)
2. If prefetch_size > 0, XLIO issues CPU prefetch instructions for the buffer
3. Then headers (L2/IP/UDP) are copied, followed by user payload (memcpy_fromiovec)
4. The prefetch brings buffer memory into L1 cache BEFORE the write operations

**CPU Cache Mechanics:**
- Uses hardware prefetch instructions: `prefetcht0` (x86), `dcbt` (PPC64), `prfm` (ARM64)
- Issues one prefetch instruction per L1 cache line (64 bytes on x86/ARM64, 128 bytes on PPC64)
- Example: 256 bytes = 4 prefetch instructions on x86 (256 ÷ 64 = 4 cache lines)
- Actual prefetch size = min(prefetch_size, payload_size)

**Value Range:** 0 to MTU size (typically 0-1500 bytes, or up to 9000 for jumbo frames)

**Performance Impact - Value of 0 (Disabled):**

| Aspect | Impact |
|--------|--------|
| Prefetch overhead | NONE - no prefetch instructions issued |
| Cache miss risk | HIGHER - buffer may not be in cache when writes begin |
| Write stalls | POSSIBLE - CPU may stall waiting for memory during copy |
| CPU cycles | Fewer for prefetch, but potentially more stall cycles |
| Best for | Systems where prefetch is counterproductive, benchmarking baseline |

**Performance Impact - LOW Values (64-256 bytes, default=256):**

| Aspect | Impact |
|--------|--------|
| Prefetch overhead | MINIMAL - 1-4 prefetch instructions per packet |
| Cache warming | PARTIAL - covers headers + small payload portion |
| Instruction count | LOW - minimal impact on instruction cache |
| Cache pollution | MINIMAL - prefetches only what's likely to be used soon |
| Best for | Small packets, latency-sensitive applications, mixed workloads |

**Performance Impact - HIGH Values (512 bytes to MTU):**

| Aspect | Impact |
|--------|--------|
| Prefetch overhead | HIGHER - 8-24 prefetch instructions per packet (for 512-1500 bytes) |
| Cache warming | MAXIMUM - entire packet buffer pre-warmed in cache |
| Instruction count | HIGHER - more CPU cycles spent on prefetch loop |
| Cache pollution | POSSIBLE - may evict other useful data from L1 cache |
| Write efficiency | BEST - all writes hit L1 cache, no stalls |
| Best for | Large packets, throughput-focused applications, bulk transfers |

**When Prefetch Helps Most:**
- TX buffers are "cold" (recently allocated from pool, not in cache)
- Large payloads where multiple cache lines need to be written
- Systems with high memory latency (multi-socket NUMA)
- Applications sending bursts after idle periods

**When Prefetch May Hurt:**
- Very small packets (< 64 bytes payload) - overhead exceeds benefit
- Extremely high PPS where prefetch instruction overhead accumulates
- L1 cache is heavily contested by other operations
- TX buffers are already "warm" from recent reuse

**Architecture-Specific Considerations:**

*x86/x86_64:*
- L1 cache line: 64 bytes
- prefetcht0 instruction brings data to all cache levels
- Default 256 bytes = 4 prefetch instructions
- Modern CPUs have good hardware prefetchers that may reduce benefit

*ARM64 (aarch64):*
- L1 cache line: 64 bytes (typical)
- Uses prfm pldl1keep instruction
- Similar behavior to x86

*PPC64:*
- L1 cache line: 128 bytes
- Uses dcbt instruction
- Default 256 bytes = 2 prefetch instructions
- Larger cache lines mean fewer instructions needed

**Tuning Recommendations:**

*For latency-sensitive applications (trading, gaming, real-time):*
- Start with default (256 bytes)
- If sending mostly small messages (< 256 bytes), try reducing to 64-128
- If latency variance is high, try increasing to match typical message size

*For throughput-oriented applications (streaming, bulk transfer):*
- Try values matching your typical payload size (512-1500 bytes)
- For jumbo frames, consider values up to 4096-8192 bytes
- Monitor CPU utilization - if prefetch overhead is visible, reduce

*For high PPS applications (millions of packets/sec):*
- Consider reducing to 64-128 bytes or disabling (0)
- At very high PPS, prefetch instruction overhead can accumulate
- Profile with perf to measure actual cache miss rates

*For NUMA systems:*
- Prefetching is more valuable when buffers may be on remote NUMA node
- Higher values (512+) may show more benefit
- Combine with proper NUMA pinning for best results

**Interaction with Other Parameters:**
- Only affects non-inline sends (payload > ~200 bytes for UDP)
- Inline sends bypass TX buffer copy entirely, making prefetch irrelevant
- Used in conjunction with `performance.rings.tx.max_inline_size`
- Does not affect TCP path (TCP uses different buffer management)

**Profiling Tips:**
- Use `perf stat -e cache-misses,cache-references` to measure L1 miss rate
- Compare with prefetch disabled (0) vs various values
- Look for reduced cache-miss percentage with appropriate prefetch size
- Monitor overall CPU utilization for prefetch overhead
Default value is 256

performance.completion_queue.interrupt_moderation.adaptive_change_frequency_msec
Maps to **XLIO_CQ_AIM_INTERVAL_MSEC** environment variable.
Interval in milliseconds between adaptive moderation recalculations.

**What This Controls:**
How frequently XLIO analyzes traffic patterns and adjusts moderation parameters.
A periodic timer triggers the adapt_cq_moderation() function at this interval.

**Low Values (100-500 ms):**
- Faster adaptation to traffic changes
- Better response to bursty/variable workloads
- Slightly higher overhead from frequent calculations
- Better for: dynamic workloads, traffic with varying patterns
- Example: value=250 adapts 4 times per second

**High Values (1000-5000 ms):**
- Slower adaptation, more stable settings
- Less overhead, but may miss short traffic pattern changes
- Better for: steady-state workloads, predictable traffic
- Example: value=2000 adapts every 2 seconds

**Default (1000 ms = 1 second):**
Balanced frequency for most workloads.

**Value of 0:**
Completely disables adaptive interrupt moderation.
Moderation uses static packet_count and period_usec values.
Use this when you want fully manual control.

**Interaction with Traffic Patterns:**
- Bursty traffic: Use lower values (250-500ms) for faster response
- Steady traffic: Higher values (1000-2000ms) reduce overhead
- Short-lived connections: May complete before adaptation takes effect

**Note on Warmup:**
New connections start with packet_count/period_usec defaults.
If connections are shorter than this interval, they never benefit from adaptation.
For short-lived connections, consider tuning the static defaults instead.
Default value is 1000

performance.completion_queue.interrupt_moderation.adaptive_count
Maps to **XLIO_CQ_AIM_MAX_COUNT** environment variable.
Maximum packet count threshold the adaptive algorithm can set.

**What This Controls:**
The adaptive interrupt moderation (AIM) algorithm dynamically adjusts moderation
parameters based on observed traffic. This value caps how high the count can go.

**How Adaptive Moderation Works:**
Every adaptive_change_frequency_msec (default: 1000ms), XLIO:
1. Measures packets received in the interval
2. Calculates the packet rate (packets/second)
3. Computes count = packet_rate / adaptive_interrupt_per_sec
4. Clamps count to [1, adaptive_count] range
5. Updates hardware CQ moderation settings

**Low Values (50-200):**
- Limits maximum batching
- More frequent interrupts even under heavy traffic
- Better latency consistency at high traffic rates
- Higher CPU usage under load
- Better for: mixed workloads, latency-sensitive applications

**High Values (500-1000):**
- Allows aggressive batching under heavy traffic
- Fewer interrupts during traffic bursts
- Better CPU efficiency for bulk transfers
- Risk: higher latency spikes during traffic bursts
- Better for: throughput-oriented workloads, streaming, bulk transfers

**Default (500):**
Allows substantial batching while keeping burst latency bounded.

**Maximum Value:**
Automatically capped at half of RX work queue size.

**Interaction with adaptive_interrupt_per_sec:**
count = packet_rate / target_interrupt_rate
Higher adaptive_interrupt_per_sec → lower count values
Higher adaptive_count → allows count to grow during traffic bursts
Default value is 500

performance.completion_queue.interrupt_moderation.adaptive_interrupt_per_sec
Maps to **XLIO_CQ_AIM_INTERRUPTS_RATE_PER_SEC** environment variable.
Target interrupt rate per second that the adaptive algorithm tries to achieve.
This is the primary tuning knob for balancing latency vs CPU efficiency.

**How It Works:**
The adaptive algorithm calculates: count = packet_rate / this_value
Example: At 1M packets/sec with target 10000 interrupts/sec → count = 100 packets/interrupt

**Low Values (1000-5000 interrupts/sec):**
- Fewer interrupts per second
- Higher latency (packets batched longer)
- Better CPU efficiency (15-25% lower CPU usage)
- Better for: throughput-oriented workloads, bulk transfers, streaming
- Tradeoff: May cause 100-500µs additional latency
- Example: value=2000 targets one interrupt every 500µs

**High Values (10000-50000 interrupts/sec):**
- More interrupts per second
- Lower latency (smaller batches, faster processing)
- Higher CPU usage (more interrupt overhead)
- Better for: latency-sensitive workloads, request/response patterns
- Example: value=20000 targets one interrupt every 50µs

**Default (10000 interrupts/sec):**
Targets ~100µs between interrupts, balancing latency and efficiency.

**Performance Impact (from benchmarks):**
| Target Rate | Throughput | CPU Usage | Use Case |
|-------------|------------|-----------|----------|
| 1000 | Baseline | Lowest | Bulk transfers |
| 5000 | +5% | Low | Balanced |
| 10000 | +10% | Moderate | General purpose |
| 20000 | +15% | Higher | Latency-sensitive |
| Disabled | +18% | Highest | Ultra-low-latency |

**Workload-Specific Recommendations:**
- **Nginx/HAProxy (small files <1KB):** 15000-20000 or disable
- **Streaming/CDN (large files):** 2000-5000
- **Database (mixed):** 10000 (default)
- **HFT/Trading:** Disable moderation entirely

**Interaction with adaptive_count:**
The calculated count is clamped to adaptive_count maximum.
If packet_rate / target_rate > adaptive_count, actual interrupt rate will be lower than target.

**Formula:**
actual_count = min(packet_rate / target_rate, adaptive_count)
actual_interrupt_rate ≈ packet_rate / actual_count
Default value is 10000

performance.completion_queue.interrupt_moderation.adaptive_period_usec
Maps to **XLIO_CQ_AIM_MAX_PERIOD_USEC** environment variable.
Maximum period (in microseconds) the adaptive algorithm can set.

**What This Controls:**
Caps the maximum hold time the adaptive algorithm will configure.
Prevents the algorithm from setting periods so long that latency becomes unacceptable.

**How It's Used:**
The adaptive algorithm calculates:
period = (1,000,000 / target_interrupt_rate) - (1,000,000 / packet_rate)
This is then clamped to [0, adaptive_period_usec].

**Low Values (100-500 µsec):**
- Maximum latency bounded to sub-millisecond
- More predictable latency under varying traffic
- May cause unnecessary interrupts under moderate traffic
- Better for: latency-sensitive applications, real-time systems

**High Values (1000-5000 µsec):**
- Allows up to 1-5ms batching window
- Better CPU efficiency for bursty workloads
- Risk: occasional latency spikes of several milliseconds
- Better for: batch processing, non-real-time throughput workloads

**Default (1000 µsec = 1ms):**
Allows adaptive algorithm flexibility while keeping worst-case latency under 1ms.

**Latency Implications:**
This value represents the worst-case additional latency the adaptive algorithm
can introduce. Under steady traffic, actual latency is typically much lower
as count thresholds are reached before period expiry.
Default value is 1000

performance.completion_queue.interrupt_moderation.enable
Maps to **XLIO_CQ_MODERATION_ENABLE** environment variable.
Master switch for Completion Queue (CQ) interrupt moderation.

**What This Controls:**
When enabled, the NIC hardware batches completion notifications and generates
interrupts based on count/period thresholds rather than per-packet.

**When true (default):**
- Hardware coalesces interrupts using packet_count and period_usec thresholds
- Adaptive algorithm (if enabled) dynamically adjusts these thresholds
- Reduces CPU interrupt overhead at the cost of some latency
- Best for: throughput-oriented workloads, large transfers, CPU-constrained systems

**When false:**
- Each packet completion immediately triggers an interrupt
- Lowest possible latency - packets processed as soon as they arrive
- Higher CPU utilization due to interrupt processing overhead
- Best for: ultra-low-latency requirements, HFT, small request/response workloads

**Performance Tradeoffs:**
- Enabled: Better CPU efficiency, ~15-20% lower CPU usage, but 50-1000µs added latency
- Disabled: Lowest latency, but up to 20% higher CPU usage

**Note:** Setting either performance.polling.blocking_rx_poll_usec or performance.polling.iomux.poll_usec to infinite (-1) will automatically disable moderation, as the application is already polling continuously.
Default value is true

performance.completion_queue.interrupt_moderation.packet_count
Maps to **XLIO_CQ_MODERATION_COUNT** environment variable.
Number of packet completions to accumulate before generating an interrupt.
This is the initial value used when moderation is first enabled, and the fallback value
when adaptive moderation detects no traffic.

**How It Works:**
The NIC generates an interrupt when EITHER this count is reached OR the period_usec expires,
whichever comes first.

**Low Values (1-16):**
- Interrupts trigger more frequently
- Lower latency - packets processed sooner
- Higher CPU overhead - more interrupt processing
- Better for: latency-sensitive workloads, small packets, request/response patterns
- Example: value=4 means interrupt after every 4 packets (or period timeout)

**High Values (64-500):**
- Interrupts trigger less frequently
- Higher latency - packets wait longer before processing
- Lower CPU overhead - better batching efficiency
- Better for: high-throughput bulk transfers, large file downloads, streaming
- Example: value=256 means batch up to 256 packets per interrupt

**Default (48):**
Balanced starting point, but adaptive algorithm will adjust based on traffic.

**Interaction with period_usec:**
Interrupt triggers on FIRST condition met:
- If count reached before period: interrupt on count
- If period expires before count: interrupt on period

**Maximum Value:**
Automatically capped at half of RX work queue size to prevent queue overflow.
Default value is 48

performance.completion_queue.interrupt_moderation.period_usec
Maps to **XLIO_CQ_MODERATION_PERIOD_USEC** environment variable.
Maximum time in microseconds to hold a packet before generating an interrupt.
This is the initial value used when moderation is first enabled, and the fallback value
when adaptive moderation detects no traffic.

**How It Works:**
The NIC generates an interrupt when EITHER this period expires OR the packet_count threshold
is reached, whichever comes first. This ensures packets are never held indefinitely.

**Low Values (10-50 µsec):**
- Maximum wait time of 10-50 microseconds
- Ensures timely processing even under light traffic
- Higher interrupt rate under sparse traffic
- Better for: latency-sensitive applications, trading, gaming
- Example: value=25 means packets wait at most 25µs

**High Values (100-1000 µsec):**
- Allows packets to accumulate for up to 1ms
- Better batching under moderate traffic
- Lower interrupt rate, better CPU efficiency
- Risk: noticeable latency under light/bursty traffic
- Better for: bulk transfers, streaming, high-throughput scenarios
- Example: value=500 means packets may wait up to 500µs (0.5ms)

**Default (50 µsec):**
Provides sub-100µs worst-case latency while allowing some batching.

**Relationship to Latency:**
This value represents the MAXIMUM additional latency introduced by moderation
under light traffic conditions. Under heavy traffic, packet_count is usually
reached first, so actual latency is lower.

**Interaction with packet_count:**
The period acts as a safety net - even if traffic is too light to reach packet_count,
an interrupt is guaranteed within this period.

**For Ultra-Low-Latency:**
Consider disabling moderation entirely (enable=false) rather than using very low values,
as even small periods add measurable latency.
Default value is 50

performance.completion_queue.keep_full
Maps to **XLIO_CQ_KEEP_QP_FULL** environment variable.

**What This Controls:**
Determines how XLIO handles buffer shortages when replenishing the hardware Receive Queue (RQ) after processing received packets.

**Background - The Buffer Flow:**
1. Hardware NIC receives packets into buffers posted to the Receive Queue (RQ)
2. CQ Manager polls for completions and processes received packets
3. After processing, XLIO must replenish the RQ with fresh buffers
4. Buffers come from the local ring cache (rx_pool) or global buffer pool

**The Debt Mechanism:**
XLIO tracks 'm_debt' - the count of buffers consumed from RQ that haven't been replenished yet. When buffers become available, the debt is paid back by posting new buffers to the RQ.

**When true (Aggressive Replenishment - DEFAULT):**
- After EVERY packet completion, XLIO immediately tries to replenish the RQ
- If no buffers are available from local pool OR global pool:
  - Re-posts the SAME buffer back to RQ (the packet data is DROPPED)
  - Increments n_rx_sw_pkt_drops counter (visible in xlio_stats)
- The RQ is ALWAYS kept at full capacity (maximum WREs)

**When false (Deferred Replenishment):**
- XLIO accumulates debt rather than forcing immediate replenishment
- Debt is paid back later when buffers become available or during poll_failed path
- Packets are NOT dropped unless debt reaches maximum QP size (m_debt >= rx_num_wr)
- Compensation is batched, reducing per-packet CPU overhead

**Performance Tradeoffs:**

*true (Aggressive):*
- PROS:
  - QP always has maximum capacity to receive packets
  - Better for bursty workloads where max receive capacity is critical
  - Minimizes risk of hardware-level RQ starvation
- CONS:
  - Drops packets when buffers are scarce (visible in xlio_stats)
  - Higher CPU overhead per packet (compensation runs on every poll)
  - Can cause application-visible packet loss under memory pressure

*false (Deferred):*
- PROS:
  - Lower CPU overhead - compensation is batched
  - No packet drops unless QP completely drains
  - Lower latency jitter (less work per packet)
  - Better for latency-sensitive applications
- CONS:
  - QP may temporarily have fewer available WREs
  - Could lead to HW packet drops if sustained traffic exceeds replenishment rate
  - Requires careful sizing of spare_buffers and rx_num_wr

**Recommended Settings:**
- High-frequency trading / ultra-low latency: false
- Throughput-focused bulk transfer: true
- Memory-constrained environments: false (avoids panic-mode drops)
- Bursty traffic patterns: true (ensures maximum receive capacity)

**Interaction with Related Parameters:**
- performance.rings.rx.spare_buffers: Larger values reduce frequency of global pool access, mitigating keep_full=true drops
- performance.rings.rx.post_batch_size: Controls batching threshold; larger values work better with keep_full=false
- memory.buffers.rx_pool_size: Larger pools reduce buffer shortages overall

**Monitoring:**
When keep_full=true causes drops, monitor via xlio_stats:
- 'SW RX Packets dropped' counter increments for each re-posted buffer
- Use 'xlio_stats -p <pid>' to observe drop rates under load

**Note:** Latency-optimized profiles (29WEST, LATENCY, NVME_BF3) automatically set this to false.
Default value is true

performance.completion_queue.periodic_drain_max_cqes
Maps to **XLIO_PROGRESS_ENGINE_WCE_MAX** environment variable.

**What This Controls:**
Limits the maximum number of Completion Queue Entries (CQEs) processed in a SINGLE periodic drain operation by XLIO's internal thread. This parameter works together with periodic_drain_msec to control background CQ draining behavior.

**Important: Application Path is NOT Affected**
This limit ONLY applies to the internal thread's drain_and_proccess() function. Application socket API calls (recv, recvfrom, poll, select, epoll_wait) use poll_and_process_element_rx() which has its own separate limit controlled by performance.polling.max_rx_poll_batch (default 16). Applications are never restricted by this parameter.

**How the Drain Mechanism Works:**
1. Timer fires every periodic_drain_msec milliseconds
2. Internal thread iterates through ALL network devices and their rings
3. For each ring, acquires ring lock (trylock - non-blocking)
4. Polls CQ in a loop until EITHER:
   - CQ is empty (no more completions available), OR
   - Number of processed CQEs reaches periodic_drain_max_cqes limit
5. Releases ring lock, moves to next ring

The polling loop logic:
```
while (progress_engine_wce_max > ret_total) {
    buff = poll(status);  // Get next CQE from hardware
    if (!buff) break;     // CQ empty, exit loop
    process_completion(buff);
    ret_total++;
}
```

**Value Tradeoffs:**

*High Values (5000-10000+, Default is 10000):*
- PROS:
  - Drains large packet backlogs in a single timer fire
  - Efficient for throughput workloads - fewer wakeups needed
  - Better TCP progress when app is busy (ACKs, window updates processed)
  - Reduces number of incomplete drains requiring another timer interval
- CONS:
  - Internal thread may hold ring lock for extended periods (milliseconds under heavy load)
  - Can cause latency spikes for application threads waiting for ring lock
  - Higher CPU burst when processing large batches
  - Less predictable timing behavior

*Moderate Values (500-2000):*
- PROS:
  - Balanced approach between throughput and latency
  - Ring lock held for shorter periods
  - More predictable CPU usage patterns
- CONS:
  - May require multiple timer fires to clear large backlogs
  - Slightly reduced efficiency for high-throughput scenarios

*Low Values (50-200):*
- PROS:
  - Internal thread releases ring lock quickly
  - Minimal latency impact on application threads
  - Very predictable, bounded CPU usage per drain
  - Best for ultra-low-latency applications
- CONS:
  - Multiple timer intervals needed to clear any significant backlog
  - TCP progress may be slower under heavy load
  - More timer overhead (frequent wakeups with little work done)

*Value of 0 (Disables Periodic Draining):*
- When set to 0, the periodic drain timer is NOT registered at all
- Same effect as setting periodic_drain_msec to 0
- Application MUST provide all execution context via socket API calls
- Suitable only for applications with guaranteed frequent polling

**Recommended Settings by Workload:**

- **General purpose / Default**: 10000
  Works well for most scenarios, ensures backlogs are cleared efficiently.

- **High-frequency trading / Ultra-low latency**: 100-500
  Minimizes lock hold time, reduces jitter from internal thread.
  Pair with periodic_drain_msec=0 if app already provides frequent context.

- **Throughput-focused bulk transfer**: 10000-50000
  Maximize work per wakeup, minimize timer overhead.

- **Mixed workload with latency sensitivity**: 1000-2000
  Balance between clearing backlogs and limiting lock contention.

- **Event-driven servers (nginx, HAProxy)**: 0 or 500
  Workers have their own polling loops; internal thread should be minimal.

**Interaction with Related Parameters:**

- **periodic_drain_msec**: These two work as a pair. High interval with low max_cqes means infrequent but limited drains. Low interval with high max_cqes means frequent drains that can process more. Setting either to 0 disables periodic draining entirely.

- **performance.polling.max_rx_poll_batch**: Controls application-path CQ polling limit (default 16). This is the limit that matters for application latency, NOT periodic_drain_max_cqes.

- **performance.threading.worker_threads**: In Worker Threads mode, periodic draining is automatically disabled - the worker threads provide continuous context.

**Monitoring:**
Use xlio_stats to observe drain behavior:
- 'Drained max' shows the maximum CQEs processed in a single drain operation
- If 'Drained max' consistently equals periodic_drain_max_cqes, consider increasing the limit
- If 'Drained max' is always very low, the limit may be unnecessarily high

**Performance Impact Analysis:**

Lock hold time calculation (approximate):
- Processing one CQE takes roughly 100-500 nanoseconds
- With periodic_drain_max_cqes=10000, worst-case lock hold: 1-5 milliseconds
- With periodic_drain_max_cqes=100, worst-case lock hold: 10-50 microseconds

For applications where every microsecond matters, reducing this value trades throughput efficiency for latency predictability.
Default value is 10000

performance.completion_queue.periodic_drain_msec
Maps to **XLIO_PROGRESS_ENGINE_INTERVAL** environment variable.

**What This Controls:**
The interval in milliseconds at which XLIO's internal thread periodically polls all Completion Queues (CQs) to drain received packets, independent of application socket API calls.

**Background - Why This Matters:**
XLIO operates as a user-space TCP/IP stack that requires execution context to make progress. Normally, the application provides this context by calling socket APIs (recv, send, poll, etc.). However, when the application is:
- Busy with computation and not calling socket APIs
- Blocked on non-socket operations
- Processing other work in a single-threaded model

...the TCP stack cannot progress without an external trigger. This parameter provides that trigger through XLIO's internal thread.

**What Happens During Periodic Drain:**
1. Internal thread wakes up every N milliseconds
2. Iterates through ALL network devices and their rings
3. For each ring, attempts to acquire the ring lock (trylock - non-blocking)
4. If lock acquired, polls the CQ and processes completions:
   - TCP packets: Processed immediately through the TCP stack (ACK handling, window updates, data delivery to socket buffers)
   - UDP packets: Queued for later application retrieval
5. Replenishes QP buffers if needed
6. Releases lock and moves to next ring

**TCP-Specific Impact:**
The periodic drain is CRITICAL for TCP because:
- **ACK Processing**: Incoming ACKs must be processed to advance the send window and allow more data transmission
- **Retransmission Timers**: Delayed ACK detection and retransmit triggers depend on stack progress
- **Window Updates**: Receive window advertisements require processing incoming data
- **Keepalive/Timers**: Various TCP timers need the stack to progress

Without periodic drain (and without app-driven polling), TCP connections can stall, exhibit poor throughput, or timeout.

**Value Tradeoffs:**

*Low Values (1-10 ms):*
- PROS:
  - Fast TCP progress even when app is busy
  - Lower retransmission latency (problems detected sooner)
  - Better throughput for apps with irregular socket access patterns
  - More responsive ACK processing
- CONS:
  - Higher CPU overhead from internal thread
  - More frequent lock contention with application threads
  - Potential latency jitter if internal thread preempts app

*High Values (50-100+ ms):*
- PROS:
  - Lower CPU utilization by internal thread
  - Less lock contention with application
  - Suitable when app provides regular execution context
- CONS:
  - TCP may stall if app doesn't call socket APIs frequently
  - Longer time to detect and retransmit lost packets
  - Throughput degradation for bursty/irregular access patterns
  - Delayed ACK processing can hurt sender's throughput

*Disabled (0):*
- PROS:
  - Zero CPU overhead from background CQ polling
  - No lock contention from internal thread
  - Maximum determinism for latency-critical apps
- CONS:
  - TCP ONLY progresses when app calls socket APIs
  - App MUST poll sockets frequently (busy-poll or tight event loop)
  - Unsuitable for apps with irregular socket access

**Recommended Settings:**

- **General purpose / Default**: 10 ms (balance of responsiveness and overhead)
- **Latency-sensitive with frequent polling**: 50-100 ms (app already provides context)
- **Throughput-focused bulk transfer**: 5-10 ms (ensures ACKs are processed promptly)
- **Single-threaded ultra-low-latency**: 0 (disabled) - app guarantees frequent polling
- **Event-driven servers (nginx, HAProxy)**: 0 (disabled) - workers have their own polling loops
- **Background/batch processing apps**: 10-20 ms (ensures TCP progress during computation phases)

**Profiles That Modify This:**
- ultra_latency: Sets to 0 (disabled) - assumes app provides execution context
- latency: Sets to 100 ms (reduced overhead, app expected to poll)
- nginx/nginx_dpu: Sets to 0 (disabled) - workers poll in their event loops
- nvme_bf3: Sets to 0 (disabled) - specialized storage workload
- Worker Threads mode: Automatically disabled (XLIO's worker threads provide context)

**Interaction with Related Parameters:**
- performance.threading.internal_handler.timer_msec: Base wakeup interval of internal thread. periodic_drain_msec should be >= timer_msec for predictable behavior.
- network.protocols.tcp.timer_msec: TCP timer resolution. Should be coordinated - if TCP timers are 100ms, drain interval > 100ms may delay timer processing.
- performance.completion_queue.periodic_drain_max_cqes: Limits CQEs processed per drain. Lower values reduce time spent in drain but may leave work undone.
- performance.completion_queue.keep_full: When true, periodic drain helps maintain QP fullness even when app is idle.

**Monitoring:**
Use xlio_stats to observe drain behavior:
- 'CQ Drain count' shows how often periodic drain runs
- If TCP throughput is poor with app idle periods, try lower values
- If CPU utilization is high on internal thread, try higher values or disable

**Important Note:**
If CQ was already drained by application socket API calls within the interval, the internal thread simply goes back to sleep without additional processing. This makes the mechanism safe and non-redundant - it only does work when the application hasn't.
Default value is 10

performance.completion_queue.rx_drain_rate_nsec
Maps to **XLIO_RX_CQ_DRAIN_RATE_NSEC** environment variable.

**IMPORTANT: This parameter applies ONLY to UDP sockets. TCP sockets are not affected.**

**Understanding the Receive Path Architecture:**

When packets arrive at the NIC, they flow through these stages:
1. NIC hardware receives the packet and posts a Completion Queue Entry (CQE) to the hardware CQ
2. XLIO polls the CQ and moves completed packets to the socket's Ready Packet Queue
3. Application calls recv()/recvfrom() and retrieves packets from the Ready Packet Queue

The key question this parameter answers: When the application calls recv() and there are already packets waiting in the Ready Packet Queue, should XLIO first check the CQ for newer packets, or immediately return what's already available?

**When Disabled (value = 0, Default):**

If packets exist in the Ready Packet Queue, XLIO returns them immediately WITHOUT polling the CQ.
The CQ is only polled when the Ready Packet Queue is empty.

```
is_readable() logic:
  if (ready_packet_count > 0) {
    return true;  // Immediate return, no CQ poll
  }
  poll_cq();  // Only poll when queue is empty
```

*Benefits:*
- Fastest possible latency - immediate return to application
- Minimal CPU overhead - no unnecessary CQ polling
- Predictable, deterministic behavior

*Drawbacks:*
- Application sees a potentially stale view of available data
- Newer packets may be waiting in CQ but not yet visible to the application
- In multi-threaded scenarios, one thread may not see packets that arrived during another thread's processing

**When Enabled (value > 0):**

Even when packets exist in the Ready Packet Queue, XLIO will poll the CQ if enough time has passed since the last poll. The value specifies the MINIMUM time interval (in nanoseconds) between forced CQ polls.

```
is_readable() logic:
  if (ready_packet_count > 0) {
    if (time_since_last_poll < rx_drain_rate_nsec) {
      return true;  // Too soon, skip CQ poll
    }
    // Enough time passed, poll CQ for fresh data
    update_last_poll_timestamp();
  }
  poll_cq();
```

*Benefits:*
- More accurate, 'real-time' view of available packets
- Better behavior in multi-threaded applications
- Catches packets that arrived during previous processing
- Useful for applications that need to know the 'true' queue depth

*Drawbacks:*
- Additional CPU cycles spent polling CQ even when data is available
- Slightly higher latency due to CQ polling overhead
- More variable latency (jitter) depending on CQ state

**Critical Implementation Detail: Global Timestamp**

The last-poll timestamp (g_si_tscv_last_poll) is a GLOBAL variable shared across ALL UDP sockets in the process. This means:
- If Socket A polls the CQ, it resets the timer for ALL sockets including Socket B
- With many active UDP sockets, this naturally rate-limits CQ polling across the process
- The actual polling frequency depends on overall socket activity, not just one socket

**Value Tradeoffs:**

*Lower Values (100-500 nsec):*
- More frequent CQ polling
- More accurate queue state visibility
- Higher CPU overhead
- Better for: Real-time data feeds, market data applications, multi-producer scenarios

*Moderate Values (500-2000 nsec):*
- Balanced polling frequency
- Good accuracy with reasonable CPU overhead
- Better for: General-purpose UDP applications with moderate latency requirements

*Higher Values (2000-5000 nsec):*
- Less frequent CQ polling
- Lower CPU overhead
- May miss recently-arrived packets until next poll
- Better for: Throughput-focused applications, less latency-sensitive workloads

*Disabled (0, Default):*
- No forced CQ polling when data is available
- Lowest latency, lowest CPU overhead
- Better for: Single-threaded applications, latency-critical paths, applications that drain sockets completely

**Recommended Settings by Use Case:**

- **Ultra-low latency trading (single-threaded)**: 0
  Immediate return is more important than queue accuracy.

- **Market data with multiple handlers**: 100-500
  Multiple threads need accurate view of incoming data.

- **Multi-threaded UDP server**: 500-1000
  Balance between responsiveness and thread coordination.

- **Streaming/media applications**: 1000-2000
  Moderate accuracy needs, CPU efficiency important.

- **Logging/monitoring collectors**: 2000-5000 or 0
  Either don't care about real-time accuracy, or want absolute minimum latency.

**Interaction with Related Parameters:**

- **performance.polling.blocking_rx_poll_usec**: Controls how long to spin-poll before sleeping. rx_drain_rate_nsec affects EACH poll iteration within that spinning.

- **performance.polling.rx_kernel_fd_attention_level**: Controls ratio of CQ polls to OS fd polls. rx_drain_rate_nsec is checked on every is_readable() call regardless of this ratio.

- **performance.completion_queue.periodic_drain_msec**: The internal thread's drain interval. This is independent of rx_drain_rate_nsec which only affects application-thread polling.

**Performance Impact Analysis:**

Typical CQ poll overhead: 50-200 nanoseconds (when CQ is empty)
With rx_drain_rate_nsec=100, worst case: one extra 50-200ns poll per recv() call
With rx_drain_rate_nsec=5000, extra poll at most every 5 microseconds of activity

For applications making millions of recv() calls per second, even small values can add measurable CPU overhead. For applications with lower call rates, the impact is negligible.

**When to Enable This Parameter:**

1. Multi-threaded UDP applications where threads share socket access
2. Applications that need accurate queue depth information
3. Scenarios where packets arrive in bursts and you need to see all of them quickly
4. Debugging/monitoring where you need to verify packet arrival timing

**When to Keep Disabled (0):**

1. Single-threaded applications with exclusive socket access
2. Ultra-low latency requirements where every nanosecond counts
3. Applications that always drain sockets completely before returning
4. High-frequency recv() patterns where CQ will be naturally polled often

Recommended value range: 100-5000 (nsec) when enabled.
Default value is 0

performance.max_gro_streams
Maps to **XLIO_GRO_STREAMS_MAX** environment variable.
Controls the maximum number of TCP flows that can perform Generic Receive Offload (GRO)
simultaneously per ring. GRO aggregates multiple consecutive TCP segments from the
same flow into larger packets before delivering them to the application.

**What is GRO (Generic Receive Offload)?**

GRO is a software technique that coalesces multiple small TCP packets from the same
connection into fewer, larger packets. When packets arrive with consecutive sequence
numbers, identical TCP flags, and compatible timestamps, XLIO chains their payloads
together and delivers them as a single aggregated packet to the application.

This reduces:
- Per-packet processing overhead (fewer recv() callbacks)
- TCP/IP stack traversal costs
- Application context switches
- Cache pollution from processing many small packets

**How It Works Internally:**

1. Each ring maintains a gro_mgr with slots for max_gro_streams simultaneous flows
2. When a TCP flow receives its first packet, it attempts to reserve a GRO slot
3. If a slot is available, subsequent packets are aggregated (up to 32 packets or 64KB)
4. Aggregation flushes when:
   - Sequence number gap detected (out-of-order packet)
   - TCP flags change (PSH, FIN, RST, etc.)
   - Aggregation limit reached (32 packets or ~64KB)
   - Poll batch completes (end of CQ processing)
5. If no slot is available (max_gro_streams reached), the flow bypasses GRO entirely
   and packets are delivered individually (still works, just without coalescing)

**Aggregation Limits (per flow):**
- Maximum packets per aggregation: 32 (MAX_GRO_BUFS)
- Maximum bytes per aggregation: ~64KB (MAX_AGGR_BYTE_PER_STREAM minus MTU)

**Memory Overhead:**
Very low: ~8 bytes per slot (pointer) plus ~100 bytes per active flow's state.
Total per ring: max_gro_streams × ~8 bytes for the pointer array.
Example: 32 streams × 8 bytes = 256 bytes per ring (negligible).

**Value of 0 - GRO Disabled:**

When set to 0, GRO is completely disabled:
- XLIO uses rfs_uc (regular flow steering) instead of rfs_uc_tcp_gro
- Every TCP packet is delivered individually to the application
- No aggregation delay - lowest per-packet latency
- Higher CPU overhead at high packet rates (more recv() calls)
- No coalescing statistics tracked

Best for:
- Ultra-low-latency applications where every microsecond matters
- Workloads with small, latency-sensitive messages
- Debugging scenarios where packet-level visibility is needed
- Applications that process each packet immediately anyway

**Low Values (1-16) - Limited GRO:**

Benefits:
- Only a few flows benefit from aggregation
- Lower memory allocation per ring
- Suitable for applications with few concurrent TCP connections
- Most flows bypass GRO (no aggregation delay)

Drawbacks:
- High-connection-count workloads see minimal GRO benefit
- Flows that exceed the limit process packets individually
- May not justify the GRO overhead for few connections

Best for:
- Single-connection or few-connection applications
- Latency-sensitive apps with occasional bulk transfers
- Testing/benchmarking GRO impact on specific flows

**Medium Values (32-64) - Balanced (Default: 32):**

Benefits:
- Good coverage for typical server workloads (web servers, proxies)
- Most active TCP connections can benefit from GRO
- Balanced between throughput gains and resource usage
- Suitable for connection counts typical in most deployments

Drawbacks:
- Very high connection counts may still exceed the limit
- Some flows may miss GRO during connection bursts

Best for:
- Web servers handling moderate concurrent connections
- Proxy applications (NGINX, HAProxy)
- General-purpose TCP servers
- Most typical deployment scenarios

**High Values (128-512) - Maximum GRO Coverage:**

Benefits:
- More concurrent TCP flows can benefit from GRO
- Better throughput for high-connection-count workloads
- Fewer packets delivered individually
- Higher CPU efficiency at scale

Drawbacks:
- Larger pointer array allocation per ring (still small: 512 × 8 = 4KB)
- Marginal benefit if actual concurrent flows are fewer than the limit
- No benefit for UDP traffic (GRO is TCP-only)

Best for:
- High-connection-count servers (1000s of connections)
- CDN edge servers, load balancers
- Applications with many simultaneous bulk transfers
- Scenarios where connection count regularly exceeds default (32)

**Performance Impact Summary:**

| Metric                    | 0 (Disabled) | Low (1-16) | Default (32) | High (128+) |
|---------------------------|--------------|------------|--------------|-------------|
| Latency per packet        | Lowest       | Low        | Slightly higher | Slightly higher |
| Throughput (high PPS)     | Lower        | Medium     | Good         | Best        |
| CPU efficiency (high load)| Lower        | Medium     | Good         | Best        |
| recv() calls per MB       | Many         | Many       | Fewer        | Fewest      |
| Memory per ring           | None         | ~128B      | ~256B        | ~4KB        |
| Connections benefiting    | None         | Few        | Moderate     | Many        |

**GRO vs LRO (Large Receive Offload):**

- LRO (hardware_features.lro): Hardware-based aggregation performed by the NIC
- GRO (max_gro_streams): Software-based aggregation performed by XLIO
- Both can work together: LRO aggregates at hardware level, GRO provides
  additional software coalescing if hardware aggregation was incomplete
- GRO is always available; LRO depends on NIC capabilities and settings

**Interaction with Other Parameters:**

- **hardware_features.lro:** Hardware LRO reduces packets before GRO sees them.
  With strong LRO, GRO may have fewer packets to aggregate.
- **performance.polling.max_rx_poll_batch:** GRO flushes at end of each poll batch.
  Larger batches allow more packets to accumulate for aggregation.
- **Ring allocation logic:** GRO slots are per-ring. With per-thread rings,
  each thread has its own max_gro_streams budget.

**Monitoring GRO Effectiveness:**

Use xlio_stats to monitor:
- n_rx_gro_packets: Count of aggregated (coalesced) packets delivered
- n_rx_gro_frags: Total individual packets that were aggregated
- n_rx_gro_bytes: Total bytes processed through GRO
- Avg GRO packet size: n_rx_gro_bytes / n_rx_gro_packets (higher = better coalescing)
- GRO frags per packet: n_rx_gro_frags / n_rx_gro_packets (higher = more aggregation)

If n_rx_gro_frags/n_rx_gro_packets is close to 1.0, packets aren't being aggregated
(possibly out-of-order arrival, small messages, or max_gro_streams too low).

**Tuning Recommendations:**

1. **Start with default (32)** - optimal for most workloads

2. **For ultra-low-latency (HFT, real-time):**
   - Set to 0 to disable GRO entirely
   - Eliminates any aggregation-induced latency

3. **For high-throughput bulk transfer:**
   - Keep default or increase to 64-128
   - Monitor GRO statistics to verify aggregation is occurring

4. **For high connection counts (1000+):**
   - Increase to 128-256 to ensure most flows benefit
   - Check if n_rx_gro_frags >> n_rx_gro_packets (good aggregation)

5. **For memory-constrained environments:**
   - Default (32) uses only ~256 bytes per ring
   - Even high values (256) use only ~2KB per ring
   - Memory is not a significant concern for this parameter
Default value is 32

performance.override_rcvbuf_limit
Maps to **XLIO_RX_BYTES_MIN** environment variable.

**What This Does:**
Sets the minimum socket receive buffer limit (in bytes) for **UDP sockets only**.
When an application calls setsockopt(SO_RCVBUF) with a value smaller than this parameter,
XLIO overrides the requested value and uses this minimum instead.

This parameter controls the maximum total bytes of received UDP datagrams that XLIO
will hold in a socket's ready queue waiting for the application to read them.
When this limit is reached, new incoming datagrams are silently dropped.

**Note:** This parameter does NOT affect TCP sockets. TCP uses flow control via
the receive window mechanism (controlled by SO_RCVBUF directly and system tcp_rmem settings),
which prevents packet loss by signaling the sender to slow down.

**How the Receive Buffer Works:**

1. UDP packets arrive from the network and are placed in the socket's ready queue
2. XLIO tracks total bytes in queue (m_rx_ready_byte_count)
3. When bytes >= limit, new packets are dropped (returned false from rx_input_cb)
4. Dropped packets are counted in statistics (n_rx_ready_byte_drop)
5. When application calls recv()/recvfrom(), packets are removed from queue
6. Applications can increase the limit via setsockopt(SO_RCVBUF), but cannot
   decrease it below this minimum parameter value

**Relationship with SO_RCVBUF:**

When a UDP socket is created:
1. XLIO reads the kernel's default SO_RCVBUF value (typically from net.core.rmem_default)
2. The kernel value is doubled (Linux convention: kernel doubles the requested value)
3. XLIO enforces: effective_limit = max(kernel_value, override_rcvbuf_limit)

When application calls setsockopt(SO_RCVBUF, value):
1. XLIO receives value×2 (kernel doubling)
2. XLIO enforces: effective_limit = max(value×2, override_rcvbuf_limit)
3. If current queue exceeds new limit, oldest packets are dropped to fit

**Memory Impact:**

This is a per-socket limit on packet payload bytes held in the ready queue.
Actual memory consumption includes packet descriptors and headers, roughly:
  Memory per socket ≈ limit × 1.1 (10% overhead for descriptors)

With many UDP sockets, total memory = num_sockets × limit × 1.1
Example: 1000 UDP sockets × 64KB × 1.1 ≈ 70MB potential memory

**High Values (256KB - 16MB) - Better Burst Tolerance:**

Benefits:
- Absorbs traffic bursts when application temporarily cannot keep up
- Tolerates application processing jitter (GC pauses, context switches)
- Handles high-bandwidth multicast streams with variable consumption rates
- Reduces packet loss during brief application stalls
- Suitable for applications doing batch processing of received data

Drawbacks:
- Higher per-socket memory consumption
- Risk of buffering stale data that becomes irrelevant by read time
- Increases end-to-end latency (packets wait longer in buffer)
- Can mask application performance problems (appears to work, but with delay)
- Memory tied up even during idle periods

Best for:
- Multicast video/audio streaming (buffering helps smooth playback)
- Batch data collection applications
- Applications with variable processing rates
- High-bandwidth bulk UDP transfers
- Scenarios where some latency is acceptable to avoid data loss

**Low Values (4KB - 32KB) - Better for Real-Time:**

Benefits:
- Forces applications to consume data promptly or lose it
- Minimizes latency - no stale data sitting in buffers
- Lower per-socket memory footprint
- Better for real-time applications where old data is worthless
- Dropped packets signal application is too slow (useful feedback)
- Encourages application to keep up with wire speed

Drawbacks:
- Higher packet drop rate during any processing hiccup
- No tolerance for application jitter or brief stalls
- Requires application to be consistently fast
- May lose packets during legitimate short delays

Best for:
- Real-time trading/financial data (stale quotes are harmful)
- Live sensor data where only latest reading matters
- Gaming and interactive applications
- Low-latency signaling protocols
- Applications that can afford to miss some updates

**Default Value (65536 = 64KB):**

The default provides a reasonable balance:
- Handles typical network jitter and small bursts (~40-50 full-size packets)
- ~64KB per socket is modest memory overhead
- Sufficient for most general-purpose UDP applications
- Not so large that it masks performance issues
- Matches common system defaults for socket buffers

**Value of 0 - No Minimum Enforcement:**

When set to 0, XLIO does not enforce a minimum:
- Application's setsockopt(SO_RCVBUF) is used directly
- Application can set arbitrarily small buffers
- Useful when application explicitly wants minimal buffering
- Risk: Application might accidentally set too small a value

**Performance Tradeoff Summary:**

| Value Range  | Packet Loss  | Latency      | Memory    | Best Use Case            |
|--------------|--------------|--------------|-----------|--------------------------||
| 0            | App decides  | App decides  | Variable  | Advanced tuning          |
| 4KB-32KB     | Higher       | Lowest       | Low       | Real-time, latency-critical |
| 64KB (default)| Moderate    | Low-moderate | Moderate  | General purpose          |
| 256KB-1MB    | Lower        | Higher       | High      | Burst tolerance, streaming |
| 2MB-16MB     | Lowest       | Highest      | Very high | Bulk transfer, buffering |

**Monitoring and Diagnostics:**

Use xlio_stats to monitor per-socket buffer usage:

- n_rx_ready_byte_count: Current bytes in receive ready queue
- n_rx_ready_byte_max: High-water mark (maximum bytes seen)
- n_rx_ready_byte_drop: Bytes dropped due to buffer limit
- n_rx_ready_pkt_drop: Packets dropped due to buffer limit

Warning signs:
- n_rx_ready_byte_drop > 0: Packets are being lost, consider increasing limit
  or improving application consumption rate
- n_rx_ready_byte_max ≈ limit: Buffer is filling up regularly
- n_rx_ready_byte_count consistently high: Application not keeping up

On socket close, XLIO logs drop statistics if packets were lost:
  "Rx byte : max X / dropped Y (Z%)"

**Interaction with Other Parameters:**

- **performance.rings.rx.ring_elements_count:** Hardware queue depth is separate
  from this socket-level buffer. Packets first arrive in hardware RQ, then are
  moved to the socket's ready queue (subject to this limit).

- **performance.completion_queue.keep_full:** Controls behavior when hardware
  buffers are exhausted - different from this socket-level limit.

- **System sysctl net.core.rmem_max:** Maximum value applications can request
  via setsockopt(SO_RCVBUF). This XLIO parameter enforces a minimum, while
  rmem_max enforces a maximum.

- **System sysctl net.core.rmem_default:** Default SO_RCVBUF for new sockets.
  XLIO uses max(rmem_default×2, override_rcvbuf_limit) as the initial limit.

**Tuning Recommendations:**

1. **Start with default (65536)** - suitable for most applications

2. **For real-time/low-latency applications:**
   - Reduce to 8192-16384 bytes
   - Application must be able to keep up with arrival rate
   - Monitor n_rx_ready_byte_drop to understand loss rate

3. **For high-bandwidth streaming/multicast:**
   - Increase to 256KB-1MB
   - Allows buffering during processing spikes
   - Watch memory consumption with many sockets

4. **For financial trading/market data:**
   - Low values (4KB-16KB) preferred - stale data is worse than no data
   - Application should process faster than data arrives
   - Drops indicate application is too slow (useful signal)

5. **For bulk UDP data transfer:**
   - Higher values (512KB-4MB) improve throughput
   - Reduces drops during application processing variations
   - Trade-off: Increased memory and latency

6. **If seeing packet drops (n_rx_ready_byte_drop > 0):**
   - First: Check if application is reading fast enough
   - If application is optimal: Increase this limit
   - If limit is already high: Application fundamentally cannot keep up

**Example Calculations:**

For a 10 Gbps multicast stream with 1500-byte packets:
- Packets per second: ~833,333 pps
- At 64KB buffer: can hold ~43 packets = ~52 microseconds of data
- At 1MB buffer: can hold ~683 packets = ~820 microseconds of data
- At 16MB buffer: can hold ~10,922 packets = ~13 milliseconds of data

Choose based on how long your application might stall during processing.
Default value is 65536

performance.polling.blocking_rx_poll_usec
Maps to **XLIO_RX_POLL** environment variable.
Controls how many times XLIO busy-polls the hardware Completion Queue (CQ) for incoming packets before transitioning to interrupt-driven sleep mode.

**How Packet Reception Works in XLIO:**

When an application calls recv(), read(), recvfrom(), or recvmsg() on a blocking socket:
1. XLIO enters a busy-poll loop, checking the hardware CQ for completed packets
2. Each poll iteration is very fast (~100-500 nanoseconds) but consumes CPU cycles
3. If data arrives during polling: immediate return with lowest possible latency
4. After exhausting the poll budget: XLIO arms CQ interrupts and sleeps via epoll_wait()
5. When a packet arrives, hardware generates an interrupt, waking the thread

For non-blocking sockets: Only 1 poll is performed regardless of this setting, then returns -1/EAGAIN if no data is available.

**Value Interpretation:**

- **-1 (Infinite Polling):**
  Never sleep; pure busy-wait until data arrives.
  CPU core runs at 100% utilization spinning on CQ.
  Provides absolute lowest latency (~sub-microsecond to packet availability).
  Best for: Latency-critical applications with dedicated CPU cores (HFT, real-time systems).
  Warning: Disables CQ interrupt moderation as interrupts are never needed.

- **0 (Minimal Polling):**
  Internally treated as 1 - performs at least one CQ poll before sleeping.
  Effectively interrupt-driven with minimal busy-wait.
  Lowest CPU usage but highest latency (context switch + interrupt overhead).
  Best for: CPU-constrained environments, background services.

- **Low Values (1-1,000):**
  Brief busy-poll window before sleeping.
  Quick transition to interrupt mode if no data.
  Latency: ~10-100 microseconds typical (varies by interrupt delivery time).
  CPU: Very low - thread sleeps most of the time.
  Best for: Applications with infrequent receives, mixed workloads.

- **Medium Values (1,000-100,000):**
  Balanced approach between latency and CPU usage.
  Default (100,000) provides good latency for most applications.
  With typical poll rate of ~1-10 million polls/second, 100,000 polls ≈ 10-100ms of busy-waiting.
  Best for: General-purpose servers, web applications.

- **High Values (100,000-100,000,000):**
  Extended busy-poll window, rarely sleeping.
  Near-minimum latency with high CPU usage.
  Thread may appear CPU-bound during idle periods.
  Best for: Latency-sensitive applications without dedicated cores.

**Performance Tradeoffs:**

| Value Range | Latency | CPU Usage | Interrupt Overhead | Use Case |
|-------------|---------|-----------|-------------------|----------|
| -1 (infinite) | Lowest (~sub-μs) | 100% | None | HFT, real-time |
| 0-100 | Highest (~50-500μs) | Minimal | Every packet | Background tasks |
| 100-10,000 | High (~10-100μs) | Low | Frequent | Mixed workloads |
| 10,000-100,000 | Medium (~1-10μs) | Medium | Occasional | General servers |
| 100,000+ | Low (~sub-μs to μs) | High | Rare | Latency-sensitive |

**Understanding Poll Hit Rate:**

XLIO tracks polling effectiveness via statistics (visible with xlio_stats):
- n_rx_poll_hit: Packets found during busy-poll phase
- n_rx_poll_miss: Polling exhausted, had to sleep

High poll hit rate (>90%) indicates optimal latency - packets arrive during polling.
Low poll hit rate (<50%) suggests over-polling - wasting CPU waiting for infrequent data.

**Latency Breakdown by Mode:**

*During Busy-Poll (poll hit):*
- Packet arrival → recv() returns: ~0.1-1 microsecond
- Dominated by: CQ poll latency, memory access, packet processing

*After Sleep (poll miss):*
- Packet arrival → recv() returns: ~10-100+ microseconds typical
- Dominated by: Interrupt delivery, kernel wakeup, context switch, CQ poll
- Components: HW interrupt (~1-5μs) + kernel handling (~2-10μs) + context switch (~1-5μs) + epoll_wait return (~1-3μs) + CQ poll (~0.1-0.5μs)

**CPU Utilization Patterns:**

*With -1 (infinite polling):*
- Thread runs at 100% CPU continuously
- No context switches during receive wait
- Ideal: Pin thread to dedicated core with isolcpus

*With high values (>100,000):*
- CPU usage correlates with receive frequency
- Frequent receives: Low CPU (data arrives during early polls)
- Infrequent receives: High CPU (most of poll budget consumed)

*With low values (<1,000):*
- CPU usage very low regardless of receive frequency
- Thread spends most time sleeping
- Context switches on every receive burst

**Interaction with Other Parameters:**

- performance.polling.iomux.poll_usec: Similar control for select()/poll()/epoll_wait()
- performance.polling.yield_on_poll: Yields CPU periodically during polling (for multi-threaded sharing)
- performance.polling.rx_cq_wait_ctrl: Optimizes epoll fd registration for sleeping
- performance.polling.skip_cq_on_rx: Can skip CQ polling entirely if using epoll
- performance.completion_queue.interrupt_moderation.*: Controls HW interrupt coalescing when sleeping

**Recommended Configurations:**

*Ultra-Low Latency (HFT, Real-time):*
  blocking_rx_poll_usec = -1
  + Dedicate CPU cores (isolcpus, taskset)
  + Disable interrupt moderation
  + Consider: yield_on_poll = 0

*Low Latency Server (Trading, Gaming):*
  blocking_rx_poll_usec = 1000000 (1M)
  + Provides near-infinite polling with eventual sleep
  + Good balance if cores aren't fully dedicated

*General Purpose Server (Web, API):*
  blocking_rx_poll_usec = 100000 (default)
  + Good latency for active connections
  + Reasonable CPU usage during idle

*CPU-Constrained Environment:*
  blocking_rx_poll_usec = 1000
  + Quick transition to interrupt mode
  + Minimal CPU waste during idle
  + Accept higher latency tradeoff

*Throughput-Optimized (Bulk Transfer):*
  blocking_rx_poll_usec = 10000
  + Latency less critical than efficiency
  + Let interrupt coalescing batch packets

**Tuning Methodology:**

1. Start with default (100,000)
2. Monitor with xlio_stats: check poll hit/miss ratio
3. If poll hit rate >95% and latency acceptable: value is appropriate
4. If poll hit rate <50%: reduce value to save CPU
5. If latency too high despite high hit rate: check other bottlenecks
6. If latency critical: increase toward -1, monitor CPU

**Warning:**
Setting -1 (infinite) disables CQ interrupt moderation. If your application has periods of no traffic, the polling thread will consume 100% of its CPU core indefinitely. Ensure this is acceptable for your deployment.
Default value is 100000

performance.polling.iomux.poll_os_ratio
Maps to **XLIO_SELECT_POLL_OS_RATIO** environment variable.
Controls the ratio between hardware-accelerated offloaded socket polling (via Completion Queue)
and OS kernel polling (via select/poll/epoll syscalls) for non-offloaded file descriptors.

**How It Works:**
During select()/poll()/epoll_wait(), XLIO enters a polling loop. A countdown counter starts
at this value and decrements with each CQ poll. When it reaches 0, XLIO performs a
zero-timeout OS syscall to check non-offloaded FDs, then resets the counter.

The effective ratio is: 1 OS poll for every N CQ polls, where N = poll_os_ratio.

**Value Meanings:**

- **0 (Disable OS polling):**
  Only offloaded sockets are polled. Non-offloaded FDs are completely ignored during the
  polling phase. Use this for pure-offloaded workloads where all important FDs are accelerated.
  WARNING: Non-offloaded FDs will only be checked when entering blocking mode, causing
  significant delays or starvation for non-offloaded traffic.
  Used in MCE_SPEC_29WEST (ultra-low latency) mode.

- **Low values (1-10):**
  Frequent OS polling. OS FDs are checked often, providing responsive handling of
  non-offloaded traffic at the cost of more syscall overhead.
  - Value 1: OS polled every iteration (highest overhead, best non-offloaded responsiveness)
  - Value 10 (default): OS polled every 10 CQ polls (balanced tradeoff)

- **High values (50-100+):**
  Infrequent OS polling. Prioritizes offloaded traffic with minimal syscall overhead.
  - Value 100: OS polled once every 100 CQ polls (used in MCE_SPEC_LATENCY mode)
  Non-offloaded FDs experience longer delays between checks.

**Performance Tradeoffs:**

*Lower Values (more OS polling):*
- Better responsiveness for non-offloaded FDs (timers, signals, non-RDMA sockets)
- Higher syscall overhead (each OS poll is a kernel transition)
- Slightly higher latency variance for offloaded traffic
- Better for mixed workloads with both offloaded and non-offloaded FDs

*Higher Values (less OS polling):*
- Lower syscall overhead, more CPU time spent in userspace
- Better latency consistency for offloaded traffic
- Non-offloaded FDs may experience delays proportional to the ratio
- Better for workloads dominated by offloaded traffic

**Related Parameters:**
- performance.polling.iomux.skip_os: Controls how often to give OS immediate priority
  (resets the countdown to 0 every N select/poll calls)
- performance.polling.iomux.poll_usec: Duration of the entire polling phase

**Recommended Configurations:**

- Mixed workloads (offloaded + non-offloaded FDs): Use default (10) or lower
- Latency-sensitive pure offloaded workloads: Use 100 or higher
- Ultra-low latency, offloaded-only: Use 0 (disable OS polling entirely)
Default value is 10

performance.polling.iomux.poll_usec
Maps to **XLIO_SELECT_POLL** environment variable.
Controls how long (in microseconds) XLIO busy-polls the hardware Completion Queue (CQ) for incoming packets during I/O multiplexing calls (select(), poll(), epoll_wait()) before transitioning to interrupt-driven sleep mode.

**How I/O Multiplexing Works in XLIO:**

When an application calls select(), poll(), or epoll_wait() with offloaded sockets:
1. **Scenario 1 (No offloaded FDs):** Goes directly to OS wait - this parameter has no effect.
2. **Scenario 2 (Polling phase):** XLIO enters a busy-poll loop, checking the hardware CQ for completed packets. The loop continues for up to poll_usec microseconds.
3. **Scenario 3 (Sleep phase):** If polling exhausts without finding data, XLIO arms CQ interrupts, goes to sleep, and waits for hardware to signal packet arrival.

The max polling duration is bounded by the lesser of: this parameter OR the timeout provided by the application to select()/poll()/epoll_wait().

**Value Interpretation:**

- **-1 (Infinite Polling):**
  Never sleep; pure busy-wait until data arrives or application timeout expires.
  CPU core runs at 100% utilization spinning on CQ during wait.
  Provides absolute lowest latency (~sub-microsecond to packet detection).
  Best for: Ultra-low-latency applications with dedicated CPU cores (HFT, real-time trading).
  Warning: Thread will consume 100% CPU during any wait period.
  Profile: Used by LATENCY profile for minimum latency.

- **0 (Minimal Polling):**
  Performs exactly one CQ poll before immediately transitioning to sleep mode.
  Essentially interrupt-driven operation with minimal busy-wait overhead.
  Lowest CPU usage but higher latency (interrupt + context switch overhead).
  Best for: High-concurrency servers (like Nginx) where thousands of connections share few workers, and CPU efficiency matters more than per-connection latency.
  Profile: Used by NGINX profile for maximum scalability.

- **Low Values (1-10,000 µsec):**
  Brief busy-poll window (1µs to 10ms) before sleeping.
  Quick transition to interrupt mode if no immediate data.
  Latency: ~10-100 microseconds typical (dominated by interrupt delivery when sleeping).
  CPU: Very low - thread sleeps most of the time during idle periods.
  Best for: Applications with sporadic traffic, mixed workloads, background services.

- **Medium Values (10,000-100,000 µsec):**
  Balanced approach between latency and CPU usage.
  Default (100,000 = 100ms) provides good latency for typical server applications.
  Allows polling for 10-100ms before giving up and sleeping.
  Best for: General-purpose servers, web applications, API servers.

- **High Values (100,000-100,000,000 µsec):**
  Extended busy-poll window (100ms to 100 seconds), rarely sleeping.
  Near-minimum latency with high CPU usage during idle waits.
  Thread may appear CPU-bound when waiting with no traffic.
  Best for: Latency-sensitive applications that can tolerate some CPU waste.

**Performance Tradeoffs:**

| Value Range | Latency (poll hit) | Latency (poll miss) | CPU During Wait | Use Case |
|-------------|-------------------|---------------------|-----------------|----------|
| -1 (infinite) | ~0.1-1 µs | N/A (never miss) | 100% | HFT, real-time |
| 0 (minimal) | ~0.1-1 µs | ~50-500 µs | ~0% | Nginx, high-concurrency |
| 1-10,000 | ~0.1-1 µs | ~10-100 µs | Very Low | Mixed workloads |
| 10K-100K | ~0.1-1 µs | ~1-10 µs | Medium | General servers |
| 100K+ | ~0.1-1 µs | Rare | High | Latency-sensitive |

**Poll Hit vs Poll Miss:**

XLIO tracks polling effectiveness via statistics (visible with xlio_stats):
- **n_iomux_poll_hit:** Data found during busy-poll phase - lowest latency path
- **n_iomux_poll_miss:** Polling exhausted without data, had to sleep

*Poll Hit (best case):*
- Packet arrived during polling phase
- select()/poll()/epoll_wait() returns in ~0.1-1 microseconds
- No interrupt overhead, no context switch, no kernel involvement

*Poll Miss (worse case):*
- Polling budget exhausted without finding data
- XLIO arms CQ interrupt and goes to sleep via OS epoll_wait()
- When packet arrives: HW interrupt (~1-5µs) + kernel handling (~2-10µs) + context switch (~1-5µs)
- Total latency: ~10-100+ microseconds

**CPU Utilization Patterns:**

*With -1 (infinite polling):*
- Thread runs at 100% CPU continuously during any select()/poll()/epoll_wait() call
- No context switches during wait periods
- Ideal: Pin thread to dedicated core with isolcpus/taskset
- All other threads on the system compete for remaining cores

*With 0 (minimal polling):*
- Thread sleeps immediately after single CQ poll
- CPU usage near zero during idle periods
- High context switch rate when traffic is bursty
- Excellent for high worker count scenarios (e.g., Nginx with 32+ workers)

*With medium/high values:*
- CPU usage inversely correlates with traffic rate:
  - High traffic: Low CPU (data found early in polling)
  - Low/no traffic: High CPU (most of poll budget consumed before sleeping)

**Interaction with Related Parameters:**

- **performance.polling.blocking_rx_poll_usec:** Similar control but for direct recv()/read() calls. Both should typically be tuned together.
- **performance.polling.iomux.poll_os_ratio:** How often to poll non-offloaded OS file descriptors during the busy-poll loop. Value of 0 means never poll OS FDs during polling phase.
- **performance.polling.iomux.skip_os:** How often to skip OS FD checking when offloaded data is found.
- **performance.completion_queue.interrupt_moderation.*:** Controls HW interrupt coalescing when in sleep mode - affects poll-miss latency.

**Recommended Configurations:**

*Ultra-Low Latency (HFT, Real-time Trading):*
  poll_usec = -1
  + Dedicate CPU cores (isolcpus, taskset)
  + Set blocking_rx_poll_usec = -1 as well
  + Consider disabling interrupt moderation

*High-Concurrency Server (Nginx, HAProxy):*
  poll_usec = 0
  + Maximize worker count efficiency
  + Reduce CPU waste on idle connections
  + Accept slightly higher latency for better scalability

*Low Latency Server (Trading, Gaming):*
  poll_usec = 100000 to 1000000 (100ms-1s)
  + Good balance of latency and CPU
  + Rarely sleeps during active trading hours
  + Sleeps during truly idle periods

*General Purpose Server (Web, API):*
  poll_usec = 100000 (default)
  + Good latency for active connections
  + Reasonable CPU usage during idle

*CPU-Constrained Environment:*
  poll_usec = 1000 to 10000 (1-10ms)
  + Quick transition to interrupt mode
  + Minimal CPU waste during idle
  + Accept higher latency tradeoff

**Warning for select()/poll() Applications:**
Applications using select() or poll() (rather than epoll) may have different performance characteristics. XLIO must iterate through all FDs, which scales poorly with FD count. Consider migrating to epoll for better scalability, or use lower poll_usec values to reduce CPU overhead.

**Tuning Methodology:**

1. Start with default (100,000)
2. Monitor with xlio_stats: check n_iomux_poll_hit / n_iomux_poll_miss ratio
3. If poll hit rate >95% and latency acceptable: value is appropriate
4. If poll hit rate <50%: reduce value to save CPU (you're wasting cycles polling when data is rarely ready)
5. If latency too high during active periods: increase value toward -1
6. Monitor CPU utilization during both active and idle periods
Default value is 100000

performance.polling.iomux.skip_os
Maps to **XLIO_SELECT_SKIP_OS** environment variable.
Controls how often the OS kernel gets priority polling at the START of select()/poll()/epoll_wait() calls.

**How It Works:**
XLIO maintains a global counter that decrements with each iomux call (select/poll/epoll_wait).
When this counter reaches 0, the OS is polled FIRST (given priority) before checking offloaded
sockets. The counter then resets to this parameter's value.

On calls where the counter is > 0, OS polling is deferred according to poll_os_ratio.

**Interaction with poll_os_ratio:**
These two parameters work together at different levels:
- skip_os (this parameter): Controls WHEN OS gets priority across multiple iomux calls
- poll_os_ratio: Controls the CQ-to-OS polling ratio WITHIN a single polling loop

Example with skip_os=4 and poll_os_ratio=10:
- Call 1: OS deferred (countdown starts at 10)
- Call 2: OS deferred (countdown starts at 10)
- Call 3: OS deferred (countdown starts at 10)
- Call 4: OS gets priority (countdown starts at 0), then resets to 4

**Value Meanings:**

- **0 (OS always gets priority):**
  OS is checked FIRST on every iomux call before checking offloaded sockets.
  Best for: Workloads where non-offloaded FD responsiveness is critical.
  Tradeoff: Adds syscall overhead on every call, even when offloaded data is waiting.

- **Low values (1-3):**
  OS gets priority frequently.
  - Value 1: OS priority every other call
  - Value 2: OS priority every 3rd call
  Good for: Mixed workloads with important non-offloaded FDs (timers, signals, pipes).
  Tradeoff: More frequent OS syscalls at loop start; slightly higher latency for
  offloaded-only workloads.

- **Default (4):**
  Balanced approach: OS gets priority every 5th iomux call.
  Good for: General-purpose workloads with both offloaded and non-offloaded FDs.
  Provides reasonable responsiveness for non-offloaded FDs without excessive overhead.

- **High values (10-100+):**
  OS priority is rare.
  - Value 10: OS priority every 11th call
  - Value 100: OS priority every 101st call
  Good for: Workloads dominated by offloaded traffic where non-offloaded FDs are
  not latency-sensitive.
  Tradeoff: Non-offloaded FDs may experience noticeable delays between priority checks.

**Performance Tradeoffs:**

*Lower Values (frequent OS priority):*
- Better initial responsiveness for non-offloaded FDs
- Higher syscall overhead at the start of iomux calls
- May add latency when offloaded packets are ready but OS is checked first
- Reduces the benefit of hardware acceleration for latency-sensitive applications

*Higher Values (infrequent OS priority):*
- Lower syscall overhead, better offloaded path latency
- Offloaded sockets are checked first most of the time
- Non-offloaded FDs may wait longer before being checked with priority
- Better for pure offloaded workloads

**Use Cases:**

- Event-driven servers with timers and signals: Use low values (2-4)
- Pure TCP offloaded workloads: Use high values (10+) or rely solely on poll_os_ratio
- Mixed UDP multicast with control channels: Use default (4)
- Ultra-low latency trading: Use higher values to minimize syscall jitter

**Related Parameters:**
- performance.polling.iomux.poll_os_ratio: Fine-grained control within polling loops
- performance.polling.iomux.poll_usec: Total duration of the polling phase
Default value is 4

performance.polling.max_rx_poll_batch
Maps to **XLIO_CQ_POLL_BATCH_MAX** environment variable.

**What This Does:**
Controls the maximum number of Completion Queue Entries (CQEs) processed in a single poll_and_process_element_rx() call. This directly limits how many received packets XLIO handles before returning control to the application.

**How the Polling Mechanism Works:**
1. Application calls recv()/recvfrom()/poll()/select()/epoll_wait()
2. XLIO acquires the ring lock (via trylock) and enters poll_and_process_element_rx()
3. First, any packets already in the software rx_queue are processed (up to max_rx_poll_batch)
4. If the limit isn't reached, XLIO polls the hardware Completion Queue in a loop:
   ```
   while (rx_polled < max_rx_poll_batch) {
       buff = poll(status);  // Read next CQE from hardware
       if (!buff) break;     // CQ is empty
       ++rx_polled;
       process_recv_buffer(buff, fd_ready_array);
   }
   ```
5. After polling, GRO aggregations are flushed via m_gro_mgr.flush_all()
6. Control returns to the application

**Return Value Semantics (for callers):**
- `max_rx_poll_batch - rx_polled`: How many more packets could have been polled
- `0`: Batch limit reached - CQ may have more packets (NOT fully drained)
- `-1`: CQ was empty (no packets to poll)
- Positive value: CQ drained before reaching limit

This return value drives the polling loop in UDP/TCP receive paths:
- UDP rx_wait() loops while return is 0 (not drained) until packets are ready
- TCP may break early if packets arrive before batch completes

**Value Range:** 1 to 32768

**Value Tradeoffs:**

*Low Values (1-8) - Best for Latency Consistency:*

Benefits:
- Lower first-packet latency: When CQ has many pending packets, the first packet reaches the application faster since XLIO processes only a small batch before returning.
- More predictable timing: Smaller processing batches mean more consistent per-call latency. Easier to reason about worst-case response time.
- Better interleaving: Application code runs more frequently between batches, enabling faster reaction to application-level events (timeouts, signals, other sockets).
- Tighter GRO aggregation: flush_all() is called more frequently with smaller batches. This limits TCP segment aggregation, which can reduce receive-side latency at the cost of more per-packet overhead.
- Shorter ring lock hold time: Other threads waiting on the ring lock (if sharing rings) wait less per acquisition.

Drawbacks:
- Higher per-packet overhead: More poll_and_process_element_rx() calls are needed to drain a CQ with many pending packets. Each call has function call overhead, lock acquisition cost, and GRO flush overhead.
- Lower maximum throughput: Under sustained high packet rates, the overhead of frequent returns to the application reduces packets-per-second capacity.
- More lock acquisitions: If the application loops calling recv(), each iteration requires re-acquiring the ring lock.
- Suboptimal GRO efficiency: TCP segment coalescing is limited since fewer packets accumulate before flush. More individual packets delivered = more application callbacks.

Recommended for:
- High-frequency trading and low-latency applications needing predictable timing
- Real-time systems with strict worst-case latency requirements
- Interactive protocols where responsiveness trumps throughput
- Applications using select()/poll() that need to react to other FDs quickly

*Medium Values (16-64) - Balanced (Default):*

Benefits:
- Good balance between latency and throughput for most workloads
- Reasonable GRO aggregation efficiency
- Manageable per-call processing time
- Default value (16) chosen based on typical workload profiling

Drawbacks:
- May not be optimal for extreme latency or extreme throughput scenarios

Recommended for:
- General-purpose servers and applications
- Mixed workloads with varying packet rates
- When unsure, start with the default

*High Values (128-32768) - Best for Throughput:*

Benefits:
- Higher sustained throughput: More packets processed per poll call means lower per-packet overhead. Amortizes function call, lock acquisition, and GRO flush costs across many packets.
- Better CPU cache utilization: Processing many packets in sequence keeps CQ and buffer structures hot in L1/L2 cache. Sequential access patterns are more cache-friendly.
- More effective GRO aggregation: With more packets processed before flush_all(), TCP Large Receive Offload (software GRO) can aggregate more segments. Larger coalesced packets reduce per-packet application overhead.
- Fewer lock acquisitions under high load: CQ fully drained in fewer calls, reducing lock contention when multiple threads share rings.

Drawbacks:
- Higher latency variance (jitter): When CQ has many packets, the first packet must wait while the entire batch is processed. Tail latency increases.
- Longer ring lock hold time: Other threads waiting for the ring lock must wait longer per acquisition. Can hurt multi-threaded scenarios with ring sharing.
- Delayed application responsiveness: Application code doesn't run until the batch completes. Timeouts, signals, or other socket events wait longer.
- No benefit under light load: If CQ drains before reaching the limit, the high value has no effect (CQ naturally empties).
- Memory locality pressure: Processing many buffers in one pass touches more memory, potentially evicting other hot data from CPU cache.

Recommended for:
- Bulk data transfer applications (file transfer, streaming)
- High-throughput servers processing millions of packets per second
- Single-threaded applications where lock contention isn't a concern
- Scenarios where maximizing PPS matters more than latency consistency

**Performance Impact Summary:**
| Metric                    | Low (1-8)      | Default (16)   | High (128+)     |
|---------------------------|----------------|----------------|------------------|
| First-packet latency      | Lowest         | Low            | Higher           |
| Latency std-dev (jitter)  | Lowest         | Low            | Higher           |
| Throughput (high load)    | Lower          | Good           | Highest          |
| Per-packet CPU overhead   | Higher         | Balanced       | Lower            |
| GRO aggregation           | Minimal        | Moderate       | Maximum          |
| Ring lock hold time       | Shortest       | Medium         | Longest          |
| CQ drain iterations       | Many           | Moderate       | Few              |

**Interaction with Other Parameters:**

- **performance.polling.blocking_rx_poll_usec:** When blocking on recv(), XLIO polls in a loop for up to this duration. Each iteration processes up to max_rx_poll_batch packets. Lower batch size = more iterations within the timeout.

- **hardware_features.lro (Large Receive Offload):** Software GRO aggregation flushes after each poll batch. Higher max_rx_poll_batch allows more packets to be aggregated before flush, improving coalescing efficiency. Low values limit aggregation benefit.

- **performance.rings.rx.allocation_logic:** With ring-per-thread/socket, lock contention is minimal, so higher batch values have less downside. With shared rings, longer lock hold times from high batch values can hurt other threads.

- **performance.polling.skip_cq_on_rx:** When enabled, recv() may skip CQ polling entirely (relying on epoll to drive polling). This makes max_rx_poll_batch less relevant for direct recv() calls but still affects epoll-driven polling.

- **performance.polling.select_poll.poll_usec:** During select()/poll()/epoll_wait(), XLIO polls rings. Batch size affects how many packets are processed per ring before moving to the next ring or returning.

**Tuning Recommendations:**

1. **Start with default (16)** - Optimal for most workloads

2. **For latency-sensitive applications (HFT, real-time):**
   - Reduce to 4-8
   - Combine with ULTRA_LATENCY profile settings
   - Accept throughput tradeoff for predictable timing

3. **For high-throughput bulk transfer:**
   - Increase to 64-256
   - Monitor tail latency to ensure it's acceptable
   - Combine with high blocking_rx_poll_usec for sustained polling

4. **For multi-threaded with shared rings:**
   - Keep lower (8-32) to limit lock hold time
   - Or use per-thread ring allocation to avoid contention entirely

5. **For diagnosing latency issues:**
   - Try reducing max_rx_poll_batch to see if batch processing causes latency spikes
   - Profile poll_and_process_element_rx() duration with perf

**Profile-Specific Values:**
- Default profile: 16
- LATENCY profile: 16
- ULTRA_LATENCY profile: May benefit from 4-8 (consider tuning)
- NGINX profile: 16 (unchanged)

**Monitoring:**
- No direct xlio_stats counter for batch utilization
- n_rx_drained_at_once_max in CQ stats shows peak packets processed in one drain_and_process() call (internal thread path)
- Profile poll_and_process_element_rx() with perf to measure time per call
- Latency histograms reveal impact of batch processing on tail latency
Default value is 16

performance.polling.nonblocking_eagain
Maps to **XLIO_TX_NONBLOCKED_EAGAINS** environment variable.

**What This Does:**
Controls how XLIO handles non-blocking UDP send operations when transmit buffers are temporarily unavailable. This parameter affects the return value and error reporting behavior when a non-blocking UDP socket attempts to send but XLIO cannot allocate TX buffers from its internal buffer pool.

**Note:** This parameter only affects UDP sockets. TCP sockets have different buffer management and flow control mechanisms.

**When TX Buffer Exhaustion Occurs:**
TX buffer exhaustion happens when:
- Application sends packets faster than the NIC can transmit them
- Completion queue processing cannot keep up with recycling used buffers
- Multiple rings compete for a limited global buffer pool
- Burst traffic exceeds available pre-allocated buffers

**Value: false (Default) - OS-Compatible Silent Drops:**

Behavior:
- Returns success (the number of bytes "sent") even though the packet was dropped
- The datagram is silently discarded inside XLIO
- Application receives no indication of the failure
- Matches standard Linux/OS behavior for non-blocking UDP sockets

Benefits:
- Drop-in compatible with existing applications designed for OS sockets
- No application code changes required
- Simpler application logic - no need to handle EAGAIN on UDP sends
- Works transparently with fire-and-forget UDP patterns
- Lower latency - no retry overhead in send path

Drawbacks:
- Silent data loss with no feedback mechanism
- Cannot implement application-level flow control or backpressure
- Harder to detect and debug packet loss issues
- Statistics may be misleading - packets counted as "sent" were actually dropped
- No opportunity for application to make intelligent decisions about dropped data

Best for:
- Applications designed for UDP's unreliable semantics
- Fire-and-forget messaging patterns
- Applications where occasional packet loss is acceptable
- Legacy applications that cannot be modified
- Scenarios where upper layers handle retransmission (e.g., QUIC, custom reliability)

**Value: true - Explicit EAGAIN on Buffer Exhaustion:**

Behavior:
- Returns -1 with errno set to EAGAIN when TX buffers are unavailable
- Application is explicitly notified that the send operation could not be completed
- Packet is not silently dropped - the failure is communicated

Benefits:
- Full visibility into send failures - application knows when drops occur
- Enables application-level flow control and backpressure mechanisms
- More accurate performance monitoring and statistics
- Application can implement intelligent retry strategies (exponential backoff, etc.)
- Can detect overload conditions and adapt sending rate
- Better debugging and troubleshooting capabilities
- Useful for applications that need delivery guarantees at the application layer

Drawbacks:
- Requires application to handle EAGAIN error on send()/sendto()/sendmsg()
- May require application code modifications from standard UDP patterns
- Adds complexity to error handling paths
- Immediate retries can cause CPU spinning if not handled carefully
- May increase latency if application busy-waits on EAGAIN

Best for:
- High-reliability UDP applications needing explicit failure feedback
- Applications implementing custom flow control
- Real-time systems where silent drops are unacceptable
- Performance-critical applications that need to adapt to backpressure
- Applications with their own retry/reliability layer that needs accurate feedback
- Debugging and performance analysis scenarios

**Example Application Handling for true:**
```c
ssize_t ret = sendto(fd, buf, len, MSG_DONTWAIT, &addr, sizeof(addr));
if (ret < 0 && errno == EAGAIN) {
    // TX buffers exhausted - implement backpressure
    // Option 1: Poll/select for POLLOUT before retrying
    // Option 2: Exponential backoff
    // Option 3: Drop packet with application-level logging
}
```

**Interaction with Other Parameters:**
- performance.rings.tx.ring_elements_count: Larger TX queues reduce frequency of buffer exhaustion
- performance.rings.tx.udp_buffer_batch: Controls how many buffers are fetched per batch
- performance.rings.tx.completion_batch_size: Affects how quickly buffers are recycled

**Monitoring:**
Use xlio_stats to monitor TX health:
- n_tx_eagain: Count of EAGAIN returns (only meaningful when this option is true)
- n_tx_dropped_wqes: Ring-level drops when Send Queue is full (separate from buffer exhaustion)
- Ring TX buffer availability in real-time stats
Default value is false

performance.polling.offload_transition_poll_count
Maps to **XLIO_RX_POLL_INIT** environment variable.
Controls CQ busy-polling iterations for UDP sockets during the 'transition phase' - the period before flow steering rules are attached via multicast group membership (ADD_MEMBERSHIP) or before the socket has any attached rings/CQs.

**Background: UDP Socket Lifecycle and Polling Modes**

When a UDP socket is created, XLIO marks it as potentially offload-capable but doesn't immediately attach hardware flow steering rules. The socket enters a 'transition phase' where:
1. No completion queue (CQ) is attached yet
2. The socket may receive unicast traffic or be waiting for ADD_MEMBERSHIP
3. This parameter controls polling behavior during this phase

Once ADD_MEMBERSHIP is called (for multicast) or flow steering is established:
- Hardware CQs are attached to the socket
- The socket transitions to using performance.polling.blocking_rx_poll_usec for polling
- This parameter no longer applies

**Important:** For UDP unicast applications that never call ADD_MEMBERSHIP, this parameter controls the ENTIRE lifetime polling behavior, making it critical for unicast UDP performance tuning.

**How the Polling Loop Works**

During blocking recv()/recvfrom() calls, XLIO enters a polling loop:
1. Each iteration polls the hardware CQ via is_readable()
2. If packets are found, return immediately (lowest latency)
3. If no packets and iterations exhausted, fall back to interrupt-based wait (epoll_wait on CQ notification channel)
4. The fallback adds latency but saves CPU

This parameter sets the iteration count for step 2 during the transition phase.

**Value Meanings**

- **0 (Disabled - Default):**
  No busy-polling during transition phase. Socket immediately goes to interrupt-driven wait.
  
  *Behavior:*
  - recv() calls skip CQ polling entirely during transition
  - Falls back directly to epoll_wait() on the CQ notification channel
  - Packet arrival triggers interrupt -> epoll wakeup -> packet delivery
  
  *Best for:*
  - UDP multicast applications where transition phase is brief (ADD_MEMBERSHIP called early)
  - Applications that don't expect traffic before multicast group join
  - CPU-constrained environments where polling overhead is unacceptable
  - Systems where power efficiency matters more than latency
  
  *Characteristics:*
  - Zero CPU usage during transition phase when waiting
  - Higher latency for packets arriving during transition (~50-200μs interrupt overhead)
  - No wasted cycles polling an empty CQ
  - Ideal when transition phase is just startup/initialization

- **-1 (Infinite Polling):**
  Busy-poll indefinitely during transition phase. Never fall back to interrupt wait.
  
  *Behavior:*
  - recv() spins continuously polling CQ until packet arrives
  - Thread never voluntarily yields CPU while waiting
  - Will poll forever if no packet arrives (use with timeout or non-blocking)
  
  *Best for:*
  - UDP unicast applications requiring absolute minimum latency
  - Dedicated cores where 100% CPU usage is acceptable
  - Real-time systems where interrupt latency is unacceptable
  - Market data feeds, high-frequency trading UDP receivers
  
  *Characteristics:*
  - 100% CPU usage while waiting for packets (one core fully consumed)
  - Lowest possible latency (~100-500ns poll overhead vs ~50-200μs interrupt)
  - Can starve other threads/processes on same core
  - Must use timeout-based recv or risk indefinite blocking

- **Low Values (1-1000):**
  Brief polling window before falling back to interrupts.
  
  *Behavior:*
  - Quick check if packet is immediately available
  - Falls back to interrupt wait relatively quickly
  - Captures 'almost ready' packets with low latency
  
  *Best for:*
  - Applications where packets sometimes arrive during transition
  - Moderate latency requirements with CPU efficiency
  - Development/testing environments
  
  *Characteristics:*
  - Low CPU overhead (~0.1-1ms of polling per recv call)
  - Catches packets that arrive 'just in time' without interrupt delay
  - Quick fallback for packets that take longer
  - Value of 100: ~10-50μs of polling before interrupt fallback
  - Value of 1000: ~100-500μs of polling before interrupt fallback

- **Medium Values (1000-100000):**
  Substantial polling window, balancing latency and CPU.
  
  *Best for:*
  - UDP unicast applications with moderate traffic rates
  - Applications where most packets arrive within polling window
  - Balancing latency optimization with system resource sharing
  
  *Characteristics:*
  - Moderate CPU usage during polling phase
  - Value of 10000: ~1-5ms of polling
  - Value of 100000: ~10-50ms of polling (matches default blocking_rx_poll_usec)
  - Good compromise for general UDP applications

- **High Values (100000-100000000):**
  Extended polling, approaching infinite behavior.
  
  *Best for:*
  - High-throughput UDP applications with bursty traffic
  - Scenarios where interrupt latency is problematic but infinite polling is too aggressive
  - Applications that need 'almost always polling' with eventual timeout
  
  *Characteristics:*
  - High CPU usage during waiting periods
  - Value of 100000000: ~10-50 seconds of polling (practically infinite for most applications)
  - Provides emergency fallback while maintaining low-latency focus

**Performance Tradeoffs**

| Value | Latency (transition) | CPU Usage | Best Use Case |
|-------|---------------------|-----------|---------------|
| 0 | High (~50-200μs) | Minimal | MC apps, brief transition |
| 1-100 | Medium-High | Very Low | Quick checks only |
| 100-1000 | Medium | Low | Occasional transition traffic |
| 1000-10000 | Low-Medium | Moderate | UDP unicast, balanced |
| 10000-100000 | Low | High | UDP unicast, latency focus |
| -1 | Lowest (~100-500ns) | Maximum | Ultra-low latency unicast |

**UDP Unicast vs Multicast Considerations**

*For UDP Multicast Applications:*
- Transition phase is typically brief (socket created -> ADD_MEMBERSHIP called)
- Default value of 0 is usually optimal
- Once ADD_MEMBERSHIP called, blocking_rx_poll_usec takes over
- Consider non-zero only if expecting traffic between socket() and ADD_MEMBERSHIP

*For UDP Unicast Applications:*
- This parameter controls polling for the ENTIRE socket lifetime
- No ADD_MEMBERSHIP is called, so blocking_rx_poll_usec never activates
- **Critical:** Set this to match your latency requirements
- Recommended: Match this to your blocking_rx_poll_usec setting for consistency
- For low-latency unicast, use -1 or high values (100000+)

**Interaction with Other Parameters**

- **performance.polling.blocking_rx_poll_usec:**
  Takes over after rings are attached (post-ADD_MEMBERSHIP). This parameter only applies during transition.

- **Blocking vs Non-blocking sockets:**
  For non-blocking sockets, XLIO always performs exactly 1 poll iteration regardless of this setting. This parameter only affects blocking sockets.

- **performance.polling.yield_on_poll:**
  During the polling loop controlled by this parameter, yield_on_poll can cause periodic CPU yielding to other threads.

**Recommended Configurations**

| Application Type | Recommended Value |
|-----------------|-------------------|
| UDP Multicast (normal) | 0 (default) |
| UDP Multicast (traffic before join) | 1000-10000 |
| UDP Unicast (low latency) | -1 or 100000 |
| UDP Unicast (balanced) | 10000-100000 |
| UDP Unicast (CPU efficient) | 0-1000 |
| Market data feed receiver | -1 |
| Generic UDP server | 10000 |

**Example Scenarios**

*Scenario 1: Multicast Video Streaming Receiver*
Value: 0 (default)
Rationale: Socket calls ADD_MEMBERSHIP immediately after creation. Transition phase is milliseconds. No benefit from polling during transition.

*Scenario 2: Low-Latency UDP Unicast Trading Application*
Value: -1
Rationale: Never calls ADD_MEMBERSHIP. Needs lowest possible latency. Dedicated cores available. CPU usage is acceptable tradeoff.

*Scenario 3: UDP Logging/Telemetry Collector*
Value: 1000
Rationale: Latency not critical. Many sockets, CPU efficiency matters. Brief polling catches ready packets, quick fallback otherwise.

*Scenario 4: UDP Unicast with Shared Cores*
Value: 10000-50000
Rationale: Needs good latency but can't monopolize CPU. Provides substantial polling window while allowing other threads to run.
Default value is 0

performance.polling.rx_cq_wait_ctrl
Maps to **XLIO_RX_CQ_WAIT_CTRL** environment variable.
Controls when CQ (Completion Queue) channel file descriptors are added to socket internal epoll descriptors.

**Background: How Socket Blocking Works in XLIO**

Each XLIO socket has its own internal epoll file descriptor (m_rx_epfd) used for blocking operations. When a socket performs a blocking receive (recv, recvmsg, etc.), it eventually calls epoll_wait() on this internal epfd to sleep until data arrives.

To wake up sleeping sockets when packets arrive, XLIO uses CQ (Completion Queue) channel FDs. These are kernel file descriptors that become readable when completions are available on the hardware completion queue. By adding these CQ channel FDs to socket epoll descriptors, sleeping sockets get woken when packets arrive on the shared ring.

**The Problem at Scale**

When multiple sockets share the same ring (common with per_interface or per_thread allocation), they all need to watch the same CQ channel FD. The default behavior adds this CQ-fd to EVERY socket's internal epfd permanently.

With many connections (e.g., 350,000), this creates a serious scalability problem:
- A single CQ event triggers the kernel to iterate through ALL epfds watching that CQ-fd
- The kernel performs a linear scan of the waiter list for each event
- This causes O(N) overhead where N is the number of sockets sharing the ring
- Results in high CPU usage in kernel space and increased latency

**Value: false (Default) - Permanent CQ-fd Registration**

Behavior:
- When a ring is associated with a socket, the CQ channel FD is immediately added to that socket's internal epfd via epoll_ctl(EPOLL_CTL_ADD)
- The CQ-fd remains registered for the lifetime of the socket-ring association
- All sockets sharing a ring permanently watch the same CQ-fd

Benefits:
- Simpler code path with fewer system calls
- No epoll_ctl overhead on each blocking operation
- Slightly lower latency for individual blocking calls
- Works well for low to moderate connection counts

Drawbacks:
- Kernel overhead scales linearly with connection count
- At 100K+ connections, significant CPU time spent in kernel epoll wakeup code
- Can cause latency spikes when CQ events wake many waiters
- Poor scalability for high-connection-count servers

Best for:
- Applications with fewer than 10,000 connections
- Scenarios where blocking socket operations are rare
- Applications using primarily non-blocking I/O with epoll
- Low connection count with frequent blocking operations

**Value: true - Dynamic CQ-fd Registration (On-Demand)**

Behavior:
- CQ channel FD is NOT added when ring is associated with socket
- Instead, CQ-fd is added via epoll_ctl(EPOLL_CTL_ADD) only when socket is about to sleep (epoll_wait)
- Immediately after waking up, CQ-fd is removed via epoll_ctl(EPOLL_CTL_DEL)
- Only sockets that are ACTUALLY sleeping watch the CQ-fd at any given time

Benefits:
- Excellent scalability for high connection counts (100K+)
- Kernel wakeup overhead proportional to sleeping sockets, not total sockets
- Much lower kernel CPU usage under high connection loads
- Dramatically reduced latency at scale
- Critical optimization for event-driven servers (Nginx, Envoy, HAProxy)

Drawbacks:
- Two additional system calls per blocking wait (epoll_ctl ADD + DEL)
- Slight overhead for each blocking operation
- Overhead is wasted if only a few connections exist
- Only beneficial when sockets actually perform blocking waits

Best for:
- High connection count servers (10K+ connections)
- Event-driven architectures (Nginx, Envoy, HAProxy)
- Applications where most sockets are idle/not sleeping
- Scenarios where many sockets share few rings (per_interface, per_thread with limited rings)

**Performance Tradeoffs Summary:**

| Metric | false (Default) | true |
|--------|-----------------|------|
| Syscalls per blocking wait | 1 (epoll_wait only) | 3 (ADD + wait + DEL) |
| Kernel wakeup overhead | O(total_sockets) | O(sleeping_sockets) |
| 1K connections | Negligible difference | Slight overhead |
| 100K connections | High kernel CPU, latency spikes | Excellent scalability |
| 350K connections | Severe performance degradation | Linear performance |
| Non-blocking sockets | No impact | No impact |

**Automatic Enablement:**

This parameter is automatically set to true when using:
- NGINX mode (XLIO_SPEC=nginx)
- Envoy mode (XLIO_SPEC=envoy)

These application profiles recognize the high-connection-count nature of these workloads and enable this optimization by default.

**Interaction with Other Parameters:**

- performance.rings.rx.allocation_logic: With per_interface (0) or per_ip_address (1), more sockets share each ring, making this optimization more impactful
- performance.rings.max_per_interface: Limiting rings increases socket-to-ring sharing ratio
- performance.polling.count: If polling count is high and sockets rarely sleep, this setting has minimal impact

**When to Change This Setting:**

Consider enabling (true) when:
- Running a server with 10,000+ concurrent connections
- Seeing high kernel CPU usage (%sy in top) with many idle connections
- Experiencing latency spikes correlated with connection count
- Using ring allocation that results in many sockets per ring
- Profiling shows time spent in epoll wakeup paths

Keep disabled (false) when:
- Running with fewer than 10,000 connections
- Each connection has its own ring (per_socket allocation)
- Blocking socket operations are very frequent
- Application doesn't use blocking I/O patterns
- Per-operation latency is more critical than scalability
Default value is false

performance.polling.rx_kernel_fd_attention_level
Maps to **XLIO_RX_UDP_POLL_OS_RATIO** environment variable.

**What This Does (UDP Only):**
Controls how often XLIO checks the OS kernel socket for pending data during UDP receive operations (recv, recvfrom, recvmsg, read). This creates a balance between hardware-accelerated CQ polling and attention to traffic that arrives via the OS kernel path.

**The Counter Mechanism:**
XLIO maintains a per-socket counter (m_rx_udp_poll_os_ratio_counter) that tracks CQ poll iterations:
1. Each time XLIO polls the Completion Queue, the counter increments
2. When counter >= rx_kernel_fd_attention_level, XLIO calls ioctl(FIONREAD) on the OS socket
3. If the OS socket has pending data (poll_os returns 1), XLIO falls back to OS recv
4. After checking OS, the counter resets to 0

The effective ratio is: 1 OS check for every N CQ polls, where N = rx_kernel_fd_attention_level.

**When Does Traffic Go to the OS Socket?**
Even with XLIO offloading active, some UDP traffic may arrive via the OS kernel:
- Packets that don't match hardware flow steering rules
- Traffic before multicast membership is established (before IP_ADD_MEMBERSHIP/IPV6_JOIN_GROUP)
- ICMP-related responses (e.g., ICMP Port Unreachable triggering ECONNREFUSED)
- Fragmented IP packets that the NIC cannot reassemble
- Traffic during socket initialization before offload rules are installed
- Packets when hardware flow table is full
- Traffic matching XLIO_SPEC rules that force OS handling
- Edge cases with certain socket options or protocol behaviors

**Value Interpretation:**

- **0 (Disable OS polling):**
  OS socket is NEVER checked during the active polling phase. Only the hardware CQ is polled.
  OS packets are only processed when:
  - Blocking socket enters sleep mode and wakes via epoll
  - Explicitly triggered by certain socket operations
  
  WARNING: Non-offloaded packets may be severely delayed or effectively starved.
  Use only for pure-offloaded workloads where ALL traffic is guaranteed to be hardware-steered.

- **Low Values (1-10) - Frequent OS Checking:**
  OS socket checked very frequently relative to CQ polling.
  - Value 1: OS checked every single CQ poll iteration (maximum OS attention)
  - Value 10: OS checked every 10th CQ poll iteration
  
  Benefits:
  - Fastest response to non-offloaded traffic
  - Best for mixed workloads with significant non-offloaded UDP traffic
  - Essential if application depends on OS-delivered packets (multicast before join, etc.)
  
  Drawbacks:
  - Higher syscall overhead (ioctl call every N polls)
  - Each OS check is a kernel transition adding ~100-500 nanoseconds
  - Slightly higher latency variance for offloaded traffic
  - CPU cycles spent checking often-empty OS socket

- **Medium Values (50-100) - Balanced (Default = 100):**
  OS socket checked occasionally, prioritizing CQ polling.
  - Default value (100): OS checked once every 100 CQ poll iterations
  
  Benefits:
  - Low syscall overhead (~1% of CQ poll iterations)
  - Good latency for offloaded traffic
  - Still catches non-offloaded traffic within reasonable time
  
  Drawbacks:
  - Non-offloaded packets may wait up to 100 polling cycles before detection
  - In high-frequency polling scenarios, this delay is typically negligible
  - In low-traffic scenarios, may delay OS packets noticeably

- **High Values (500+) - Rare OS Checking:**
  OS socket checked very infrequently.
  
  Benefits:
  - Minimal syscall overhead
  - Maximum focus on offloaded fast path
  - Best latency consistency for offloaded traffic
  
  Drawbacks:
  - Significant delays for non-offloaded packets
  - May appear as packet loss or high latency for OS-delivered traffic
  - Only suitable when non-offloaded traffic is rare and non-critical

**Performance Tradeoffs Summary:**

| Value | OS Check Frequency | Syscall Overhead | Non-Offloaded Latency | Offloaded Latency |
|-------|-------------------|------------------|----------------------|-------------------|
| 0 | Never (disabled) | None | Very High/Infinite | Lowest |
| 1-10 | Every 1-10 CQ polls | High (~10%+) | Lowest | Slightly Higher |
| 50-100 | Every 50-100 CQ polls | Low (~1-2%) | Medium | Low |
| 500+ | Every 500+ CQ polls | Minimal (<0.2%) | High | Lowest |

**Counter Reset Behavior:**
The counter is managed to optimize common scenarios:
- Reset to 0 after successful OS poll (ensures continued OS checking if OS traffic is flowing)
- Set to threshold value after successful OS read (triggers immediate OS check on next recv)
- Set to threshold when ring is added (skip initial OS checking until offload is active)
- Set to threshold after rx_add_ring_cb (start offloaded operation)

**Interaction with Related Parameters:**

- **performance.polling.iomux.poll_os_ratio:** Similar parameter but for select()/poll()/epoll_wait() operations. This parameter (rx_kernel_fd_attention_level) affects direct recv() calls, while poll_os_ratio affects iomux operations. Tune both consistently for coherent behavior.

- **performance.polling.blocking_rx_poll_usec:** Duration of CQ polling before sleeping. With longer polling durations, the OS check ratio has more iterations to work with. Example: 100ms polling at 1µs/iteration = ~100,000 iterations, so ratio of 100 means ~1000 OS checks.

- **performance.polling.offload_transition_poll_count:** For UDP sockets before ADD_MEMBERSHIP, traffic typically goes to OS. This parameter controls polling behavior during that transition phase.

**Recommended Configurations:**

*Pure Offloaded UDP Workload (all traffic hardware-steered):*
  rx_kernel_fd_attention_level = 0
  - Disable OS checking entirely
  - Maximum performance for offloaded path
  - Only use when certain ALL traffic is offloaded

*Latency-Sensitive Trading/HFT Applications:*
  rx_kernel_fd_attention_level = 100-500
  - Minimal OS overhead
  - Focus on offloaded fast path
  - Non-critical OS traffic (if any) can tolerate delays

*Mixed UDP Workload (some multicast, some unicast, control traffic):*
  rx_kernel_fd_attention_level = 10-50
  - Balance between offloaded and OS traffic
  - Reasonable response time for both paths
  - Good for applications with diverse traffic patterns

*Applications Depending on OS-Delivered Traffic:*
  rx_kernel_fd_attention_level = 1-10
  - Maximum attention to OS socket
  - Essential if multicast setup/teardown is frequent
  - Or if significant traffic doesn't match flow steering

*Nginx/High-Connection Servers:*
  rx_kernel_fd_attention_level = 0 (via NGINX profile)
  - Focuses entirely on offloaded path
  - OS traffic handled through epoll sleep/wake mechanism instead

**Profile-Specific Defaults:**
- Default profile: 100
- LATENCY profile: 100
- NGINX profile: 0 (disabled - relies on epoll for OS traffic)
- 29WEST profile: 0 (disabled - pure offloaded focus)

**Troubleshooting:**

*Symptom: Non-offloaded UDP packets arrive with high latency or seem lost*
- Cause: rx_kernel_fd_attention_level too high or 0
- Solution: Lower the value (e.g., 10-50) to check OS more frequently

*Symptom: Higher than expected syscall rate in perf/strace*
- Cause: rx_kernel_fd_attention_level too low, causing frequent ioctl calls
- Solution: Increase the value if OS traffic is rare

*Symptom: Multicast traffic works after JOIN but initial packets are delayed*
- Cause: Before flow steering is established, packets go to OS
- Solution: Lower rx_kernel_fd_attention_level during multicast setup phase

**Monitoring:**
- xlio_stats shows n_rx_poll_os_hit: counts when OS poll found pending data
- xlio_stats shows n_rx_os_errors: errors during OS polling (should be 0)
- High n_rx_poll_os_hit with high ratio suggests OS traffic is flowing and may benefit from lower ratio
- Low n_rx_poll_os_hit suggests ratio could be increased to reduce overhead
Default value is 100

performance.polling.rx_poll_on_tx_tcp
Maps to **XLIO_RX_POLL_ON_TX_TCP** environment variable.

**What This Does:**
When enabled, each TCP send() operation will opportunistically poll the RX completion queue to process incoming TCP ACKs before returning. This creates a tight feedback loop between transmission and acknowledgment processing, allowing send buffer space to be freed faster during data transmission.

**Note:** This parameter only affects POSIX API sockets (standard send/recv). XLIO Ultra API sockets with entity_context have their own polling mechanism and ignore this setting.

**Understanding the TCP Send Buffer Mechanism:**
TCP maintains a send buffer (controlled by SO_SNDBUF / network.protocols.tcp.wmem) that limits how much unacknowledged data can be outstanding. When you call send():
1. Data is copied into the send buffer and transmitted
2. The send buffer space is consumed (reducing sndbuf_available())
3. When the remote peer sends an ACK, that acknowledged data is released
4. Only then is the send buffer space restored for new data

Without rx_poll_on_tx_tcp, ACKs are only processed during:
- Explicit recv() calls
- epoll_wait()/poll()/select() calls
- XLIO internal timer events

This creates a potential bottleneck: if you're sending data in a tight loop without receiving, ACKs accumulate in the RX queue unprocessed, and your send buffer fills up.

**Value: false (Default) - Standard TX/RX Separation:**

Behavior:
- send() only transmits data and returns immediately
- ACK processing happens separately during RX operations
- Send buffer is released only when application polls for RX events

Benefits:
- Lower per-call latency for send() operations
- Predictable CPU usage - TX and RX are cleanly separated
- No wasted CPU cycles polling when no ACKs are pending
- Works well with event-driven architectures (epoll/poll)
- Better for applications that naturally interleave TX and RX

Drawbacks:
- Send buffer can fill up faster in TX-heavy workloads
- Blocking sends may stall waiting for send buffer space
- Congestion window growth may be slower (ACKs drive cwnd increase)
- May require explicit RX polling to maintain TX throughput

Best for:
- Event-driven applications using epoll/poll for both TX and RX
- Applications with natural request/response patterns
- Low-latency scenarios where per-call overhead matters
- Applications already polling RX frequently
- XLIO Ultra API applications (this setting is ignored anyway)

**Value: true - Opportunistic ACK Processing on TX:**

Behavior:
- Each send() call first polls the RX completion queue (CQ)
- Incoming ACKs are processed immediately, freeing send buffer space
- Then the actual transmission proceeds
- Creates a feedback loop: TX triggers ACK processing triggers more TX capacity

Benefits:
- Faster send buffer release during streaming workloads
- Better throughput for bulk data transfer scenarios
- Faster TCP congestion window (cwnd) growth - ACKs drive cwnd increase
- Reduced blocking time for blocking sockets when send buffer is full
- Self-sustaining TX flow - no need for separate RX polling to maintain TX
- Particularly effective with performance.threading.internal_handler.behavior=delegate

Drawbacks:
- Increased per-call latency for send() - each call does extra RX polling
- Additional CPU cycles even when no ACKs are pending
- Redundant work if application already polls RX frequently
- May increase jitter in send latency due to variable RX processing
- Extra lock contention on the RX ring during TX operations

Best for:
- Streaming/bulk data transfer applications
- TX-heavy workloads with minimal RX polling
- Blocking socket applications sending large volumes
- Applications using performance.threading.internal_handler.behavior=delegate mode
- Scenarios where maximizing throughput is more important than per-call latency
- Applications not using epoll for event-driven I/O

**Performance Tradeoffs:**

| Scenario | false (Default) | true |
|----------|-----------------|------|
| Per-send() latency | Lower | Higher (adds RX poll overhead) |
| Bulk transfer throughput | May stall if not polling RX | Better sustained throughput |
| CPU efficiency | Better (no redundant polling) | May waste cycles polling empty CQ |
| Send buffer utilization | May fill up in TX-heavy loops | Stays freed via ACK processing |
| cwnd growth rate | Depends on RX poll frequency | Faster (immediate ACK processing) |

**Interaction with Other Parameters:**
- performance.threading.internal_handler.behavior: With "delegate" mode, enabling rx_poll_on_tx_tcp is often recommended
- network.protocols.tcp.wmem: Larger send buffers reduce the impact of delayed ACK processing
- performance.polling.skip_cq_on_rx: Affects overall RX polling behavior

**Typical Use Cases:**

1. **High-throughput file transfer:** Enable (true) - Sustained TX benefits from immediate ACK processing
2. **Low-latency trading:** Disable (false) - Per-call latency matters more than throughput
3. **Event-driven web server:** Disable (false) - epoll already handles ACK processing
4. **Blocking socket bulk sender:** Enable (true) - Prevents send buffer stalls
5. **Request/response pattern:** Disable (false) - Natural TX/RX interleaving handles ACKs
Default value is false

performance.polling.skip_cq_on_rx
Maps to **XLIO_SKIP_POLL_IN_RX** environment variable.
Controls whether TCP socket recv() operations skip direct Completion Queue (CQ) polling.

**Background: How Packet Reception Works in XLIO**

When a TCP socket calls recv()/read()/recvmsg(), XLIO needs to check for available packets. By default, this involves:
1. Checking the socket's ready packet list (m_rx_pkt_ready_list)
2. If no packets ready, polling the hardware Completion Queue (CQ) via poll_and_process_element_rx()
3. Processing any completed packets from the CQ and moving them to socket ready lists
4. If still no packets and blocking, sleeping until CQ notification or timeout

CQ polling involves acquiring ring locks, accessing hardware CQ memory, and potentially processing multiple packets. This adds CPU cycles to every recv() call.

**Value: "disable" or 0 (Default) - Always Poll CQ in recv()**

Behavior:
- Every recv()/read() call on a TCP socket polls the CQ directly
- Socket gets the most up-to-date packet information immediately
- The is_readable() check (used by select/poll/epoll) also polls the CQ

Benefits:
- Lowest latency from packet arrival to application delivery
- Works correctly without epoll or other CQ polling mechanisms
- Self-contained: each socket handles its own packet reception
- Best for applications doing direct recv() without event loops

Drawbacks:
- Higher CPU overhead per recv() call (CQ polling, lock acquisition)
- Redundant work if epoll is also polling the same CQ
- Lock contention when multiple sockets share a ring
- More syscall-like overhead even when packets are already ready

Best for:
- Applications not using epoll (direct blocking recv patterns)
- Low-frequency recv() calls where per-call overhead doesn't matter
- Scenarios requiring lowest possible latency to first byte
- Applications with few sockets per ring (per_socket allocation)
- Request-response patterns where each recv() is meaningful

**Value: "enable" or 1 - Always Skip CQ Polling in recv()**

Behavior:
- recv()/read() calls NEVER poll the CQ directly
- Only checks if packets are already in the ready queue (m_rx_pkt_ready_list)
- If no packets ready, immediately sets errno=EAGAIN and returns -1 (for non-blocking)
- For blocking sockets, goes directly to sleep waiting for CQ notification
- The is_readable() check also skips CQ polling, returning false immediately if no ready packets
- REQUIRES external CQ polling mechanism (epoll_wait, worker threads, or internal thread)

Benefits:
- Fastest recv() path: minimal overhead, no CQ polling or lock acquisition
- Eliminates redundant CQ polling when using epoll
- Reduces CPU usage for high-frequency recv() calls
- Better performance for epoll-based event loops
- Lower lock contention on shared rings

Drawbacks:
- Packets may not be visible until epoll_wait() or similar polls the CQ
- Slightly higher latency from packet arrival to availability in recv()
- Can cause recv() to return EAGAIN even when packets exist in hardware CQ
- INCORRECT BEHAVIOR if used without epoll: recv() may spin returning EAGAIN
- Breaks applications that rely on recv() to drive packet processing

Best for:
- Event-driven servers using epoll_wait() as primary event loop
- High-frequency recv() patterns (many small reads)
- Applications where epoll already polls CQs centrally
- Worker thread mode where background threads poll CQs
- Reducing CPU usage in recv-heavy workloads

**Value: "enable_epoll_only" or 2 - Skip CQ Polling Only When Socket is in epoll**

Behavior:
- When socket is NOT in any epoll instance: behaves like "disable" (0) - polls CQ in recv()
- When socket IS added to an epoll instance (epoll_ctl ADD): switches to skip CQ polling
- When socket is removed from epoll (epoll_ctl DEL): switches back to polling CQ
- Dynamically adapts based on socket's epoll membership

Benefits:
- Best of both worlds: auto-detects the right behavior
- Safe default for mixed-mode applications
- Works correctly even if socket is sometimes used without epoll
- No need to know application architecture in advance
- Provides optimization when epoll is used, correctness when not

Drawbacks:
- Slightly more complex behavior to understand
- State changes when socket enters/leaves epoll
- Minor overhead tracking epoll membership
- May not be optimal for all edge cases

Best for:
- Applications with unknown or mixed I/O patterns
- Libraries that don't control how sockets are used
- Gradual migration from blocking to event-driven I/O
- When correctness is more important than squeezing last bit of performance
- Production deployments where safety matters

**Performance Tradeoffs Summary:**

| Scenario | disable (0) | enable (1) | enable_epoll_only (2) |
|----------|-------------|------------|----------------------|
| recv() CPU overhead | Higher (CQ poll) | Lowest (no CQ poll) | Adaptive |
| Latency to first byte | Lowest | Higher (waits for epoll) | Adaptive |
| Works without epoll | Yes | NO - breaks | Yes |
| Works with epoll | Yes (redundant work) | Optimal | Optimal |
| Lock contention | Higher | Lower | Adaptive |
| Safety | Always correct | Requires epoll | Always correct |

**Detailed Performance Characteristics:**

*CPU Cycles per recv() call (approximate):*
- disable (0): ~200-500 cycles (CQ poll + lock + memory access)
- enable (1): ~50-100 cycles (just ready list check)
- enable_epoll_only (2): Same as (0) or (1) depending on epoll state

*Latency Impact:*
- With disable (0): Packet available immediately when CQ has completion
- With enable (1): Packet available after next epoll_wait() polls CQ
  - If epoll_wait is called frequently (< 1ms): minimal impact
  - If epoll_wait has long timeout: packets delayed until timeout or other events

*Throughput Impact:*
- High-frequency small recv() patterns: enable (1) can improve throughput 10-30%
- Bulk recv() patterns: minimal difference (CQ poll overhead amortized)

**Interaction with Other Parameters:**

- performance.polling.rx_cq_wait_ctrl: Complementary optimization for CQ-fd registration
- performance.rings.rx.allocation_logic: With per_thread/per_core, fewer sockets share rings, reducing lock benefit
- performance.threading.worker_threads: Worker threads can poll CQs, making enable (1) safer
- performance.threading.internal_handler.behavior: delegate mode provides CQ polling

**Recommended Configurations:**

*For Nginx/Envoy/HAProxy (event-driven):*
- Use "enable" (1) or "enable_epoll_only" (2)
- These applications use epoll_wait as their main event loop
- Benefit from reduced recv() overhead

*For custom applications with epoll:*
- Use "enable_epoll_only" (2) for safety
- Or "enable" (1) if you're certain all sockets are in epoll

*For applications without epoll:*
- Use "disable" (0) - the only correct option
- Or "enable_epoll_only" (2) which auto-detects

*For latency-critical trading applications:*
- Use "disable" (0) to ensure immediate packet visibility
- Every microsecond of latency from packet arrival to recv() matters

*For high-throughput servers with many connections:*
- Use "enable" (1) with epoll to minimize CPU usage
- Combine with rx_cq_wait_ctrl=true for best scalability
Default value is 0

performance.polling.yield_on_poll
Maps to **XLIO_RX_POLL_YIELD** environment variable.
Controls voluntary CPU yielding during the busy-polling phase of receive operations (recv, read, recvfrom, recvmsg), enabling cooperative multitasking when multiple threads share CPU cores.

**Background: The Polling Loop**

When XLIO performs a blocking receive operation, it enters a busy-poll loop controlled by performance.polling.blocking_rx_poll_usec. During this loop, the thread continuously polls hardware completion queues (CQs) for incoming packets. This polling is very fast (~100-500ns per iteration) and provides the lowest possible latency when packets arrive during the poll window.

However, this busy-polling monopolizes the CPU core. In multi-threaded applications where multiple threads share cores (common when thread count exceeds core count), this can cause thread starvation - other threads waiting for CPU time cannot make progress, potentially missing their own incoming packets.

**How yield_on_poll Works**

This parameter inserts periodic sched_yield() calls into the polling loop, voluntarily releasing the CPU to allow other threads to run.

*For UDP sockets:*
Yields every N iterations, specifically on iterations N-1, 2N-1, 3N-1, etc.
Example with value=10: yields on poll iterations 9, 19, 29, 39...

*For TCP sockets:*
Yields on EVERY iteration after the threshold is exceeded.
Example with value=10: no yield for iterations 0-10, then yields on 11, 12, 13...

**Value Meanings**

- **0 (Disabled - Default):**
  No yielding occurs during the entire polling phase.
  
  *Best for:*
  - Dedicated cores per thread (thread count <= core count)
  - Latency-critical applications where every microsecond matters
  - Applications using CPU affinity (taskset, numactl) to pin threads
  - Real-time trading, low-latency messaging systems
  
  *Characteristics:*
  - Lowest latency variance (no context switch jitter)
  - Maximum CPU utilization per thread during polling
  - Will starve other threads on shared cores
  - Thread holds CPU for full blocking_rx_poll_usec duration

- **Low values (1-100):**
  Frequent yielding, maximum fairness.
  
  *Best for:*
  - Heavily oversubscribed systems (many threads per core)
  - Mixed workloads where non-XLIO threads need CPU time
  - Development/testing environments with limited cores
  - Containerized deployments with CPU limits
  
  *Characteristics:*
  - Very frequent context switches during polling
  - Excellent fairness between threads
  - Higher latency variance due to context switch overhead (~1-10μs per switch)
  - Reduced CPU monopolization
  - Value of 1: yields on nearly every poll iteration (maximum sharing)
  - Value of 10: yields every ~10 iterations

- **Medium values (100-1000):**
  Balanced approach between latency and fairness.
  
  *Best for:*
  - Moderately oversubscribed systems (2-4x threads per core)
  - Applications with mixed latency requirements
  - General-purpose server workloads
  
  *Characteristics:*
  - Occasional context switches during polling
  - Good balance between latency and fairness
  - Value of 100: yields every ~100 iterations
  - Value of 1000: yields every ~1000 iterations

- **High values (1000+):**
  Infrequent yielding, latency-focused with minimal fairness.
  
  *Best for:*
  - Near-dedicated core scenarios with occasional sharing needs
  - Applications where latency is primary but fairness occasionally matters
  
  *Characteristics:*
  - Rare context switches during polling
  - Most of polling phase runs without interruption
  - Minimal fairness improvement over disabled (0)

**Performance Impact Analysis**

*Latency Impact:*
Each sched_yield() can add 1-10 microseconds of delay (varies by OS scheduler, CPU load, and whether another thread is ready to run). With blocking_rx_poll_usec=100000 (default):
- value=10: up to ~10,000 yields possible, potentially adding 10-100ms total
- value=100: up to ~1,000 yields possible, potentially adding 1-10ms total
- value=1000: up to ~100 yields possible, potentially adding 0.1-1ms total

*Throughput Impact:*
Yielding reduces effective polling density, meaning fewer CQ checks per unit time. This can reduce packet processing throughput in high-rate scenarios.

*CPU Impact:*
Yielding reduces per-thread CPU usage, potentially lowering overall system utilization when many threads are polling simultaneously.

**Interaction with Other Parameters**

- **performance.polling.blocking_rx_poll_usec:**
  Controls total polling iterations. yield_on_poll only has effect while within this polling budget. If blocking_rx_poll_usec=0 (interrupt-driven), yield_on_poll has no effect.

- **performance.rings.rx.allocation_logic:**
  With per_thread or per_socket ring allocation, each thread polls its own ring. Yielding allows other threads to poll their rings. With per_interface, threads share rings and may benefit more from yielding to reduce lock contention.

- **performance.threading.worker_threads:**
  Worker thread mode has different polling semantics. This parameter primarily affects POSIX API application threads.

**Recommended Configurations**

| Scenario | Recommended Value |
|----------|-------------------|
| Dedicated cores, ultra-low latency | 0 (disabled) |
| Thread count = core count | 0 (disabled) |
| Thread count = 2x core count | 500-1000 |
| Thread count = 4x core count | 100-500 |
| Thread count >> core count | 10-100 |
| Containerized with CPU limits | 50-200 |
| Mixed XLIO/non-XLIO threads | 100-500 |

**Example Scenarios**

*Scenario 1: 8 XLIO threads on 8 cores*
Use value=0. Each thread has its own core, no contention.

*Scenario 2: 32 XLIO threads on 8 cores*
Use value=100-500. 4 threads compete per core; yielding prevents starvation while maintaining reasonable latency.

*Scenario 3: Trading application with strict latency SLA*
Use value=0 with thread pinning (numactl/taskset). Accept CPU monopolization for minimum latency variance.

*Scenario 4: Microservices in Kubernetes with 2 CPU limit*
Use value=50-100. Aggressive yielding ensures fair CPU sharing within the container's allocation.
Default value is 0

performance.rings.max_per_interface
Maps to **XLIO_RING_LIMIT_PER_INTERFACE** environment variable.
Limits the number of rings that can be allocated per network interface.
When 0 (default), unlimited rings are created based on the ring allocation logic
(e.g., one ring per thread, per socket, or per core).

**What is a ring?**
A ring is a fundamental XLIO data structure containing:
- Queue Pairs (QPs) for hardware-accelerated send/receive operations
- Completion Queues (CQs) for operation completion notifications
- Work Request Elements (WREs) - pre-allocated packet buffers
- Dedicated per-ring locks (m_lock_ring_rx, m_lock_ring_tx) for thread-safe access
- Local buffer caches for TX/RX operations

Each ring consumes significant resources:
- Memory: ~300+ MB per ring (varies by WRE configuration)
- Hardware: Dedicated QP and CQ resources on the NIC
- File descriptors: Completion event channels

**How the limit works:**
When a socket/thread requests a ring and the limit has been reached:
1. The request is redirected to an existing ring with the minimum reference count (load balancing)
2. Multiple sockets/threads share the same ring and its resources
3. Shared rings use locking to ensure thread-safe access

**Value of 0 (unlimited rings - default):**
Benefits:
- No per-ring lock contention - each thread/socket has its own dedicated ring
- Full hardware parallelism - independent QPs and CQs per ring
- Better data locality - each thread accesses only its own ring
- Optimal for applications where threads rarely share resources

Drawbacks:
- High memory consumption - total memory scales linearly with ring count
- Global buffer pool lock contention - all rings share global buffer pools
  (g_buffer_pool_rx_ptr, g_buffer_pool_tx) protected by a single lock.
  With many rings (e.g., 64+), threads compete heavily for buffer allocation.
- Can cause scalability issues: 32+ workers may perform WORSE than 16 workers
  due to buffer pool contention overwhelming the benefits of ring isolation.
- Higher hardware resource usage (QPs, CQs, memory regions)

**Low values (e.g., 4-16 rings):**
Benefits:
- Reduced memory footprint - fewer rings = less total memory
- Less global buffer pool contention - fewer concurrent buffer allocators
- Better scalability at high thread counts - fixes the "more workers = worse performance" problem
- More efficient hardware resource utilization

Drawbacks:
- Per-ring lock contention - multiple threads compete for shared ring locks
- Potential serialization - packet processing may serialize when threads share rings
- Reduced hardware-level parallelism

**Performance guidance:**

For event-driven applications (Nginx, HAProxy):
- Workers spend most time in epoll, rarely accessing rings simultaneously
- Ring sharing is nearly free due to low overlap
- Recommended: Set to worker_count / 2 (e.g., 16 for 32 workers)
- Expected improvement: 15-25% throughput increase at high worker counts

For CPU-intensive per-connection applications:
- Threads actively process packets continuously
- Ring sharing causes significant lock contention
- Recommended: Keep at 0 (unlimited) or set equal to thread count

For memory-constrained environments:
- Each ring uses ~300+ MB
- Set to minimum viable value (4-8) to reduce footprint
- Trade some latency consistency for memory savings

**Dual-port / multi-interface scenarios:**
The limit applies PER INTERFACE. With 2 ports and limit=16:
- Port 1: max 16 rings
- Port 2: max 16 rings
- Total: up to 32 rings

**Tuning strategy:**
1. Start with limit = number_of_threads / 2
2. Benchmark throughput and latency
3. If per-ring lock contention is observed (via xlio_stats), increase the limit
4. If global buffer pool contention persists, decrease the limit
5. Find the optimal balance for your specific workload

**Monitoring:**
Use xlio_stats to observe:
- n_buffer_pool_no_bufs: Buffer exhaustion events (high = increase memory or reduce rings)
- Per-ring packet counts: Verify load is balanced across shared rings
- Ring lock contention: High contention suggests too few rings

**Example configurations:**
- 32 Nginx workers, dual-port: XLIO_RING_LIMIT_PER_INTERFACE=16
- 8 CPU-intensive threads: XLIO_RING_LIMIT_PER_INTERFACE=0 (unlimited)
- Memory-limited environment: XLIO_RING_LIMIT_PER_INTERFACE=4
Default value is 0

performance.rings.rx.allocation_logic
Maps to **XLIO_RING_ALLOCATION_LOGIC_RX** environment variable.
Controls how reception rings are allocated and separated across sockets, threads, or cores.

**What is a Ring?**
A ring is a fundamental XLIO data structure containing:
- Queue Pairs (QPs) for hardware-accelerated send/receive operations
- Completion Queues (CQs) for completion notifications
- Work Request Elements (WREs) - pre-allocated packet buffers
- Dedicated per-ring locks (m_lock_ring_rx) for thread-safe access
- Local buffer caches for RX operations

Each ring consumes significant resources:
- Memory: ~300+ MB per ring (varies by WRE configuration)
- Hardware: Dedicated QP and CQ resources on the NIC
- File descriptors: Completion event channels

**The Logic Options (Low to High values):**

- "per_interface" or 0 - Ring per interface (FEWEST RINGS)
  All sockets on the same network interface share a single RX ring.
  Key: Always 0 (constant).

- "per_ip_address" or 1 - Ring per IP address
  Sockets binding to the same local IP share a ring.
  Key: Hash of the IP address.

- "per_socket" or 10 - Ring per socket (MOST RINGS)
  Each socket file descriptor gets its own dedicated ring.
  Key: Socket file descriptor number.

- "per_thread" or 20 - Ring per thread (DEFAULT)
  Each thread gets its own ring, shared by all sockets created in that thread.
  Key: pthread_self() thread ID.

- "per_cpuid" or 30 - Ring per CPU core
  Ring determined by the CPU core executing the socket operation.
  Key: sched_getcpu() at time of ring allocation.

- "per_core" or 31 - Ring per core with thread pinning
  Like per_cpuid, but XLIO also pins the thread to that CPU core
  for consistent ring assignment. Uses cpu_manager to balance threads.

**Performance Tradeoffs:**

*Low values (0, 1 - per_interface, per_ip_address):*
Benefits:
- Minimal memory footprint (fewest rings = least memory)
- Less global buffer pool contention (fewer rings competing for global buffer pools)
- Can scale better at VERY HIGH thread counts (32+) where buffer pool lock contention dominates

Drawbacks:
- HIGH per-ring lock contention: multiple threads compete for the same ring lock
- Serializes packet reception: poll_and_process_element_rx() uses trylock()
  which returns immediately if lock is held, potentially missing packets
- Reduces hardware parallelism: single QP/CQ handles all traffic
- May significantly hurt applications using select()/poll() (see warning below)

*High values (10, 20, 30, 31 - per_socket, per_thread, per_core):*
Benefits:
- NO per-ring lock contention: each thread/socket has its own dedicated ring
- Full hardware parallelism: independent QPs and CQs per ring
- Better data locality: each thread accesses only its own ring structures
- Lower latency variability: no waiting for ring locks
- Better receive side scaling: packets distributed across multiple CQs

Drawbacks:
- HIGH memory consumption: total memory scales linearly with ring count
  (e.g., 32 threads × 2 ports × 300MB = ~19GB)
- Global buffer pool lock contention: ALL rings share global buffer pools
  (g_buffer_pool_rx_ptr) protected by a single lock. With many rings (32+),
  threads compete heavily for buffer allocation, potentially causing
  WORSE performance than fewer threads.
- Higher hardware resource usage (more QPs, CQs, memory regions)

**Warning for select()/poll() applications:**
Applications using select() or poll() require polling ALL rings to check for data.
With many rings, global_ring_poll_and_process_element() iterates through all rings,
creating overhead proportional to ring count. Consider using per_interface (0) or
limiting rings via performance.rings.max_per_interface for such applications.

**Recommended Configurations:**

For event-driven servers (Nginx, HAProxy with epoll):
- Use per_thread (20) with performance.rings.max_per_interface set to worker_count/2
- Workers spend most time in epoll, reducing ring access overlap
- Expected: Better scalability at high worker counts

For CPU-intensive per-connection applications:
- Use per_thread (20) or per_socket (10) with unlimited rings
- Threads actively process packets continuously
- Ring sharing would cause significant lock contention

For memory-constrained environments:
- Use per_interface (0) or per_ip_address (1)
- Trade latency consistency for memory savings

For NUMA-aware deployments:
- Use per_cpuid (30) or per_core (31)
- Ensures memory locality between ring buffers and processing cores
- Combine with proper numactl binding for best results

**Interaction with other parameters:**
- performance.rings.max_per_interface: Limits total rings regardless of logic
- performance.rings.rx.migration_ratio: Enables automatic ring migration for per_thread/per_core modes
  (allows sockets to move to the ring of the thread currently processing them)
Default value is 20

performance.rings.rx.migration_ratio
Maps to **XLIO_RING_MIGRATION_RATIO_RX** environment variable.
Controls when to replace a socket's RX ring with the current thread's ring.

**When Does Migration Apply?**
Migration only works when allocation_logic is set to:
- per_thread (20)
- per_cpuid (30)
- per_core (31)

It has no effect for per_interface, per_ip_address, or per_socket modes.

**The Two-Phase Migration Algorithm:**

1. **Detection Phase (every `migration_ratio` ring accesses):**
   Every time your application receives data (recv/recvfrom/recvmsg/poll),
   XLIO increments a counter. When this counter reaches `migration_ratio`,
   XLIO checks if the current thread/CPU differs from the ring's assigned
   thread/CPU.

2. **Stability Phase (20 additional ring accesses):**
   If a mismatch is detected, the new thread becomes a "migration candidate".
   Migration does NOT happen immediately. The candidate must remain stable for
   20 consecutive ring accesses. If the socket bounces back to the original
   thread during these 20 accesses, the process resets.

**Total accesses before migration = migration_ratio + 20**
Example: If migration_ratio=100, at least 120 recv/poll operations are needed
before the socket actually migrates to a new ring.

**What Happens During Migration (Expensive!):**
- Acquiring locks on RX queue and migration lock
- Releasing the old ring (may deallocate if last user)
- Reserving a new ring (may allocate new QP/CQ)
- Moving pending receive descriptors between rings
- Re-registering flow steering rules with new ring
- Updating socket's ring map entries

**RX Migration is Particularly Impactful Because:**
- Hardware flow steering rules must be updated (HW reconfiguration)
- Pending packets in the old ring's CQ must be drained
- Buffer ownership must be transferred between rings

**Value Tradeoffs:**

*Low values (1-100):*
- Pros: Faster adaptation to thread affinity changes; socket quickly moves
  to the "correct" ring when application migrates work between threads.
  Incoming packets will be delivered to the current thread's CQ faster.
- Cons: Higher per-packet overhead; XLIO checks thread ID on every poll.
  May trigger unnecessary migrations and flow steering updates in apps
  with transient thread borrowing patterns.
- Best for: Applications that permanently move sockets between threads
  (e.g., connection hand-off patterns, work stealing schedulers).

*High values (500-10000):*
- Pros: Lower per-packet overhead; checks are infrequent.
- Cons: Slower adaptation; socket continues polling the "wrong" ring.
  This causes cross-CPU cache misses when accessing CQ and buffer
  structures, and NUMA penalties if ring is on remote socket.
- Best for: Applications with mostly stable thread affinity but occasional
  thread migrations. Balances adaptation capability with low overhead.

*Value of -1 (disabled, DEFAULT):*
- Pros: Zero overhead; no thread ID checking, no migration attempts.
  No flow steering rule updates. Optimal for single-threaded apps or
  when sockets never change threads.
- Cons: No adaptation capability. If a socket is created in thread A but
  consistently polled from thread B, it will always use thread A's ring.
  Thread B must poll thread A's CQ, causing cache misses and potential
  lock contention.
- Best for: Single-threaded applications, or applications where each socket
  is exclusively polled by its creating thread.

**Performance Impact of Wrong Ring (RX specific):**
Using an RX ring from a different thread/CPU causes:
1. CQ polling from remote CPU: Cache lines containing CQ entries must be
   fetched from the ring owner's CPU cache.
2. Receive buffer access latency: Packet data buffers are allocated from
   the ring owner's buffer pool, potentially on a different NUMA node.
3. Lock contention: Current thread competes with ring owner for ring lock
   during poll operations.
4. Reduced receive throughput: Cross-CPU synchronization overhead.

**Recommended Settings:**

- Single-threaded app: -1 (disabled)
- Thread-per-connection server: -1 (sockets stay with their thread)
- Work-stealing scheduler: 100-500 (balance adaptation vs overhead)
- Connection hand-off pattern: 50-100 (quick adaptation needed)
- epoll-based server with thread pool: Consider 200-500 if threads
  dynamically pick up connections from the epoll set
- Debugging/testing: 10 (see migrations happen quickly)
Default value is -1

performance.rings.rx.post_batch_size
Maps to **XLIO_RX_WRE_BATCHING** environment variable.

**What This Does:**
Controls how many receive buffers XLIO accumulates before posting them back to the hardware Receive Queue (RQ) in a single batch. This batching mechanism trades off between doorbell write frequency (CPU overhead) and buffer replenishment granularity (latency consistency).

**How the Mechanism Works:**
1. **Debt Tracking:** XLIO maintains a 'm_debt' counter that tracks how many buffers have been consumed from the RQ since the last batch post.
2. **Accumulation Phase:** As packets arrive and are processed, buffers are consumed and m_debt increments. Processed buffers go to the local rx_pool cache.
3. **Batch Trigger:** When m_debt reaches post_batch_size, XLIO triggers batch replenishment.
4. **Batch Posting:** XLIO fetches buffers from rx_pool (or global pool if needed), populates the WRE array with buffer descriptors, and posts the entire batch to the hardware RQ.
5. **Doorbell Write:** A single doorbell record update notifies the NIC that new receive buffers are available.

The key insight: one doorbell write posts post_batch_size buffers, amortizing the cost of the memory barrier and PCIe transaction across many buffers.

**What Happens During Batch Posting:**
- post_recv_buffers() iterates through buffers, populating m_ibv_rx_wr_array[]
- Each buffer's address, length, and lkey are written to scatter-gather entries
- When the array is full (post_batch_size entries), xlio_raw_post_recv() is called
- xlio_raw_post_recv() writes descriptors to the RQ ring buffer
- A write memory barrier (wmb) ensures descriptors are visible before doorbell
- Doorbell record update: *m_rq_data.dbrec = htonl(m_rq_data.head & 0xffff)
- NIC sees the doorbell and knows new receive buffers are available

**Value Tradeoffs:**

*High values (512-1024, default) - Best for throughput:*

Benefits:
- Fewer doorbell writes per packet → lower CPU overhead
- Better amortization of memory barrier cost (wmb) across many buffers
- Reduced PCIe transaction overhead (fewer doorbell writes)
- Better CPU cache utilization - batch processing keeps data hot in L1/L2
- Higher sustained packets-per-second (PPS)
- Fewer lock acquisitions on buffer pools (fetch buffers in bulk)

Drawbacks:
- Higher latency variance (jitter): when replenishment happens, the processing burst is larger
- Longer time between RQ replenishments → RQ level fluctuates more
- Under light load, buffers may sit in rx_pool longer before recycling
- If application polls infrequently, RQ may run low before replenishment triggers
- The burst of work when posting can cause momentary latency spikes

Used by: Default profile (1024)
Best for: High-bandwidth streaming, bulk data transfer, throughput-oriented workloads

*Low values (1-16) - Best for latency consistency:*

Benefits:
- Lower latency standard deviation - more predictable, smoother timing
- Smaller processing bursts → more consistent per-packet processing time
- RQ level stays more stable - buffers replenished frequently in small batches
- Buffers recycled more quickly - shorter time from packet processed to buffer available
- Better for real-time applications that need predictable response times
- Easier to reason about worst-case latency bounds

Drawbacks:
- More doorbell writes per packet → higher CPU overhead (memory barriers, PCIe writes)
- More frequent buffer pool accesses → potential for more lock contention
- Lower maximum throughput due to per-batch overhead
- Each doorbell write incurs ~100-500ns overhead
- At very high PPS, doorbell writes can become a bottleneck

Used by: ULTRA_LATENCY profile (4), LATENCY profile (4) - only when Striding RQ disabled
Best for: High-frequency trading, real-time control systems, latency-sensitive applications

**Performance Impact Summary:**
| Metric                    | High Value (1024) | Low Value (4)    |
|---------------------------|-------------------|------------------|
| Doorbell writes/packet    | ~1/1024           | ~1/4             |
| CPU overhead              | Lower             | Higher           |
| Max throughput (PPS)      | Higher            | Lower            |
| Latency mean              | Slightly lower    | Slightly higher  |
| Latency std-dev (jitter)  | Higher            | Lower            |
| RQ buffer level stability | More variable     | More stable      |
| Worst-case latency        | Higher spikes     | More bounded     |

**Interaction with Other Parameters:**

- **ring_elements_count (RX_WRE):** System enforces: ring_elements_count > (post_batch_size × 2). If violated, ring_elements_count is automatically increased.
- **spare_buffers (QP_COMPENSATION_LEVEL):** The local rx_pool cache must have enough buffers to satisfy batch requests. spare_buffers should be >= post_batch_size.
- **Striding RQ mode:** With Striding RQ enabled (default), fewer WREs are posted overall because each WRE contains multiple strides. The batch size still applies to WRE posting, but effective buffer count per post is multiplied by strides_num.
- **cq_poll_batch_max:** Controls how many CQEs are polled per iteration. If cq_poll_batch_max < post_batch_size, multiple poll iterations are needed before batch posting triggers.

**Striding RQ Interaction:**
With Striding RQ enabled (default), the meaning shifts slightly:
- Instead of posting individual packet buffers, you're posting multi-stride WQE buffers
- Each WQE contains strides_num strides (default: 2048), each stride can hold a packet
- Effective receive capacity per batch post = post_batch_size × strides_num
- Default: 1024 × 2048 = 2 million+ receive slots per batch cycle
- This makes high post_batch_size values even more efficient with Striding RQ

**Tuning Recommendations:**

1. **For latency-sensitive applications (HFT, real-time):**
   - Use 4-16 to minimize jitter
   - Accept the throughput tradeoff for predictable timing
   - Consider disabling Striding RQ and using small ring_elements_count

2. **For throughput applications (streaming, bulk transfer):**
   - Keep default (1024) for optimal efficiency
   - High batch size amortizes doorbell overhead effectively

3. **For balanced workloads:**
   - Try 64-256 as a middle ground
   - Monitor both latency histograms and throughput metrics

4. **For debugging latency spikes:**
   - Reduce post_batch_size to identify if batch posting is causing jitter
   - Use perf/tracing to measure time spent in post_recv_buffers()

**Monitoring:**
- xlio_stats doesn't directly show batch posting frequency
- Profile compensate_qp_poll_success() and post_recv_buffers() with perf
- Monitor RQ buffer levels via internal debugging if available
- Latency histograms reveal jitter patterns caused by batch processing
Default value is 1024

performance.rings.rx.ring_elements_count
Maps to **XLIO_RX_WRE** environment variable.

**What This Does:**
Controls the size of the RX Receive Queue (RQ) for each receive QP.
The Receive Queue is where Work Request Elements (WREs) are posted to receive
incoming packets. Each WRE represents a buffer (or set of buffers with Striding RQ)
that the NIC can use to store incoming packet data.

**Interaction with Striding RQ (Default: Enabled):**
The effective receive capacity depends on whether Striding RQ is enabled:

*With Striding RQ (hardware_features.striding_rq.enable=true, DEFAULT):*
- Default: 128 WREs
- Each WRE contains multiple strides (default: 2048 strides per WRE)
- Effective receive slots: ring_elements_count × strides_num = 128 × 2048 = 262,144
- Memory per WRE: strides_num × stride_size = 2048 × 64 = 128KB

*Without Striding RQ (hardware_features.striding_rq.enable=false):*
- Default: 32768 WREs
- Each WRE holds one packet
- Memory per WRE: ~2KB (MTU-sized buffer)

**How It Affects Reception:**
When packets arrive, the NIC writes data into the next available receive buffer
and generates a completion. If all WREs are consumed (queue is empty) before the
application replenishes them, the NIC has nowhere to put incoming packets.

Unlike TX drops (which XLIO can track), RX overruns happen in hardware:
- Packets are silently dropped by the NIC
- No direct counter visible to application
- Monitor via ethtool -S (rx_out_of_buffer, rx_discards_phy)

**Memory Impact:**
With Striding RQ: Memory per ring ≈ ring_elements_count × strides_num × stride_size
Example: 128 × 2048 × 64 = 16MB per ring

Without Striding RQ: Memory per ring ≈ ring_elements_count × buffer_size
Example: 32768 × 2048 ≈ 64MB per ring

**Relationship with post_batch_size:**
The system enforces: ring_elements_count > (post_batch_size × 2).
If violated, ring_elements_count is automatically increased.

**Profile-Specific Values:**
- Default (STRQ): 128 WREs (262K effective receive slots)
- Default (no STRQ): 32768 WREs
- Ultra-Latency (STRQ): 4 WREs (8K effective slots)
- Ultra-Latency (no STRQ): 256 WREs
- Nginx: 32 WREs (STRQ)

**Value Tradeoffs:**

*Low values (4-32 with STRQ, 256-512 without) - Best for latency:*
- Smaller completion queue = faster CQ polling
- Less memory per ring
- Faster buffer recycling
- Risk: May drop packets under burst traffic if application can't keep up
- Used by: ULTRA_LATENCY profile, LATENCY profile
- Best for: Low-latency trading, real-time systems

*Medium values (64-256 with STRQ, 4096-8192 without) - Balanced:*
- Good burst absorption
- Reasonable memory usage
- Best for: General purpose, web servers

*High values (512+ with STRQ, 16384+ without) - Best for throughput:*
- Large burst absorption capacity
- Handles packet storms without drops
- More memory consumed
- Larger CQ to poll = potential latency variance
- Used by: Default profile (128 STRQ / 32768 non-STRQ)
- Best for: High-bandwidth streaming, bulk receivers

**Tuning Recommendations:**
- Check ethtool -S for rx_out_of_buffer - non-zero indicates queue overruns
- For STRQ (default): Start with 128, reduce if memory constrained, increase if seeing drops
- For non-STRQ: Start with 8192-16384, adjust based on packet rate and drop counters
- Calculate total: value × (strides_num if STRQ) × stride_size × num_rings
Default value is 32768

performance.rings.rx.spare_buffers
Maps to **XLIO_QP_COMPENSATION_LEVEL** environment variable.

**What This Does:**
Controls the size of the local buffer cache (m_rx_pool) that each ring's Completion Queue
Manager maintains. This cache provides a ready supply of pre-allocated receive buffers
to quickly replenish the hardware Receive Queue (RQ) after packets are processed.

**How the Buffer Flow Works:**
1. Hardware NIC receives packets into buffers posted to the Receive Queue (RQ)
2. CQ Manager polls for completions and processes received packets
3. Processed buffers are returned to m_rx_pool (or directly to global pool)
4. When m_rx_pool is depleted, XLIO fetches spare_buffers from global pool (g_buffer_pool_rx_rwqe)
5. When m_rx_pool exceeds 2×spare_buffers, excess buffers return to global pool

This batching mechanism reduces lock contention on the global buffer pool.

**The Debt Mechanism:**
XLIO tracks 'm_debt' - the number of buffers consumed from RQ that need replenishment.
When debt reaches post_batch_size, buffers from m_rx_pool are posted back to the RQ.
If m_rx_pool is empty and global pool allocation fails:
- With performance.completion_queue.keep_full=true: current packet is dropped to recycle its buffer
- Without keep_full: debt accumulates until buffers become available
- If debt equals entire RQ size: packets must be dropped to prevent RQ starvation

**Memory Impact Per Ring:**
The local cache consumes: spare_buffers × buffer_size bytes per ring.
- With STRQ (default): buffer_size ≈ strides_num × stride_size = 128KB per WQE buffer
- Without STRQ: buffer_size ≈ MTU (~2KB for standard, ~9KB for jumbo frames)

Total memory tied in local caches = spare_buffers × buffer_size × number_of_rings
Example (STRQ, 4 rings, default 128): 128 × 128KB × 4 = 64MB in local caches

**High Values (larger spare_buffers) - Best for Burst Tolerance:**

Benefits:
- Fewer global pool lock acquisitions → less lock contention
- More resilient to traffic bursts - buffers ready locally without global pool access
- Lower latency variance - no waiting for global pool allocation during bursts
- Better tolerance when application holds buffers longer (slow processing)
- Handles scenarios where multiple packets arrive before any are fully processed

Drawbacks:
- Higher memory consumption per ring (memory tied up in local caches)
- With many rings (many threads), can exhaust total memory budget faster
- Memory is reserved even during idle periods

Recommended for: High packet rates, bursty traffic, applications with variable processing times

**Low Values (smaller spare_buffers) - Best for Memory Efficiency:**

Benefits:
- Lower memory footprint per ring
- More memory available in global pool for other rings
- Better memory utilization when traffic patterns vary across rings
- Suitable when rings have predictable, steady traffic

Drawbacks:
- More frequent global pool access → more lock contention on g_buffer_pool_rx_rwqe
- Higher latency variance during traffic bursts (must wait for global allocation)
- Increased risk of packet drops under sudden traffic spikes
- May see n_buffer_pool_no_bufs increments in statistics

Recommended for: Memory-constrained environments, steady low-rate traffic

**Relationship with Other Parameters:**
- Default is ring_elements_count / 2 (half the RQ size)
- Minimum enforced is post_batch_size (cannot be smaller than batch size)
- Works with performance.completion_queue.keep_full to determine drop behavior
- Interacts with ring allocation_logic - more rings = more total buffer memory

**Monitoring and Diagnostics:**
Use xlio_stats to monitor:
- n_buffer_pool_len: current local cache size per CQ
- n_rx_sw_pkt_drops: packets dropped due to buffer exhaustion
- n_buffer_pool_no_bufs: global pool allocation failures

If n_rx_sw_pkt_drops is non-zero, consider:
1. Increasing spare_buffers for better burst handling
2. Increasing XLIO_MEMORY_LIMIT if global pool is exhausted
3. Reducing number of rings if memory is constrained

**Default Values:**
- With Striding RQ (default): 128 (since fewer, larger WQE buffers)
- Without Striding RQ: 32768 (half of default ring_elements_count)

**Tuning Guidelines:**
- Start with default (ring_elements_count / 2)
- For latency-sensitive: ensure value handles expected burst size
- For memory-constrained: reduce to post_batch_size × 2 minimum
- For high-throughput: increase if seeing sw_pkt_drops or buffer_pool_no_bufs
- Calculate total: spare_buffers × buffer_size × num_rings × 2 (accounting for pool range)
Default value is 32768

performance.rings.rx.spare_strides
Maps to **XLIO_STRQ_STRIDES_COMPENSATION_LEVEL** environment variable.

**What This Does:**
Controls the size of the local stride descriptor cache (_stride_cache) that each ring's Striding RQ Completion Queue Manager maintains. This parameter only applies when Striding RQ is enabled (hardware_features.striding_rq.enable=true, the default).

**Understanding Stride Descriptors:**
In Striding RQ mode, received packets don't get individual buffers. Instead:
- Large WQE buffers (strides_num × stride_size bytes each) are posted to the hardware RQ
- Each packet occupies one or more 'strides' (slots) within a WQE buffer
- A stride descriptor (mem_buf_desc_t, ~300-400 bytes) is a metadata object that points into a portion of the WQE buffer, tracking packet location, size, timestamps, etc.

Stride descriptors are lightweight metadata objects - they do NOT contain actual packet data. The data buffers are managed separately via spare_buffers and the RWQE buffer pool.

**How the Stride Cache Works:**
1. When a packet arrives, the CQ manager needs a stride descriptor to represent it
2. Descriptors are fetched from the local _stride_cache (lock-free operation)
3. If _stride_cache is empty, XLIO fetches spare_strides descriptors from the global pool (g_buffer_pool_rx_stride) in one batch - this requires acquiring a lock
4. When a packet is fully processed, its stride descriptor returns to _stride_cache
5. If _stride_cache exceeds 2×spare_strides, excess descriptors are returned to the global pool (with lock)

This batching mechanism minimizes lock contention on the global stride pool.

**Memory Impact:**
Unlike spare_buffers (which manages large data buffers), spare_strides manages small metadata objects:
- Per-descriptor memory: ~300-400 bytes (sizeof(mem_buf_desc_t))
- Per-ring cache memory: spare_strides × ~350 bytes (can grow to 2× before trimming)
- With default 32768: ~11-23 MB per ring in stride descriptor metadata
- Total metadata memory = spare_strides × ~350 bytes × number_of_rings × (up to 2)

Example with 4 rings: 32768 × 350 × 4 × 2 = ~92 MB maximum in stride descriptors

**High Values (32768 - default) - Best for High Packet Rates:**

Benefits:
- Fewer global pool lock acquisitions → less lock contention between rings/threads
- Local cache almost never exhausted, even under packet bursts
- Better batch efficiency - large batches amortize the cost of lock acquisition
- Essential for high packet-per-second (PPS) workloads (millions of packets/sec)
- More resilient to traffic bursts - descriptors ready locally without global pool access
- Lower latency variance - no stalls waiting for global pool allocation

Drawbacks:
- Higher metadata memory consumption per ring (~23 MB per ring at 2× high watermark)
- With many rings (many threads), metadata memory can add up significantly
- Most descriptor memory sits idle during low-traffic periods
- Initial pool expansion may cause brief latency spike at startup

Recommended for:
- High packet rate workloads (100K+ packets/sec per ring)
- Multi-threaded applications where lock contention matters
- Bursty traffic patterns (financial trading, gaming, real-time systems)
- Systems with ample memory (servers with 32GB+ RAM)

**Low Values (512-4096) - Best for Memory Efficiency:**

Benefits:
- Lower metadata memory footprint per ring (~180KB-3MB per ring)
- More memory available for other purposes (application buffers, page cache)
- Suitable when packet rates are moderate or predictable
- Good for embedded or memory-constrained environments

Drawbacks:
- More frequent global pool accesses → increased lock contention
- Under high packet rates, frequent lock acquisition becomes a bottleneck
- Higher latency variance during traffic bursts (waiting for global pool)
- Risk of descriptor exhaustion under sudden traffic spikes (XLIO panics if pool empty)
- Performance degradation with many concurrent rings competing for descriptors

Recommended for:
- Memory-constrained environments
- Low to moderate packet rates (<50K packets/sec per ring)
- Single-threaded applications (no lock contention concern)
- Predictable, steady traffic patterns

**Relationship with Other Parameters:**
- Only effective when hardware_features.striding_rq.enable=true (default)
- Independent of spare_buffers (which manages actual data buffers, not descriptors)
- The global stride pool is infinite (expandable), but expansion has latency cost
- Initial pool size = 2 × spare_strides (pre-allocated at startup)
- Works per-ring: more rings = more total descriptor memory

**Interaction with Ring Allocation:**
With ring allocation_logic per_thread (default) and many threads:
- Total descriptor memory = spare_strides × 350 bytes × num_threads × num_interfaces × 2
- Example: 32768 × 350 × 16 threads × 2 interfaces × 2 = ~735 MB in stride descriptors
- Consider reducing spare_strides if this becomes significant

**Tuning Guidelines:**
1. **Start with default (32768)** - optimal for most high-performance scenarios
2. **For memory-constrained systems:**
   - Reduce to 4096-8192 for moderate packet rates
   - Reduce to 512-1024 for low packet rates (<10K pps)
3. **For extreme high PPS:**
   - Keep default or increase if seeing lock contention in profiling
4. **For many-ring scenarios (32+ threads):**
   - Consider reducing to 8192-16384 to balance memory vs contention
   - Use performance.rings.max_per_interface to limit total ring count

**Monitoring:**
The stride descriptor pool doesn't have direct xlio_stats counters, but:
- If XLIO panics with 'Unable to retrieve strides from global pool', increase this value
- Profile lock contention on g_buffer_pool_rx_stride if performance degrades
- Monitor total memory usage to ensure descriptor memory doesn't crowd out data buffers
Default value is 32768

performance.rings.tx.allocation_logic
Maps to **XLIO_RING_ALLOCATION_LOGIC_TX** environment variable.
Controls how transmission rings are allocated and separated across sockets, threads, or cores.

**What is a Ring?**
A ring is a fundamental XLIO data structure containing:
- Queue Pairs (QPs) for hardware-accelerated send/receive operations
- Completion Queues (CQs) for completion notifications
- Work Request Elements (WREs) - pre-allocated packet buffers
- Dedicated per-ring locks (m_lock_ring_tx) for thread-safe access
- Local buffer caches for TX operations

Each ring consumes significant resources:
- Memory: ~300+ MB per ring (varies by WRE configuration)
- Hardware: Dedicated QP and CQ resources on the NIC
- File descriptors: Completion event channels

**The Logic Options (Low to High values):**

- "per_interface" or 0 - Ring per interface (FEWEST RINGS)
  All sockets on the same network interface share a single TX ring.
  Key: Always 0 (constant).

- "per_ip_address" or 1 - Ring per IP address
  Sockets binding to the same local IP share a ring.
  Key: Hash of the IP address.

- "per_socket" or 10 - Ring per socket (MOST RINGS)
  Each socket file descriptor gets its own dedicated ring.
  Key: Socket file descriptor number.

- "per_thread" or 20 - Ring per thread (DEFAULT)
  Each thread gets its own ring, shared by all sockets created in that thread.
  Key: pthread_self() thread ID.

- "per_cpuid" or 30 - Ring per CPU core
  Ring determined by the CPU core executing the socket operation.
  Key: sched_getcpu() at time of ring allocation.

- "per_core" or 31 - Ring per core with thread pinning
  Like per_cpuid, but XLIO also pins the thread to that CPU core
  for consistent ring assignment. Uses cpu_manager to balance threads.

**Performance Tradeoffs:**

*Low values (0, 1 - per_interface, per_ip_address):*
Benefits:
- Minimal memory footprint (fewest rings = least memory)
- Less global buffer pool contention (fewer rings competing for global buffer pools)
- Can scale better at VERY HIGH thread counts (32+) where buffer pool lock contention dominates

Drawbacks:
- HIGH per-ring lock contention: multiple threads compete for the same ring lock
- Serializes packet transmission: only one thread can send at a time per ring
- Reduces hardware parallelism: single QP handles all traffic
- May significantly hurt applications using select()/poll() (see warning below)

*High values (10, 20, 30, 31 - per_socket, per_thread, per_core):*
Benefits:
- NO per-ring lock contention: each thread/socket has its own dedicated ring
- Full hardware parallelism: independent QPs and CQs per ring
- Better data locality: each thread accesses only its own ring structures
- Lower latency variability: no waiting for ring locks

Drawbacks:
- HIGH memory consumption: total memory scales linearly with ring count
  (e.g., 32 threads × 2 ports × 300MB = ~19GB)
- Global buffer pool lock contention: ALL rings share global buffer pools
  (g_buffer_pool_tx) protected by a single lock. With many rings (32+),
  threads compete heavily for buffer allocation, potentially causing
  WORSE performance than fewer threads.
- Higher hardware resource usage (more QPs, CQs, memory regions)

**Warning for select()/poll() applications:**
Applications using select() or poll() require polling ALL rings to check for completions.
With many rings, global_ring_poll_and_process_element() iterates through all rings,
creating overhead proportional to ring count. Consider using per_interface (0) or
limiting rings via performance.rings.max_per_interface for such applications.

**Recommended Configurations:**

For event-driven servers (Nginx, HAProxy with epoll):
- Use per_thread (20) with performance.rings.max_per_interface set to worker_count/2
- Workers spend most time in epoll, reducing ring access overlap
- Expected: Better scalability at high worker counts

For CPU-intensive per-connection applications:
- Use per_thread (20) or per_socket (10) with unlimited rings
- Threads actively process packets continuously
- Ring sharing would cause significant lock contention

For memory-constrained environments:
- Use per_interface (0) or per_ip_address (1)
- Trade latency consistency for memory savings

For NUMA-aware deployments:
- Use per_cpuid (30) or per_core (31)
- Ensures memory locality between ring buffers and processing cores
- Combine with proper numactl binding for best results

**Interaction with other parameters:**
- performance.rings.max_per_interface: Limits total rings regardless of logic
- performance.rings.tx.migration_ratio: Enables automatic ring migration for per_thread/per_core modes
- TSO (hardware_features.tcp.tso.enable): Forces migration_ratio to -1 (disabled) when TSO is enabled
Default value is 20

performance.rings.tx.completion_batch_size
Maps to **XLIO_TX_WRE_BATCHING** environment variable.

**What This Does:**
Controls how many TX Work Request Elements (WREs) are posted to the Send Queue
before XLIO requests a Completion Queue Entry (CQE) from the NIC. When a CQE
arrives, XLIO must process ALL accumulated WREs since the last completion -
releasing buffers back to the pool, returning SQ credits, and invoking callbacks.

**How It Works:**
When sending packets, each WRE can optionally set the MLX5_WQE_CTRL_CQ_UPDATE flag
to request a completion. XLIO uses a counter (m_n_unsignaled_count) to track how
many WREs were posted without requesting completion. When the counter reaches zero,
the next WRE requests a completion, and the counter resets to completion_batch_size-1.

**What Happens at Completion Time:**
When a CQE arrives, handle_sq_wqe_prop() iterates through a linked list of all WREs
posted since the last completion. For each WRE it:
- Returns the buffer descriptor to the TX buffer pool
- Handles TI (Transport Interface) callbacks for TLS/NVMe offloads
- Accumulates credits for batch return to the Send Queue
This batch processing means higher values cause longer completion processing bursts.

**Special Behaviors:**
- Zero-copy (ZCOPY) packets ALWAYS request immediate completion regardless of this
  setting, so the application is notified that user buffers can be reused.
- The first send operation always triggers a completion.
- After each signaled WRE, XLIO polls the CQ to process completions inline.

**Value Tradeoffs:**

*High values (32-64) - Best for throughput:*
- Higher PPS (packets per second) - less completion overhead per packet
- Lower average latency - fewer CQ polls interrupt the send path
- Better CPU efficiency - fewer interrupts and context switches
- BUT: Higher latency variance (jitter) - when completion finally arrives,
  many buffers must be processed in a burst, causing latency spikes
- More TX buffers held in-flight before release
- Longer completion processing time per CQE
- Used by: Default profile (64), NVME_BF3 profile (128 via custom override)
- Best for: Bulk data transfer, streaming, high-bandwidth applications

*Low values (1-8) - Best for latency consistency:*
- Lower latency standard deviation - more predictable, consistent timing
- Smoother buffer release - resources returned more frequently
- More responsive to backpressure - faster SQ credit recovery
- BUT: Lower PPS - more completion overhead per packet
- Higher CPU utilization - more frequent CQ polling
- Slightly higher average latency due to polling overhead
- Used by: ULTRA_LATENCY profile (4), LATENCY profile (4)
- Best for: High-frequency trading, real-time systems, latency-sensitive apps

**Performance Impact Summary:**
| Metric              | High Value (64) | Low Value (4) |
|---------------------|-----------------|---------------|
| Packets/sec (PPS)   | Higher          | Lower         |
| Average latency     | Lower           | Slightly higher |
| Latency std-dev     | Higher (spiky)  | Lower (smooth) |
| CPU utilization     | Lower           | Higher         |
| Buffers in-flight   | More            | Fewer          |

**Tuning Recommendations:**
- For latency-sensitive applications: Use 1-8 (profiles use 4)
- For throughput applications: Use default (64)
- For balanced workloads: Try 16-32 as a middle ground
- Monitor latency histograms, not just averages, to detect jitter
- Consider alongside ring_elements_count - system enforces:
  ring_elements_count > (completion_batch_size × 2)
Default value is 64

performance.rings.tx.max_inline_size
Maps to **XLIO_TX_MAX_INLINE** environment variable.

**What This Does:**
Controls the maximum packet size (in bytes) that can be transmitted using the
fast "inline" path, where packet data is embedded directly into the Work Queue
Entry (WQE) rather than requiring a separate DMA operation.

**How Packet Transmission Works:**
XLIO uses three transmission paths, in order of preference:

1. **WQE Inline Path** (packet ≤ max_inline_size):
   Packet data is COPIED directly into the WQE in Send Queue memory. The NIC
   reads data from the WQE itself - no separate memory fetch needed. This is
   the fastest path because when BlueFlame is available, the first 8 bytes are
   written directly to the NIC's BlueFlame register, triggering immediate send.
   The 18-byte Ethernet header is always inlined regardless of this setting.

2. **On-Device Memory Path** (max_on_device_memory > 0, buffer available):
   For packets exceeding inline size, data can be copied to NIC's internal
   memory. WQE contains a pointer to this ODM location. Eliminates PCIe DMA
   latency but requires completion processing to release the ODM buffer.

3. **Host DMA Path** (fallback for larger packets):
   WQE contains POINTERS to data buffers in host memory. The NIC must perform
   a PCIe DMA read to fetch the actual packet data, adding round-trip latency.

**Value Tradeoffs:**

*High Values (up to 884 bytes) - Optimize for latency:*
+ More packets use the fast inline path (no DMA delay)
+ Lower latency for small-to-medium packets
+ Fewer PCIe transactions and DMA operations
+ Better for latency-sensitive workloads (trading, real-time)
- Larger WQEs consume more Send Queue space
- Reduces maximum number of in-flight packets (queue depth)
- More CPU cycles spent on memcpy() to copy data into WQE
- At 884 bytes: ~3x fewer WQEs fit in SQ compared to default 204

*Low Values (including 0 to disable) - Optimize for throughput:*
+ Smaller WQEs = more WQEs fit in Send Queue
+ Higher queue depth = more packets can be in-flight simultaneously
+ Less CPU overhead on data path (no memcpy for large packets)
+ Better for bulk transfer and high-throughput scenarios
- More packets use DMA path = higher per-packet latency
- More PCIe transactions
- Only beneficial when typical packets exceed the inline threshold

*Default Value (204 bytes) - Balanced:*
The default is carefully chosen to match the BlueFlame buffer capacity:
- 1st WQEBB (64B): Control segment + Ethernet segment with 18B inline header
- Remaining: 3 WQEBBs (192B) + 12B from 1st WQEBB = 204B for data
This fits most control plane packets (TCP ACKs, small UDP messages) inline
while preserving reasonable queue depth for bulk data transfers.

**Queue Depth Impact:**
Higher inline size directly reduces max WQEs in Send Queue:
- At 204B: ~6000+ WQEs can fit (formula: SQ_size / WQE_size)
- At 884B: ~2000 WQEs can fit (3x reduction)
This affects how many packets can be outstanding before backpressure.

**Tuning Recommendations:**
- Latency-critical (HFT, real-time): Consider 400-884B if packets fit
- High throughput (bulk transfer): Keep default 204B or lower
- Mixed workloads: Default 204B is a good balance
- Large packets (>500B typical): Lower values won't hurt, may help throughput
- Small packets (<200B typical): Higher values reduce DMA overhead
- Consider with max_on_device_memory: ODM can help packets just above inline

**Note:** In older releases this parameter was called XLIO_MAX_INLINE.
Default value is 204

performance.rings.tx.max_on_device_memory
Maps to **XLIO_RING_DEV_MEM_TX** environment variable.

**What This Does:**
On Device Memory (ODM) stores egress packet data directly on the NIC's internal
memory instead of keeping it in host memory for DMA. This eliminates PCIe
round-trip latency for packets that exceed the WQE inline size (~204 bytes).

**How Packet Transmission Works:**
XLIO uses three paths for sending packet data, in order of preference:

1. **WQE Inline** (≤ max_inline_size, ~204 bytes):
   Packet data is embedded directly in the Work Queue Entry (WQE) in Send Queue
   memory. The NIC reads data from the SQ - no separate fetch needed. Fastest path.

2. **On Device Memory** (ODM enabled, buffer available):
   Packet data is copied to NIC's internal memory buffer. WQE contains a pointer
   to this ODM location. NIC reads from its local memory - no PCIe DMA needed.
   Eliminates host-to-NIC PCIe latency.

3. **Host DMA** (ODM disabled or buffer full):
   WQE contains a pointer to host memory. NIC must DMA the data across PCIe.
   Adds PCIe round-trip latency (~hundreds of nanoseconds).

**Buffer Behavior:**
ODM operates as a circular buffer per TX ring. When the buffer is full
(Out-of-Buffer/OOB event), XLIO gracefully falls back to host DMA - no packets
are lost, just slightly higher latency for those packets. Monitor OOB events
via xlio_stats (n_tx_dev_mem_oob counter).

**Hardware Limits:**
- Single-port HCA: 256KB total shared across all TX rings
- Dual-port HCA: 128KB total shared across all TX rings

**Value Tradeoffs:**

*Value of 0 (disabled, DEFAULT):*
- All packets larger than inline size use standard PCIe DMA from host memory
- No HCA memory consumption
- Best for: Memory-constrained environments, small-packet workloads,
  applications where latency variation is acceptable

*Low values (1KB-4KB):*
- Minimal ODM buffer, frequent fallback to host DMA under load
- Conserves HCA memory for other rings
- Best for: High ring count configurations, memory-conscious deployments,
  steady low-rate traffic patterns

*Medium values (8KB-16KB) - Recommended for latency-sensitive workloads:*
- Good balance between buffer capacity and memory usage
- Handles moderate bursts without OOB events
- Used by ULTRA_LATENCY and LATENCY profiles (16KB)
- Best for: Trading applications, real-time systems, request-response patterns

*High values (32KB-64KB):*
- Large burst absorption capacity, minimal OOB events
- More HCA memory consumed per ring
- Best for: Bursty traffic patterns, high-throughput single-ring configurations,
  applications prioritizing consistent latency over memory efficiency

*Maximum values (approaching HW limits):*
- Maximizes burst handling for a single ring
- May starve other rings of HCA memory
- Best for: Single-ring scenarios with extreme burst requirements

**When ODM Has No Effect:**
- On VMs without BlueFlame support (ODM is disabled on such systems due to
  observed performance issues with dm_copy operations)
- When packets fit within WQE inline size (already using optimal path)
- When HCA doesn't support device memory allocation

**Tuning Recommendations:**
- Start with 0 (disabled) and measure baseline latency
- Enable with 16KB for latency-sensitive workloads
- Monitor n_tx_dev_mem_oob stats - high OOB rates suggest increasing buffer
- Calculate total: (value × number_of_tx_rings) must fit within HW limit
- For multi-ring setups, divide HW limit appropriately across rings
Default value is 0

performance.rings.tx.migration_ratio
Maps to **XLIO_RING_MIGRATION_RATIO_TX** environment variable.
Controls when to replace a socket's TX ring with the current thread's ring.

**When Does Migration Apply?**
Migration only works when allocation_logic is set to:
- per_thread (20)
- per_cpuid (30)
- per_core (31)

It has no effect for per_interface, per_ip_address, or per_socket modes.

**The Two-Phase Migration Algorithm:**

1. **Detection Phase (every `migration_ratio` ring accesses):**
   Every time your application sends data (send/sendto/writev), XLIO increments a counter.
   When this counter reaches `migration_ratio`, XLIO checks if the current thread/CPU
   differs from the ring's assigned thread/CPU.

2. **Stability Phase (20 additional ring accesses):**
   If a mismatch is detected, the new thread becomes a "migration candidate".
   Migration does NOT happen immediately. The candidate must remain stable for
   20 consecutive ring accesses. If the socket bounces back to the original thread
   during these 20 accesses, the process resets.

**Total accesses before migration = migration_ratio + 20**
Example: If migration_ratio=100, at least 120 send operations are needed before
the socket actually migrates to a new ring.

**What Happens During Migration (Expensive!):**
- Acquiring locks on both old and new rings
- Releasing the old ring (may deallocate if last user)
- Reserving a new ring (may allocate new QP/CQ)
- Reallocating Send Gather Element (SGE) arrays
- Invalidating cached packet headers and state
- Potential flow steering rule updates

**Value Tradeoffs:**

*Low values (1-100):*
- Pros: Faster adaptation to thread affinity changes; socket quickly moves
  to the "correct" ring when application migrates work between threads.
- Cons: Higher per-packet overhead; XLIO checks thread ID more frequently.
  May trigger unnecessary migrations in applications with transient thread
  borrowing patterns.
- Best for: Applications that permanently move sockets between threads
  (e.g., connection hand-off patterns, work stealing schedulers).

*High values (500-10000):*
- Pros: Lower per-packet overhead; checks are infrequent.
- Cons: Slower adaptation; socket operates on the "wrong" ring longer when
  threads change. This causes cross-CPU cache misses and potential NUMA
  penalties if the ring's data structures are in a remote CPU's cache.
- Best for: Applications with mostly stable thread affinity but occasional
  thread migrations. Balances adaptation capability with low overhead.

*Value of -1 (disabled, DEFAULT):*
- Pros: Zero overhead; no thread ID checking, no migration attempts.
  Optimal for single-threaded apps or when sockets never change threads.
- Cons: No adaptation capability. If a socket is created in thread A but
  consistently used from thread B, it will always use thread A's ring,
  causing cache misses and lock contention with thread A's other sockets.
- Best for: Single-threaded applications, or applications where each socket
  is exclusively used by its creating thread.

**Performance Impact of Wrong Ring:**
Using a ring from a different thread/CPU than the current one causes:
1. L1/L2 cache misses: Ring data structures (locks, pointers, buffers) are
   cached on the ring owner's CPU, not the current CPU.
2. NUMA penalties: Cross-socket memory access adds 50-100+ ns latency.
3. False sharing: Multiple threads accessing the same ring's cache lines.
4. Lock contention: Current thread competes with ring owner for ring lock.

**Recommended Settings:**

- Single-threaded app: -1 (disabled)
- Thread-per-connection server: -1 (sockets stay with their thread)
- Work-stealing scheduler: 100-500 (balance adaptation vs overhead)
- Connection hand-off pattern: 50-100 (quick adaptation needed)
- Debugging/testing: 10 (see migrations happen quickly)
Default value is -1

performance.rings.tx.ring_elements_count
Maps to **XLIO_TX_WRE** environment variable.

**What This Does:**
Controls the size of the TX Send Queue (SQ) ring buffer for each transmit QP.
The Send Queue is a circular buffer where Work Request Elements (WREs) are posted
for the NIC to transmit. Each WRE is a packet descriptor that occupies 1-8 WQEBBs
(64 bytes each) depending on packet size and scatter-gather complexity.

**How It Affects Transmission:**
When an application sends data, XLIO posts WREs to the Send Queue. The NIC
asynchronously processes these WREs and generates completions. The queue depth
determines how many packets can be "in flight" (posted but not yet completed)
at any given time.

If the Send Queue fills up (all credits consumed) before completions free space:
- In non-blocking mode: packets are silently dropped (n_tx_dropped_wqes counter)
- In blocking mode: the send call blocks until space becomes available

**Memory Impact:**
Memory per ring ≈ ring_elements_count × 64 bytes (WQEBBs) + tracking structures.
With ring-per-thread allocation (default), total memory scales with thread count.
Example: 32768 WREs × 64 bytes × 16 threads × 2 interfaces ≈ 64MB for SQ alone.

**Hardware Limits:**
The value is automatically capped by the NIC's max_qp_wr capability.
If you request more than hardware supports, XLIO logs a warning and reduces
the value. Typical ConnectX limits are 8K-32K depending on configuration.

**Relationship with completion_batch_size:**
The system enforces: ring_elements_count > (completion_batch_size × 2).
If violated, ring_elements_count is automatically increased to satisfy this.

**Value Tradeoffs:**

*Low values (256-512) - Best for latency-sensitive applications:*
- Faster queue drain time on shutdown/error recovery
- Smaller completion batches = more predictable latency (lower std-dev)
- Lower memory footprint per ring
- Risk: Higher chance of SQ full under burst traffic = packet drops
- Used by: ULTRA_LATENCY profile (256), LATENCY profile (256)
- Best for: High-frequency trading, real-time systems prioritizing consistency

*Medium values (1024-4096) - Balanced profiles:*
- Good burst absorption with reasonable memory usage
- Moderate queue drain times
- Used by: NGINX profile (1024)
- Best for: Web servers, request-response workloads, general purpose

*High values (16384-32768) - Best for throughput:*
- Large burst absorption capacity
- More packets in flight = higher sustained throughput
- Longer completion processing cycles
- Higher memory consumption (especially with many rings)
- Risk: Increased latency variance under heavy load
- Used by: Default profile (32768)
- Best for: Bulk data transfer, streaming, high-bandwidth applications

**Tuning Recommendations:**
- Monitor n_tx_dropped_wqes via xlio_stats - non-zero indicates SQ full events
- For latency: Start low (256-512), increase only if seeing dropped WREs
- For throughput: Use default (32768) unless memory constrained
- Calculate total memory: value × 64 × num_rings × num_interfaces
- Consider ring allocation logic - per-thread creates more rings than per-interface
Default value is 32768

performance.rings.tx.tcp_buffer_batch
Maps to **XLIO_TX_BUFS_BATCH_TCP** environment variable.

**What This Does:**
Controls how many TX buffers a TCP socket fetches at once from the ring's
shared buffer pool. Each TCP connection maintains a local cache of pre-fetched
buffers to reduce contention on the shared pool.

**How Buffer Fetching Works:**
1. When sending TCP data, the socket first checks its local buffer cache
2. If the cache is empty, it acquires a lock on the ring's TX buffer pool
   and fetches 'tcp_buffer_batch' buffers in a single operation
3. One buffer is used for the current segment, the rest remain cached locally
4. TCP also supports zero-copy buffers with a separate cache (m_p_zc_mem_buf_desc_list)
5. When the connection closes, any unused cached buffers return to the pool

**TCP vs UDP Differences:**
- TCP connections are long-lived, so cached buffers are reused across many sends
- TCP supports zero-copy transmission with separate buffer management
- TCP has additional segment batching (see tcp_segments.socket_batch_size)
- Higher default (16 vs UDP's 8) reflects typical TCP streaming patterns

**Value Tradeoffs:**

*High Values (16, 32, 64+) - Optimize for throughput and streaming:*
+ Fewer lock acquisitions on the ring's shared buffer pool
+ More consistent latency (refill operations happen less frequently)
+ Better sustained throughput for streaming workloads
+ Reduced CPU overhead from lock contention in high-connection scenarios
+ Well-suited for long-lived connections sending continuously
- Higher memory consumption per connection
- Buffers stay in connection's cache even during idle periods
- May waste memory for short-lived or bursty connections
- Can starve other connections if many cache large batches

*Low Values (1, 2, 4) - Optimize for memory efficiency:*
+ Lower memory footprint per connection
+ Better buffer sharing across many connections
+ More predictable memory usage
+ Buffers return to shared pool faster
+ Better for servers with thousands of mostly-idle connections
- More frequent lock acquisitions on the ring (higher contention)
- Higher latency variance due to more frequent buffer fetching
- May limit streaming throughput

*Value of 1 - Minimal caching:*
Fetches one buffer per send operation. Used in latency-sensitive profiles
and when buffer_batching_mode is 'disable'. Also used when worker_threads > 0
to minimize per-connection memory overhead in high-connection scenarios.

**Default Value (16) - Optimized for TCP streams:**
Higher than UDP default (8) because TCP connections typically send many
packets over their lifetime, making the amortized cost of larger batches
worthwhile. Suitable for most TCP workloads.

**Tuning Recommendations:**
- High-throughput streaming (file transfer, video): Consider 32-64
- Web servers (many short-lived connections): Use 4-8
- High-connection servers (10K+ connections): Use 1-4 to save memory
- Latency-sensitive (trading, real-time): Use 1 for predictable timing
- Long-lived persistent connections: Higher values (16-32) work well
- Worker thread mode (worker_threads > 0): Keep at 1 (auto-configured)

**Related Parameters:**
- tcp_segments.socket_batch_size: Batching for TCP segment structures
- tcp_segments.ring_batch_size: Ring-level segment batching
- buffers.batching_mode: Master switch for buffer batching behavior
Default value is 16

performance.rings.tx.udp_buffer_batch
Maps to **TX_BUFS_BATCH_UDP** environment variable.

**What This Does:**
Controls how many TX buffers a UDP socket fetches at once from the ring's
shared buffer pool. Each socket maintains a local cache of pre-fetched
buffers to reduce contention on the shared pool.

**How Buffer Fetching Works:**
1. When sending a UDP packet, the socket first checks its local buffer cache
2. If the cache is empty, it acquires a lock on the ring's TX buffer pool
   and fetches 'udp_buffer_batch' buffers in a single operation
3. One buffer is used for the current packet, the rest remain cached locally
4. After sending, if the cache is depleted, buffers are proactively pre-fetched
5. When the socket is destroyed, any unused cached buffers return to the pool

**Value Tradeoffs:**

*High Values (8, 16, 32+) - Optimize for throughput and consistency:*
+ Fewer lock acquisitions on the ring's shared buffer pool
+ More consistent latency (refill operations happen less frequently)
+ Better burst performance (buffers ready for rapid consecutive sends)
+ Reduced CPU overhead from lock contention in multi-socket scenarios
- Higher memory consumption per socket (each socket holds N buffers locally)
- Buffers stay in socket's cache even when idle, reducing pool availability
- May starve other sockets if many sockets cache large batches
- Not ideal for applications with many sockets sending infrequently

*Low Values (1, 2) - Optimize for memory efficiency and fairness:*
+ Lower memory footprint per socket
+ Better buffer sharing across many sockets
+ More predictable memory usage
+ Buffers return to shared pool faster, improving availability
- More frequent lock acquisitions on the ring (higher contention)
- Higher latency variance due to more frequent buffer fetching
- May limit burst performance (must fetch buffers more often)
- Each send may incur locking overhead

*Value of 1 - Minimal caching:*
Fetches one buffer per send operation. Used in latency-sensitive profiles
where predictable timing matters more than throughput. Also used when
buffer_batching_mode is set to 'disable' (BUFFER_BATCHING_NONE).

**Default Value (8) - Balanced:**
Provides a good balance between reducing lock contention and memory usage.
Suitable for most UDP workloads with moderate sending rates.

**Tuning Recommendations:**
- High-throughput UDP streaming: Consider 16-32 for fewer pool accesses
- Many sockets, infrequent sends: Use 1-4 to reduce per-socket memory
- Latency-sensitive (low jitter): Use 1 to eliminate cache variability
- Burst traffic patterns: Higher values ensure buffers are ready
- Memory-constrained environments: Use lower values
- Single socket, high rate: Higher values reduce locking overhead
Default value is 8

performance.steering_rules.disable_flowtag
Maps to **XLIO_DISABLE_FLOW_TAG** environment variable.
Controls whether XLIO uses hardware flow tags for accelerated packet-to-socket mapping.

**What it does:**
- When false (default): XLIO uses hardware flow tags. Each socket gets a unique flow_tag_id embedded in its hardware steering rule. When packets arrive, the NIC includes this tag in the completion queue entry (CQE), enabling O(1) socket lookup without parsing packet headers.
- When true: Disables flow tag functionality. All incoming packets go through the slow path requiring full header parsing (Ethernet, IP, TCP/UDP) and hash table lookups to find the destination socket.

**Technical background:**
Flow tags are a hardware feature on NVIDIA/Mellanox network adapters. When a steering rule is created, XLIO assigns a flow_tag_id (derived from socket file descriptor). The hardware attaches this tag to matching packets. On receive, XLIO reads the tag directly from the CQE and performs an instant array lookup to find the socket.

**When to keep disabled (false, default):**
- All latency-sensitive applications - flow tags provide the fastest packet delivery path.
- High packet rate workloads where CPU cycles per packet matter.
- Applications using 2-step socket migration (requires flow tags).
- Most production deployments where performance is important.

**Performance implications when disabled (false):**
- BENEFIT: Fast path packet processing - direct socket lookup via hardware-provided tag.
- BENEFIT: Minimal CPU overhead per packet - no header parsing required for socket identification.
- BENEFIT: Lower and more consistent latency - fewer instructions in critical receive path.
- BENEFIT: Enables 2-step socket migration feature for zero-copy socket handoff.
- COST: Each flow with a unique flow tag consumes slightly more hardware resources.
- COST: Flow tags are disabled automatically for sockets using SO_REUSEADDR/SO_REUSEPORT (unless mc_force_flowtag is set for multicast).

**When to enable (true):**
- Hardware doesn't support flow tags (XLIO checks capability and disables automatically if unsupported).
- Debugging scenarios where you need to trace packet classification through software path.
- Specific compatibility requirements with certain hardware configurations.
- Testing or benchmarking the software classification path.

**Performance implications when enabled (true):**
- COST: All packets go through slow path - requires parsing Ethernet, IP, and TCP/UDP headers.
- COST: Hash table lookup required for each packet to find destination socket.
- COST: Higher CPU utilization per packet - more instructions in receive path.
- COST: Increased latency variability due to hash lookups and cache behavior.
- COST: 2-step socket migration feature becomes unavailable.
- BENEFIT: Slightly fewer hardware resources used (no flow tag action in steering rules).
- BENEFIT: Simpler packet flow for debugging purposes.

**Feature interactions:**
- Cannot be used together with network.multicast.mc_flowtag_acceleration (mc_force_flowtag).
  If both are set, mc_force_flowtag is automatically disabled with a warning.
- 2-step socket migration requires flow tags. Migration attempts will fail with ENOTSUP if disabled.
- When performance.steering_rules.tcp.3t_rules or performance.steering_rules.tcp.2t_rules are enabled,
  flow tags are automatically masked (FLOW_TAG_MASK) for those connections regardless of this setting.
- Sockets with SO_REUSEADDR or SO_REUSEPORT have flow tags disabled unless mc_force_flowtag overrides for multicast.

**Receive path comparison:**
- Flow tags enabled: CQE -> read flow_tag_id -> array lookup -> socket -> dispatch (fast path)
- Flow tags disabled: CQE -> parse ETH header -> parse IP header -> parse TCP/UDP header ->
  extract 5-tuple -> hash lookup in flow map -> socket -> dispatch (slow path)

**Recommendation:**
Keep this disabled (false) for production deployments unless you have a specific reason to disable flow tags. The performance benefit of flow tags is significant for latency-sensitive workloads.
Default value is false

performance.steering_rules.tcp.2t_rules
Maps to XLIO_TCP_2T_RULES environment variable.
Controls hardware flow steering rule granularity for TCP connections.

**What it does:**
- When false (default): Uses 5-tuple rules matching (protocol + local_ip + local_port + remote_ip + remote_port). Each TCP connection gets its own dedicated hardware steering rule.
- When true: Uses 2-tuple rules matching only (protocol + local_ip). All TCP connections sharing the same local IP address share a single hardware steering rule.

**When to enable (true):**
- Applications with many outgoing TCP connections (clients calling connect()) where ephemeral source ports would exhaust hardware flow table.
- Scenarios where hardware flow steering table size is a bottleneck.
- High connection-count workloads where hardware rule scalability matters more than per-packet efficiency.

**Performance implications when enabled:**
- BENEFIT: Dramatically reduces hardware flow steering table usage. One rule covers unlimited connections per local IP.
- BENEFIT: Overcomes adapter steering table limitations for high connection counts.
- COST: Flow tags are disabled, removing hardware-assisted fast-path packet identification.
- COST: Requires software demultiplexing to route packets to correct sockets, increasing CPU overhead.
- COST: Requires unique local IP address per XLIO ring to ensure correct packet steering.

**When to keep disabled (false, default):**
- Server workloads using listen()/accept() - consider 3t_rules instead.
- Low-to-moderate connection counts where hardware table is not exhausted.
- Latency-sensitive applications benefiting from flow tag optimization.
- Configurations where unique IP per thread is not feasible.

**Requirements when enabled:**
Each XLIO ring must bind sockets to a unique local IP address. In the default ring-per-thread configuration, each thread must use a thread-local IP address for its sockets.
Default value is false

performance.steering_rules.tcp.3t_rules
Maps to XLIO_TCP_3T_RULES environment variable.
Controls hardware flow steering rule granularity for incoming (accepted) TCP connections on server sockets.

**What it does:**
- When false (default): Uses 5-tuple rules matching (protocol + local_ip + local_port + remote_ip + remote_port). Each accepted TCP connection gets its own dedicated hardware steering rule with flow tag support.
- When true: Uses 3-tuple rules matching only (protocol + local_ip + local_port). All accepted connections on the same listening socket share a single hardware steering rule.

**Scope:** Only affects incoming TCP connections received via listen()/accept(). Outgoing connections established via connect() are not affected by this option.

**When to enable (true):**
- High-connection-count servers accepting thousands of concurrent TCP connections.
- Servers where hardware flow table exhaustion is a concern.
- Workloads prioritizing connection scalability over per-packet efficiency.
- Applications where accept()/close() latency matters more than packet processing latency.

**Performance implications when enabled:**
- BENEFIT: Dramatically reduces hardware flow steering table usage. One rule handles unlimited accepted connections per listening socket.
- BENEFIT: Faster accept() - no hardware rule creation overhead per new connection.
- BENEFIT: Faster connection teardown - no hardware rule deletion per closed connection.
- BENEFIT: Overcomes adapter flow table size limitations for servers with many clients.
- COST: Flow tags are disabled. Hardware cannot identify which socket a packet belongs to.
- COST: Software demultiplexing required. XLIO must perform a hash lookup to route each packet to the correct socket, increasing CPU cycles per packet.
- COST: Slightly higher packet processing latency due to software lookup overhead.

**When to keep disabled (false, default):**
- Low-to-moderate connection count servers where hardware table is not exhausted.
- Ultra-low-latency applications where per-packet CPU overhead is critical.
- Servers benefiting from hardware flow tag optimization.
- Workloads where connection rate is low but packet rate per connection is high.

**Comparison with 2t_rules:**
- 3t_rules: For servers (incoming connections via accept()). Shares rule per listening port.
- 2t_rules: For clients (outgoing connections via connect()). Shares rule per local IP address. Requires unique IP per ring.
Default value is false

performance.steering_rules.udp.3t_rules
Maps to XLIO_UDP_3T_RULES environment variable.
Controls hardware flow steering rule granularity for connected UDP sockets.

**What it does:**
- When true (default): Uses 3-tuple rules matching only (protocol + local_ip + local_port). Remote IP/port are wildcarded. All connected UDP sockets bound to the same local IP:port share a single hardware steering rule.
- When false: Uses 5-tuple rules matching (protocol + local_ip + local_port + remote_ip + remote_port). Each connected UDP socket gets its own dedicated hardware steering rule with flow tag support.

**Scope:** This parameter is relevant for connected UDP sockets (sockets on which connect() was called). For non-connected UDP sockets, 3-tuple rules are always used regardless of this setting.

**When to keep enabled (true, default):**
- Applications with many UDP "connections" to different remote peers (e.g., game servers, video streaming servers).
- Scenarios where hardware flow table exhaustion is a concern.
- Workloads where connection setup/teardown rate matters more than per-packet efficiency.
- Applications that do not require ultra-low-latency packet processing.

**Performance implications when enabled (true):**
- BENEFIT: Dramatically reduces hardware flow steering table usage. One rule can handle unlimited connected UDP sockets per local IP:port.
- BENEFIT: Faster connect() - no hardware rule creation overhead per new connection.
- BENEFIT: Faster socket teardown - no hardware rule deletion per closed connection.
- BENEFIT: Overcomes adapter flow table size limitations for applications with many UDP peers.
- COST: Flow tags are disabled. Hardware cannot identify which socket a packet belongs to.
- COST: Software demultiplexing required. XLIO must perform a hash table lookup to route each incoming packet to the correct socket, increasing CPU cycles per packet.
- COST: Slightly higher packet processing latency due to software lookup overhead.

**When to disable (false):**
- Low-to-moderate connection count applications where hardware table is not exhausted.
- Ultra-low-latency applications (e.g., trading systems, real-time control) where per-packet CPU overhead is critical.
- Applications with few UDP "connections" but very high packet rates per connection.
- Workloads benefiting from hardware flow tag optimization for direct socket delivery.

**Performance implications when disabled (false):**
- BENEFIT: Flow tags work - hardware identifies the exact socket directly (fast path).
- BENEFIT: Lower per-packet latency and fewer CPU cycles per packet.
- BENEFIT: No software demultiplexing overhead - packets are delivered directly to the target socket.
- COST: Each connected UDP socket consumes a hardware steering rule entry.
- COST: Risk of exhausting hardware flow table with many concurrent connections.
- COST: Slower connect/disconnect as each requires hardware rule creation/deletion operations.

**Comparison with TCP steering rules:**
- UDP 3t_rules: Affects connected UDP sockets. Default is true (3-tuple) to conserve resources.
- TCP 3t_rules: Affects accepted TCP connections on server sockets. Default is false (5-tuple) for optimal latency.
- TCP 2t_rules: For outgoing TCP connections. Requires unique IP per ring.
Default value is true

performance.steering_rules.udp.only_mc_l2_rules
Maps to XLIO_ETH_MC_L2_ONLY_RULES environment variable.
Controls hardware flow steering rule granularity for UDP multicast traffic.

**What it does:**
- When false (default): Creates specific hardware rules per (multicast_ip, port) pair.
  Hardware matches on: Destination MAC + Destination IP + Destination Port + Protocol (UDP).
  Each socket joining a multicast group on a different port gets its own dedicated hardware rule.
- When true: Creates broad L2-only hardware rules per multicast MAC address.
  Hardware matches on: Destination MAC + Protocol (UDP) only.
  Multiple sockets on different ports sharing the same multicast IP share ONE hardware rule.
  XLIO performs software filtering by port after packets arrive.

**Technical detail:**
Multicast MAC addresses are derived from multicast IP addresses (e.g., 239.1.1.1 maps to
01:00:5e:01:01:01). When L2-only rules are enabled, all traffic to that derived MAC arrives
at XLIO, regardless of port. Software then demultiplexes packets to the correct socket.

**When to keep disabled (false, default):**
- Applications with few multicast group/port combinations.
- Ultra-low-latency multicast receivers where every microsecond counts.
- High packet rate scenarios where CPU cycles per packet matter.
- Environments with adequate hardware flow steering table capacity.

**Performance implications when disabled (false):**
- BENEFIT: Lower per-packet latency - hardware delivers packets directly to correct flow.
- BENEFIT: Lower CPU overhead - no software port filtering required.
- BENEFIT: Optimal for small-scale multicast (few groups, few ports per group).
- COST: Each (multicast_ip, port) combination consumes one hardware steering rule entry.
- COST: Risk of exhausting hardware flow steering table with many multicast subscriptions.
- COST: More hardware rule create/delete operations when joining/leaving groups.

**When to enable (true):**
- Applications joining many ports on the same multicast group (e.g., 239.1.1.1:5000, 239.1.1.1:5001, ..., 239.1.1.1:5100).
- Market data feed handlers receiving many instruments on different ports of the same multicast address.
- Environments with limited hardware flow steering resources.
- Applications hitting flow steering table capacity limits.
- Multicast-heavy applications where conserving steering rules is critical.

**Performance implications when enabled (true):**
- BENEFIT: Dramatically reduces hardware flow steering resource usage.
  One rule handles unlimited ports per multicast IP.
- BENEFIT: Prevents flow table exhaustion for multicast-heavy workloads.
- BENEFIT: Faster multicast group join/leave - fewer hardware rule operations.
- COST: Higher CPU overhead per packet - software must filter by destination port.
- COST: Increased latency due to additional software lookup.
- COST: All multicast loopback traffic is handled by XLIO instead of OS
  (relevant when IP_MULTICAST_LOOP is enabled).
- COST: Packets for ports not subscribed (but same multicast IP) still arrive at XLIO
  and must be filtered out in software.

**Hardware resource context:**
Network adapters have finite flow steering table capacity (varies by adapter model and
firmware). Each specific rule (5-tuple or 4-tuple) consumes one entry. Applications
subscribing to hundreds or thousands of multicast group/port combinations can exhaust
this capacity, causing steering rule creation failures.

**Example scenario:**
- Market data application subscribes to 500 instruments, each on a unique port of
  multicast group 239.1.1.1.
- With only_mc_l2_rules=false: 500 hardware steering rules required.
- With only_mc_l2_rules=true: 1 hardware steering rule (matching MAC 01:00:5e:01:01:01).
  Software filters packets to correct socket by port.

**Latency vs resource tradeoff:**
- false = Lower latency, higher resource usage. Best for latency-critical, low-scale multicast.
- true = Higher latency, lower resource usage. Best for resource-constrained, high-scale multicast.
Default value is false

performance.threading.cpu_affinity
Maps to **XLIO_INTERNAL_THREAD_AFFINITY** environment variable.
Control which CPU core(s) the XLIO internal thread is serviced on.

**What is the Internal Thread?**

XLIO spawns an internal event handler thread that runs independently of your application.
This thread is responsible for:

1. **Timer Processing**: Handles all registered timers including:
   - TCP retransmission timers (for reliable delivery)
   - TCP keepalive timers (connection health monitoring)
   - ARP/neighbor resolution timers (address translation)
   - IP fragment reassembly timers
   - Socket cleanup and garbage collection

2. **Async Hardware Events**: Monitors and processes:
   - IB Verbs async events (device errors, port state changes)
   - RDMA CM events (connection management)
   - Agent progress for daemon communication

3. **Event Loop Behavior**:
   - Wakes up periodically based on performance.threading.internal_handler.timer_msec (default: 10ms)
   - Uses epoll_wait with timeout to sleep between timer expirations
   - Processes registered event handlers when async events arrive

**Input Format:**

The cpu set should be provided as *EITHER* a hexadecimal value representing a bitmask,
*OR* as a comma-delimited list of values (ranges are supported).
Both formats are identical to what the taskset command supports.

Bitmask Examples:
- 0x00000001 - Run on processor 0 only
- 0x00000007 - Run on processors 0, 1, and 2
- 0x0000000F - Run on processors 0-3
- 0xFFFFFFFF - Run on processors 0-31

Comma-Delimited Examples:
- 0          - Run on processor 0 only
- 0,4,8      - Run on processors 0, 4, and 8
- 0,1,7-10   - Run on processors 0, 1, 7, 8, 9, and 10
- 0-7        - Run on processors 0 through 7

**Value Tradeoffs:**

*Value = "-1" (default) - No Affinity (OS Scheduler Decides):*

Benefits:
- Maximum flexibility - OS scheduler optimizes for overall system performance
- Automatic load balancing across available cores
- Works well for general-purpose deployments
- No risk of pinning to an overloaded or offline core

Drawbacks:
- Thread may migrate between cores, causing cache invalidation
- Less deterministic behavior - latency may vary as thread moves between CPUs
- Cross-NUMA migrations can significantly increase timer processing latency
- Harder to reason about worst-case performance

Best for: Development, testing, or when you don't know the system topology.

*Single Core (e.g., "0" or "0x01") - Dedicated Isolated Core:*

Benefits:
- Most predictable, lowest-jitter timer processing
- No cache pollution from thread migration
- Easier to isolate using isolcpus or cset for guaranteed resources
- Consistent NUMA locality - all internal thread data stays in one memory domain
- Recommended for latency-sensitive applications

Drawbacks:
- Wastes potential parallelism if internal thread has burst workloads
- If pinned to a core also used by application threads, may cause contention
- Must ensure the selected core isn't being used by other latency-critical components

Best for: Latency-sensitive applications (trading, real-time control).

*Multiple Cores (e.g., "0-3" or "0x0F") - Bounded Migration:*

Benefits:
- Allows some OS scheduling flexibility within a bounded set
- Can specify cores within the same NUMA node for locality
- Provides redundancy if one core becomes overloaded
- Balances determinism and flexibility

Drawbacks:
- Thread may still migrate within the allowed set, causing some cache misses
- More complex to configure correctly
- Must ensure all specified cores are in the same NUMA node for best results

Best for: Multi-socket systems where you want NUMA-aware flexibility.

**NUMA Considerations:**

For multi-socket systems, NUMA topology significantly affects performance:

- **Same NUMA Node as Application**: Internal thread and application share L3 cache.
  Timer-triggered operations (like TCP retransmits) have lower latency accessing
  socket data structures.

- **Different NUMA Node**: Cross-NUMA memory access adds ~50-100ns latency per access.
  TCP timer processing may take longer, slightly delaying retransmissions.
  However, this isolates the internal thread's cache footprint from application threads.

Recommendation: Pin to a core in the same NUMA node as your primary application threads,
but ideally on a different physical core to avoid contention.

Example for a 2-socket system with cores 0-15 on NUMA 0 and 16-31 on NUMA 1:
- If application runs on cores 0-7, pin internal thread to core 15 (same NUMA, isolated)
- Configuration: cpu_affinity = "15" or cpu_affinity = "0x8000"

**CPU Contention Considerations:**

- Internal thread wakes up every timer_msec (default 10ms) to process timers
- Each wakeup is brief (~microseconds) unless many timers expire simultaneously
- Sharing a core with application threads causes context switches at timer intervals
- For latency-critical apps, dedicate a core to internal thread using isolcpus

**Interaction with Other Parameters:**

- **performance.threading.cpuset**: If set, thread is first moved to that cpuset,
  then cpu_affinity is applied within that cpuset. Ensure affinity cores exist in cpuset.

- **performance.threading.internal_handler.timer_msec**: Controls how often internal thread
  wakes up. Lower values = more frequent wakeups = more important to have low-latency
  access to the pinned core.

- **performance.threading.internal_handler.behavior**: When set to "delegate", TCP timers
  are handled by application threads instead of the internal thread, reducing the
  internal thread's workload and the importance of this affinity setting.

- **performance.threading.worker_threads**: When > 0, worker threads handle socket
  operations. Internal thread still handles async events and non-delegated timers.

**Recommended Configurations:**

1. **Latency-Critical (HFT, Real-Time Control):**
   - Use isolcpus to reserve a dedicated core (e.g., core 1)
   - cpu_affinity = "1"
   - internal_handler.timer_msec = 10 or lower
   - Ensure application threads are pinned to different cores

2. **High-Throughput Server (Many Connections):**
   - cpu_affinity = "-1" (let OS balance) or pin to a specific NUMA node
   - internal_handler.timer_msec = 100 (reduce wakeup frequency)
   - Consider internal_handler.behavior = "delegate" if connections are thread-local

3. **Multi-Socket NUMA System:**
   - Identify which NUMA node your application primarily uses
   - Pin internal thread to a core on that NUMA node: cpu_affinity = "<core_on_same_numa>"
   - Verify with: numactl --hardware and lstopo

4. **Container/VM Environment:**
   - cpu_affinity = "-1" (container runtime may restrict available cores)
   - Or use values matching container's cpuset allocation

**Monitoring:**

- Use `top -H -p <pid>` to see internal thread CPU usage
- Thread is named with PID visible in /proc/<pid>/task/
- High CPU on internal thread suggests many timers expiring - consider tuning timer_msec

**Profiles That Set This Value:**
- LATENCY profile: "0" (pinned to core 0 for consistency)
- ULTRA_LATENCY profile: "0" (pinned to core 0)
- NGINX_ULTRA_LATENCY profile: "0x01" (pinned to core 0)
- Default profile: "-1" (no affinity, OS decides)

NOTE: Only hexadecimal values are supported for this parameter in XLIO_INLINE_CONFIG.
Default value is -1

performance.threading.cpuset
Maps to **XLIO_INTERNAL_THREAD_CPUSET** environment variable.
Select a cpuset for XLIO internal thread.

**What is a cpuset?**

cpuset is a Linux kernel mechanism (part of cgroups v1) that constrains which CPUs and memory nodes
a process or thread can use. Unlike cpu_affinity which is a per-thread hint that the scheduler
may override, cpuset provides hard isolation - the kernel will NEVER schedule the thread on CPUs
outside its cpuset.

cpusets are represented as filesystem directories under /dev/cpuset (or /sys/fs/cgroup/cpuset on
modern systems). Each cpuset directory contains:
- cpuset.cpus: The CPUs this cpuset can use (e.g., "0-3" or "0,2,4")
- cpuset.mems: The NUMA memory nodes this cpuset can allocate from
- tasks: PIDs/TIDs of processes/threads in this cpuset

When XLIO's internal thread starts, it writes its TID to <cpuset_path>/tasks to join the cpuset.

**Value Options:**

*Value = "" (empty string - DEFAULT):*

The internal thread inherits the cpuset of the process that loaded XLIO.
This is usually the root cpuset containing all CPUs, but may be restricted if:
- The application was launched with cset, cpuset, or systemd-run --scope
- Running in a container with CPU limits (Docker, Kubernetes)
- The parent shell has cpuset restrictions

Benefits:
- No configuration required
- Thread follows the application's resource constraints automatically
- Works correctly in containerized environments

Drawbacks:
- No explicit control over CPU isolation
- Thread competes with all application threads for the inherited CPUs

Best for: Most deployments, especially containers.

*Value = "/path/to/cpuset" (explicit cpuset path):*

The internal thread explicitly joins the specified cpuset, gaining its CPU and memory constraints.

Common paths:
- /dev/cpuset/my_isolated_set (cgroups v1 style)
- /sys/fs/cgroup/cpuset/my_isolated_set (cgroups v1 on modern systems)

Benefits:
- Hard CPU isolation - kernel guarantees thread runs only on specified CPUs
- Can isolate internal thread from application threads for predictable latency
- NUMA memory binding - thread allocates memory from specified NUMA nodes only
- Integrates with system-level resource management (cgroups, Kubernetes)
- Survives across thread priority changes and scheduler reconfigurations

Drawbacks:
- Requires cpuset to exist before XLIO initialization (created by admin/orchestrator)
- More complex deployment - cpuset must be pre-configured
- If cpuset's CPUs become offline, thread may be unable to run
- Wrong configuration can severely limit performance

Best for: Production systems with strict isolation requirements.

**Interaction with cpu_affinity:**

When BOTH cpuset and cpu_affinity are set, they are applied in order:

1. FIRST: Thread joins the cpuset (moved to new CPU/memory constraints)
2. SECOND: cpu_affinity is applied WITHIN that cpuset

CRITICAL: cpu_affinity must specify CPUs that exist in the cpuset!
If cpu_affinity includes CPUs not in the cpuset, pthread_setaffinity_np() will fail
and XLIO logs: "Internal thread affinity failed. Did you try to set affinity outside of cpuset?"

Example:
```
cpuset: /dev/cpuset/xlio_set       # Contains CPUs 4,5,6,7
cpu_affinity: 4                     # OK - CPU 4 is in the cpuset
cpu_affinity: 0                     # FAIL - CPU 0 is not in the cpuset
cpu_affinity: 4,5                   # OK - both CPUs are in the cpuset
```

Recommendation: If using cpuset, set cpu_affinity to "-1" (no affinity) and let the cpuset
control CPU placement, OR ensure cpu_affinity is a subset of the cpuset's CPUs.

**cpuset vs cpu_affinity - When to Use Each:**

| Feature                    | cpuset                         | cpu_affinity                    |
|----------------------------|--------------------------------|---------------------------------|
| Enforcement                | Hard (kernel guarantee)        | Soft (scheduler hint)           |
| Persistence                | Survives reconfigurations      | Can be overridden               |
| NUMA memory binding        | Yes (cpuset.mems)              | No                              |
| Setup complexity           | Requires pre-created cpuset    | Just set the parameter          |
| Container integration      | Native (cgroups)               | Per-thread only                 |
| Recommended for            | System-level isolation         | Thread-level tuning             |

For most users: Use cpu_affinity alone (simpler, effective for latency tuning).
For production isolation: Use cpuset for hard isolation, optionally with cpu_affinity within it.

**Performance Considerations:**

*Benefits of Using cpuset:*
- Guaranteed CPU isolation prevents unexpected thread migration
- NUMA-aware memory allocation (set cpuset.mems to match cpuset.cpus NUMA nodes)
- Predictable worst-case latency (no cross-NUMA surprises)
- Integrates with Kubernetes CPU manager (static policy) for pod-level isolation

*Costs of Using cpuset:*
- Joining a cpuset requires filesystem write (one-time ~1ms at startup)
- If cpuset is too restrictive, thread may be starved of CPU time
- Configuration complexity and potential for misconfiguration

**Creating a cpuset (for administrators):**

```bash
# Create cpuset for XLIO internal thread on CPUs 4-7, NUMA node 0
mkdir -p /dev/cpuset/xlio_internal
echo 4-7 > /dev/cpuset/xlio_internal/cpuset.cpus
echo 0 > /dev/cpuset/xlio_internal/cpuset.mems
echo 1 > /dev/cpuset/xlio_internal/cpu_exclusive  # Optional: exclusive access
```

Then set: cpuset = "/dev/cpuset/xlio_internal"

**Example Configurations:**

1. **Default (no cpuset, use cpu_affinity only):**
   - cpuset = "" (default)
   - cpu_affinity = "4" (or desired core)
   - Simple setup, effective for most use cases

2. **Container environment (inherit container's cpuset):**
   - cpuset = "" (default - inherits container's cgroup)
   - cpu_affinity = "-1" (let container scheduler decide)
   - Works with Docker --cpuset-cpus and Kubernetes CPU limits

3. **Strict isolation with NUMA binding:**
   - cpuset = "/dev/cpuset/xlio_isolated" (admin pre-creates with specific CPUs/NUMA)
   - cpu_affinity = "-1" (let cpuset handle placement)
   - For latency-critical deployments with system-level resource management

4. **cpuset + specific affinity within cpuset:**
   - cpuset = "/dev/cpuset/xlio_set" (contains CPUs 4-7)
   - cpu_affinity = "4" (pin to one specific CPU within the cpuset)
   - Maximum control - cpuset provides isolation, affinity provides precision

**Kubernetes Integration:**

When running in Kubernetes with CPU manager (static policy), pods with guaranteed QoS class
get their own cpuset automatically. XLIO will inherit this cpuset if cpuset = "".

For exclusive CPU access in Kubernetes:
```yaml
resources:
  requests:
    cpu: "2"       # Request exactly 2 CPUs
    memory: "4Gi"
  limits:
    cpu: "2"       # Limit to exactly 2 CPUs (creates exclusive cpuset)
    memory: "4Gi"
```

**Troubleshooting:**

- If XLIO fails to start with cpuset errors, verify the path exists and is readable/writable
- Use `cat /dev/cpuset/your_set/cpuset.cpus` to check which CPUs are in the cpuset
- Use `cat /dev/cpuset/your_set/tasks` to verify the thread joined successfully
- If cpu_affinity fails with cpuset, ensure affinity cores are in cpuset.cpus
Default value is ""

performance.threading.internal_handler.behavior
Maps to **XLIO_TCP_CTL_THREAD** environment variable.
Controls how TCP control flows (timers, state machine operations) are executed.

**What Are TCP Control Flows?**
TCP control flows include timer-driven operations critical to TCP reliability:
- Retransmission timers (RTO): Retransmit unacknowledged packets
- Delayed ACK timers: Send ACKs for received data
- Keepalive timers: Detect dead connections
- Persist timers: Probe zero-window receivers
- Connection state cleanup: TIME_WAIT, FIN_WAIT, etc.
- Congestion control updates

**The Two Modes:**

**"disable" or 0 (DEFAULT) - Internal Thread Mode:**

Architecture:
- XLIO's internal thread (event_handler_manager) processes ALL TCP timers
- Global timer collection (g_tcp_timers_collection) tracks all TCP sockets
- Application threads use the global event handler for timer registration
- TCP sockets use REAL locks (spinlocks or mutexes per mutex_over_spinlock setting)

How it works:
1. Internal thread wakes up every performance.threading.internal_handler.timer_msec (default: 10ms)
2. Processes all registered TCP timers in the global collection
3. Acquires TCP connection locks (trylock pattern) to process each socket
4. If lock contention occurs, timer processing for that socket is deferred

Benefits:
+ Thread-safe: Sockets can be used from ANY thread at any time
+ Application threads can block indefinitely (poll/select/sleep) without affecting TCP
+ Timers fire reliably regardless of application activity
+ Supports traditional multi-threaded socket patterns (accept in one thread, read/write in another)
+ No restrictions on socket lifecycle or thread affinity

Drawbacks:
- Lock contention: Internal thread competes with application threads for TCP locks
- Latency variance: Timer processing may be delayed if socket lock is held
- Cross-thread coordination overhead between internal thread and application
- Internal thread CPU cost (though typically minimal)

**"delegate" or 1 - Application Thread Mode (Lock-Free):**

Architecture:
- NO internal thread processing of TCP timers
- Thread-local timer collection (g_thread_local_tcp_timers) per application thread
- Thread-local event handler (g_event_handler_manager_local) per thread
- TCP sockets use DUMMY locks (NO real locking - lock-free operation)

How it works:
1. Each application thread has its own timer infrastructure (thread_local)
2. Timers are processed when application calls socket operations:
   - recv()/send() trigger do_tasks() to check timers
   - epoll_wait() periodically wakes up to run timers
   - Explicit polling in rx_wait_helper() runs timers
3. No cross-thread communication for timer handling

Benefits:
+ ZERO lock contention: No locks on TCP connection operations
+ Lower latency: No lock acquisition/release overhead
+ Better CPU cache locality: All socket data stays on one CPU
+ Eliminates internal thread CPU usage
+ Ideal for busy-polling applications with dedicated threads

Drawbacks:
- STRICT thread affinity: Socket MUST be owned by ONE thread from creation to destruction
- Socket migration between threads causes undefined behavior (corruption, crashes)
- Timers only fire when application is active on the socket
- INCOMPATIBLE with blocking poll()/select() - see below
- Application must provide consistent execution context

**CRITICAL: Blocking poll()/select() Incompatibility**

When using "delegate" mode:
- Blocking poll() and select() DO NOT process TCP timers internally
- While blocked in poll()/select(), NO timers fire for delegate-mode sockets
- This can cause:
  * TCP retransmissions to be delayed indefinitely
  * Keepalive probes to never be sent
  * Connections to hang or timeout incorrectly
  * Delayed ACKs to never be sent, stalling senders

Why epoll is OK:
- XLIO's socket-level os_epoll_wait_with_tcp_timers() method periodically
  wakes up (every tcp_timer_resolution_msec) and calls do_tasks()
- This ensures timer processing during blocking waits
- Only applies to per-socket internal epoll, not global epoll multiplexing

**Performance Impact Summary:**

| Aspect                  | disable (0)            | delegate (1)          |
|-------------------------|------------------------|-----------------------|
| Lock overhead           | Real locks acquired    | Zero (dummy locks)    |
| Timer reliability       | Always fires on time   | Depends on app activity|
| Thread flexibility      | Any thread can use     | Single owner thread   |
| Blocking poll/select    | ✓ Fully supported      | ✗ DO NOT USE          |
| Blocking epoll          | ✓ Supported            | ✓ Supported           |
| Non-blocking operations | ✓ Supported            | ✓ Best performance    |
| CPU overhead            | Internal thread cost   | Zero internal thread  |
| Latency (uncontended)   | Lock overhead (~5-50ns)| Near-zero             |
| Latency (contended)     | Variable (lock wait)   | N/A (no contention)   |

**Forced Configuration Changes:**
When "delegate" is enabled, XLIO automatically forces:
- Ring allocation logic: RING_LOGIC_PER_THREAD (for both TX and RX)
- Progress engine interval: DISABLED (MCE_CQ_DRAIN_INTERVAL_DISABLED)

These changes ensure per-thread isolation required for lock-free operation.

**When to Use "disable" (0) - DEFAULT:**
- Applications using blocking poll() or select()
- Multi-threaded socket patterns (accept/read/write on different threads)
- Unknown or legacy application threading models
- Applications that may migrate sockets between threads
- Mixed blocking and non-blocking socket usage
- General-purpose, conservative configuration

**When to Use "delegate" (1):**
- Busy-polling applications (trading systems, packet processors)
- Applications using epoll exclusively (not poll/select)
- Single-threaded event loops (one thread per connection)
- Applications with strict thread-to-socket affinity
- Latency-critical workloads where lock overhead matters
- Applications already using thread-per-connection models
- When maximum lock-free performance is required

**Example Configurations:**

1. High-Frequency Trading (latency-critical, single-threaded):
   behavior = "delegate"
   + epoll-based event loop
   + busy-polling (rx_poll = -1)
   + CPU pinning per thread

2. Web Server (nginx-like, blocking accept):
   behavior = "disable" (default)
   + May use blocking operations in various threads
   + Connections may be handed between threads

3. Redis-like Server (single-threaded event loop):
   behavior = "delegate"
   + Single thread handles all connections via epoll
   + No blocking poll/select usage

4. Multi-threaded Server (thread pool):
   behavior = "disable" (default)
   + Connection may be processed by different worker threads
   + Thread safety required

**Monitoring and Debugging:**
With delegate mode, if you observe:
- TCP retransmissions delayed → Application not calling socket ops frequently enough
- Connections hanging → Likely using blocking poll/select
- Crashes or corruption → Socket being used from multiple threads

**WARNING:** Violating the single-thread ownership rule in delegate mode causes
undefined behavior including data corruption, crashes, and memory leaks.
There is NO runtime check - it is the application's responsibility to ensure
each socket is only accessed from its owning thread.
Default value is 0

performance.threading.internal_handler.timer_msec
Maps to **XLIO_TIMER_RESOLUTION_MSEC** environment variable.
Controls the minimum wakeup interval (in milliseconds) for XLIO's internal thread.

**What is the Internal Thread?**
XLIO spawns a dedicated internal thread that handles:
- Timer events (TCP timers, neighbor/ARP discovery, netlink route updates)
- RDMA Connection Manager (CM) events for connection establishment
- IBVerbs async events (hardware errors, port state changes, device events)
- Netlink monitoring for routing table and neighbor cache updates

The internal thread uses epoll_wait() with a timeout. This parameter sets the
MINIMUM sleep duration—the thread wakes up at least every timer_msec milliseconds,
even if no events are pending.

**Relationship with TCP Timer Resolution:**
This parameter is a LOWER BOUND for network.protocols.tcp.timer_msec.
TCP timer resolution cannot be smaller than this value. The TCP timer collection
uses buckets = (tcp_timer_msec / timer_msec), so this ratio affects how TCP
connections are grouped for timer processing.

**Low Values (1-10 ms) - Latency-Optimized:**
Benefits:
- Faster reaction to timer expirations (retransmissions, keepalives)
- More accurate TCP RTO calculations
- Lower worst-case latency for handling hardware/connection events
- Better responsiveness to network topology changes (link failures)

Drawbacks:
- Higher CPU utilization due to frequent epoll_wait() syscalls
- More context switches between internal thread and application
- May cause unnecessary wakeups when no work is pending
- Can add scheduling noise affecting tail latencies

**High Values (32-100+ ms) - Throughput/Efficiency-Optimized:**
Benefits:
- Lower CPU utilization from reduced syscall frequency
- Less scheduler noise, better for CPU-bound applications
- Allows longer sleep periods when idle, improving power efficiency
- Better suited for high-throughput, batch-oriented workloads

Drawbacks:
- Delayed reaction to TCP retransmission timeouts under packet loss
- Coarser timer granularity affects RTO accuracy
- Slower detection of connection/hardware failures
- May increase tail latency for timer-dependent operations
- Keepalive and persist timer probes sent less precisely

**TCP Operations Affected by Timer Granularity:**
- Retransmission timeout (RTO) - triggers packet retransmission
- Keepalive probes - detects dead connections
- Persist timer - zero-window probes when receiver advertises 0 window
- TIME_WAIT cleanup - releases connection resources after 2*MSL
- Connection timeouts - SYN_RCVD, FIN_WAIT_2, LAST_ACK state cleanup
- Out-of-sequence segment timeout - drops stale OOO queued data

**Recommended Configurations:**

For latency-sensitive applications (trading, real-time):
- Use 1-10 ms (default 10 ms is good for most cases)
- Pair with low tcp.timer_msec (e.g., 10-50 ms)

For throughput-focused applications (web servers, proxies like Nginx):
- Use 32-64 ms to reduce CPU overhead
- XLIO uses 32 ms for NGINX profile internally

For idle/low-traffic scenarios:
- Higher values (50-100 ms) reduce unnecessary wakeups
- Consider pairing with higher tcp.timer_msec (100-256 ms)

**Note:** This parameter affects ALL timer-based operations in XLIO,
not just TCP. Changes impact neighbor discovery, route monitoring,
and hardware event handling as well.
Default value is 10

performance.threading.mutex_over_spinlock
Maps to **XLIO_MULTILOCK** environment variable.
Controls the locking primitive used for XLIO's internal synchronization in performance-critical paths.

**What Locks Are Affected:**
This parameter controls the lock type for:
- Ring RX locks (m_lock_ring_rx): Protect packet reception and CQ polling
- Ring TX locks (m_lock_ring_tx): Protect packet transmission and buffer management
- TCP connection locks (m_tcp_con_lock): Protect TCP state machine and send/receive paths
- Socket receive locks (m_lock_rcv): Protect socket receive buffer access
- Application socket locks (m_app_lock): Protect general socket operations
- Network device locks: Protect device-level operations

**How Locks Are Used in XLIO:**
XLIO uses two locking patterns depending on the code path:

1. **trylock pattern (hot path - non-blocking):**
   ```
   if (!m_lock_ring_rx.trylock()) {
       process_packets();
       m_lock_ring_rx.unlock();
   }
   // If trylock fails, operation is skipped (returns 0)
   ```
   This is critical: in the packet processing hot path, XLIO uses trylock() and
   skips the operation if the lock is held. This avoids blocking in latency-sensitive code.

2. **lock pattern (blocking path):**
   Used when the operation must complete, such as buffer allocation or socket setup.

**Value = false (Spinlock - DEFAULT):**

Spinlocks use busy-waiting: the thread continuously checks the lock in a tight loop
until it becomes available. No kernel involvement, pure userspace atomic operations.

*Benefits:*
- Lowest uncontended latency: ~5-20 nanoseconds for lock/unlock
- Fastest trylock: Single atomic test-and-set instruction
- No context switches: Thread keeps running on the CPU
- No kernel involvement: Pure userspace operation
- Ideal for short critical sections: XLIO's ring operations are typically <1μs
- Best for dedicated cores: When threads have exclusive CPU access

*Drawbacks:*
- CPU waste on contention: Spinning thread consumes 100% CPU while waiting
- Priority inversion: Low-priority spinning thread can prevent high-priority work
- Poor for long waits: If lock holder is preempted, spinner wastes CPU cycles
- Cache line bouncing: Multiple spinners cause cache coherency traffic
- Bad for oversubscribed systems: More threads than cores = wasted spinning

*Performance characteristics:*
- Uncontended lock: ~5-20ns
- Contended lock: Spins until acquired (variable, depends on holder)
- trylock (uncontended): ~5-10ns
- trylock (contended): ~5-10ns (returns immediately with failure)

**Value = true (Mutex):**

Mutexes use blocking: if the lock is unavailable, the thread is put to sleep
by the kernel and woken up when the lock becomes available.

*Benefits:*
- No CPU waste on contention: Sleeping thread consumes no CPU
- Fair scheduling: OS scheduler manages thread priorities properly
- Better for oversubscribed systems: Many threads sharing few cores
- Power efficient: Sleeping threads reduce power consumption
- Better for long critical sections: Appropriate when lock is held longer
- Plays well with other processes: Doesn't monopolize CPU

*Drawbacks:*
- Higher uncontended latency: ~50-200ns due to kernel involvement
- Context switch on contention: ~1-10μs when thread must sleep/wake
- Higher latency variance: Scheduling introduces jitter (microseconds)
- syscall overhead: Every lock/unlock involves kernel
- Not ideal for short critical sections: Overhead dominates for tiny operations

*Performance characteristics:*
- Uncontended lock: ~50-200ns
- Contended lock: ~1-10μs (context switch + wake time)
- trylock (uncontended): ~50-100ns
- trylock (contended): ~50-100ns (returns immediately with failure)

**Performance Impact Summary:**
| Scenario                        | Spinlock (false)  | Mutex (true)      |
|---------------------------------|-------------------|-------------------|
| Uncontended lock latency        | ~5-20ns           | ~50-200ns         |
| Contended lock (light)          | ~100ns-1μs        | ~1-10μs           |
| Contended lock (heavy)          | Wastes CPU        | Efficient sleep   |
| CPU usage while waiting         | 100%              | 0%                |
| Latency jitter                  | Lower             | Higher            |
| Best thread:core ratio          | 1:1 or fewer      | Many:1            |

**When to Keep Default (false - Spinlock):**
- Latency-sensitive applications (HFT, real-time systems)
- Dedicated CPU cores per thread (no oversubscription)
- Low lock contention (per-thread rings, single-threaded sockets)
- Busy-polling applications (already consuming CPU continuously)
- Short critical sections (typical XLIO ring operations)
- Maximum throughput is the primary goal
- Predictable latency is critical (lower jitter)

**When to Use true (Mutex):**
- Highly oversubscribed systems (many more threads than CPU cores)
- Shared/multi-tenant environments (cloud VMs, containers)
- Power-sensitive deployments (reduce idle CPU power)
- Applications with high lock contention
- Legacy applications with unpredictable threading patterns
- Mixed workloads where non-XLIO threads need CPU time
- When lock holders may be preempted by the OS
- Systems running multiple latency-tolerant applications

**Interaction with Ring Allocation:**
The impact of this setting depends heavily on ring allocation:

- With per_thread rings (default): Each thread has its own ring, minimal contention.
  Spinlocks are ideal - uncontended performance matters most.

- With per_interface rings: Multiple threads share rings, higher contention.
  Mutex may help if threads frequently compete for the same ring.

- With ring limits (XLIO_RING_LIMIT_PER_INTERFACE): Forced ring sharing.
  Consider mutex if contention causes excessive CPU waste from spinning.

**Interaction with Worker Threads:**
When worker_threads > 0, XLIO worker threads are constantly polling.
Spinlocks ensure minimal overhead for their frequent lock operations.
Mutex would add unnecessary latency to the worker polling loops.

**Tuning Strategy:**
1. Start with default (false - spinlock) for best latency
2. Monitor CPU utilization during lock contention scenarios
3. If CPU usage is high but throughput is low → threads are spinning
4. Switch to mutex (true) and measure impact
5. If latency increases unacceptably, investigate ring allocation instead

**Profiling Lock Contention:**
Use Linux perf to identify lock contention:
```bash
perf lock record -a -p <pid> -- sleep 30
perf lock report
```
Look for high wait times on XLIO locks (ring locks, tcp_con_lock).

**Example Configurations:**

1. Ultra-low-latency trading (dedicated cores):
   mutex_over_spinlock = false (default)
   + per_thread rings + CPU pinning

2. Web server on shared cloud VM:
   mutex_over_spinlock = true
   + limited rings + moderate worker count

3. High-throughput proxy (dedicated server):
   mutex_over_spinlock = false (default)
   + per_thread rings + NUMA-aware pinning

4. Containerized microservice (CPU limits):
   mutex_over_spinlock = true
   + per_interface rings + minimal memory footprint
Default value is false

performance.threading.worker_threads
Maps to **XLIO_WORKER_THREADS** environment variable.
Controls which execution mode XLIO uses to handle networking and progress sockets.
Applicable only to POSIX API. XLIO Ultra API is not supported with Worker Threads mode.

**The Two Execution Modes:**

**Run-to-Completion Mode (value = 0, default):**
Application threads directly process networking operations within socket API calls.
XLIO code runs in the context of your application threads when they call socket functions
(send, recv, epoll_wait, poll, select, etc.).

Benefits:
- Best raw performance: lowest latency, highest throughput
- No additional CPU cores consumed by XLIO
- No inter-thread communication overhead
- Direct data path - packets processed immediately in calling thread
- Optimal cache locality - data stays on the same CPU that processes it

Requirements:
- Application must be network-aware and well-designed:
  - Avoid sharing sockets between threads
  - Each thread should have its own listen socket
  - Each thread should have its own epoll file descriptor
  - Application must call socket APIs frequently enough to progress networking
- Blocking in application code blocks networking progress

**Worker Threads Mode (value > 0):**
XLIO spawns N dedicated worker threads that continuously poll for and process network events
independently of the application. Application socket operations are delegated to these threads
via efficient job queues.

Benefits:
- Minimal network awareness required from application
- Sockets can be safely shared between application threads
- Single listen socket works (XLIO creates internal RSS children per worker)
- Single epoll context can be used across threads
- Works even if application rarely calls socket APIs
- Better suited for legacy applications not designed for high-performance networking

Costs:
- Each worker thread consumes 100% of a CPU core (busy-polling loop)
- Added latency from job queue communication between app threads and workers
- Memory overhead per worker thread (see "Resources per Worker Thread" below)
- TCP only - UDP sockets not currently supported in this mode
- Non-blocking sockets only - blocking connect() is not supported

**Resources Allocated per Worker Thread:**
Each worker thread gets its own entity_context containing:
- poll_group: Contains rings, completion queues, and hardware resources
- job_queue: Lock-free queue for receiving socket operations from application
- event_handler_manager_local: For timer and event management
- tcp_timers_collection: Per-worker TCP timer handling
- Statistics tracking structure

Note: Rings are created per socket/thread/interface based on
performance.rings.rx.allocation_logic and performance.rings.tx.allocation_logic settings.

**Choosing the Right Value:**

*Value = 0 (Run-to-Completion):*
Use when:
- Application is designed for high-performance networking
- Threads don't share sockets
- Low latency is critical (microsecond-sensitive)
- CPU efficiency matters (no cores dedicated to XLIO)
- Using XLIO Ultra API

*Value = 1:*
Minimal worker threads mode. Use when:
- Testing worker threads mode behavior
- Application has minimal networking needs
- Single-threaded applications that need background networking

*Value = 2-8 (Low):*
Use when:
- Application has moderate connection counts
- Want balanced resource usage
- Typical web servers or proxy applications

*Value = 8-32 (Medium):*
Use when:
- High connection counts with active traffic
- Multi-socket servers
- Want to match worker threads to CPU cores for NUMA locality

*Value = 32-512 (High):*
Use when:
- Very high connection density (thousands of connections)
- Each listen socket creates N RSS children, so high values enable
  massive parallel accept scaling
- WARNING: Each thread consumes a full CPU core - ensure sufficient
  CPU resources are available

**Scaling Considerations:**

1. CPU Usage: N worker threads = N CPU cores at 100% utilization
   (busy-polling). Plan hardware accordingly.

2. Listen Socket Scaling: When a socket calls listen(), XLIO creates
   N RSS child sockets (one per worker thread) with hardware flow
   steering rules to distribute incoming connections across workers.
   This enables parallel accept() processing.

3. Socket Distribution: Outgoing connections are distributed round-robin
   across worker threads via the entity_context_manager.

4. Memory Impact: Higher values increase total memory usage due to
   per-worker structures. Monitor with xlio_stats.

**Parameter Interactions:**

When worker_threads > 0, XLIO automatically adjusts:
- tx_buf_size: Set to 256KB for efficient batched TX operations
- tx_bufs_batch_tcp: Set to 1 for immediate TX processing
- select_poll_num: Set to -1 (worker threads handle polling)
- progress_engine_interval_msec: Set to 0 (workers poll continuously)

**Monitoring:**
Use xlio_stats to observe per-worker-thread statistics:
- idle_time: Time spent with no work (should be low if workers are needed)
- hit_poll_time: Time spent processing received packets
- job_proc_time: Time processing application job requests
- job_queue_size_max: Peak job queue depth (high values suggest backpressure)
- socket_num_added/removed: Socket lifecycle tracking

**Example Configurations:**

1. Latency-critical trading application:
   worker_threads = 0 (Run-to-Completion for minimal latency)

2. Nginx with 16 workers:
   worker_threads = 8 (half of worker count, efficient for event-driven apps)

3. Legacy application with socket sharing:
   worker_threads = 4-8 (provides execution context app doesn't provide)

4. High-connection-count load balancer:
   worker_threads = 16-32 (parallel accept and connection handling)
Default value is 0


================================================================================

PROFILES
--------

profiles.spec
Maps to **XLIO_SPEC** environment variable.
XLIO predefined specification profiles that optimize for different workload characteristics.
Each profile pre-configures dozens of internal parameters for a specific use case.

**IMPORTANT**: The integer values correspond to enum order, NOT latency ranking.

═══════════════════════════════════════════════════════════════════════════════
"none" or 0 - DEFAULT BALANCED CONFIGURATION
═══════════════════════════════════════════════════════════════════════════════
No profile optimizations applied. Uses XLIO defaults which provide:
- Moderate latency (100ms polling budget before sleep)
- Good throughput (TSO auto-detect, GRO enabled with 32 streams)
- Reasonable CPU usage (internal thread runs every 10ms)
- Ring per thread allocation (good parallelism)

Best for: General-purpose applications, initial testing, or when you want
full control over individual parameters.

═══════════════════════════════════════════════════════════════════════════════
"ultra_latency" or 1 - EXTREME LOW LATENCY (SINGLE-THREADED)
═══════════════════════════════════════════════════════════════════════════════
Aggressive optimizations for absolute minimum latency at the cost of CPU and throughput.

**Key Parameter Changes:**
- rx_poll_num = -1 (INFINITE busy polling - never sleeps)
- select_poll_num = -1 (INFINITE polling on select/poll/epoll)
- select_poll_os_ratio = 0 (NEVER polls OS file descriptors)
- rx_udp_poll_os_ratio = 0 (NEVER polls OS for UDP)
- progress_engine_interval_msec = 0 (DISABLES internal thread entirely)
- enable_tso = OFF (disables TCP Segmentation Offload)
- gro_streams_max = 0 (DISABLES Generic Receive Offload)
- tcp_nodelay = true (disables Nagle's algorithm)
- tx_bufs_batch_udp = 1, tx_bufs_batch_tcp = 1 (no TX batching)
- rx_bufs_batch = 4 (minimal RX batching)
- tx_num_wr_to_signal = 4 (frequent TX completions = lower jitter)
- cq_keep_qp_full = false (reduces completion overhead)
- ring_dev_mem_tx = 16KB (uses on-device memory for TX)
- memory_limit = 128MB (constrained memory footprint)
- internal_thread_affinity = CPU 0 (pinned)

**Performance Characteristics:**
+ Lowest possible latency (sub-microsecond improvements)
+ Lowest latency variance/jitter
+ Immediate packet transmission (no batching delays)
+ No context switches to internal thread
- 100% CPU utilization (busy polling never yields)
- Lower throughput (no TSO/GRO, small batches)
- Single-threaded model assumption
- Cannot handle non-offloaded sockets (OS polling disabled)
- Reduced memory efficiency

**When to Use:**
- High-frequency trading (HFT) applications
- Financial market data systems
- Real-time control systems
- Latency-critical request-response workloads
- Applications that can dedicate CPU cores to networking

**When NOT to Use:**
- Applications using non-offloaded sockets (they will starve)
- Throughput-sensitive workloads (bulk data transfer)
- Memory-constrained environments needing >128MB buffers
- Multi-threaded applications sharing sockets across threads

Example: profiles.spec=ultra_latency

═══════════════════════════════════════════════════════════════════════════════
"latency" or 2 - BALANCED LOW LATENCY
═══════════════════════════════════════════════════════════════════════════════
Low latency optimizations while maintaining system compatibility.

**Key Parameter Changes (vs ultra_latency):**
- rx_poll_num = -1 (still infinite polling)
- select_poll_num = -1 (still infinite polling)
- select_poll_os_ratio = 100 (STILL POLLS OS every 100 iterations)
- progress_engine_interval_msec = 100 (internal thread runs every 100ms)
- Other settings same as ultra_latency

**Key Differences from ultra_latency:**
- Maintains OS file descriptor polling (non-offloaded sockets work)
- Keeps internal thread for TCP state management (slower but safer)
- No memory_limit override (uses default, typically larger)
- Better compatibility with mixed socket workloads

**Performance Characteristics:**
+ Very low latency (slightly higher than ultra_latency)
+ Non-offloaded sockets still functional
+ TCP timers and state managed by internal thread
+ Better system integration
- Still high CPU usage (busy polling)
- Lower throughput than default (no TSO/GRO)

**When to Use:**
- Latency-sensitive apps that also use non-offloaded sockets
- Applications that cannot dedicate 100% CPU to networking
- Mixed workloads with some latency-critical paths
- When ultra_latency causes socket starvation issues

Example: profiles.spec=latency

═══════════════════════════════════════════════════════════════════════════════
"nginx" or 3 - HTTP PROXY / WEB SERVER THROUGHPUT
═══════════════════════════════════════════════════════════════════════════════
Optimized for nginx and similar HTTP proxy/reverse proxy workloads.
Automatically enabled when applications.nginx.workers_num > 0.

**Key Parameter Changes:**
- ring_allocation_logic = PER_INTERFACE (shared rings across workers)
- enable_tso = ON (TCP Segmentation Offload ENABLED)
- tcp_send_buffer_size = 2MB (large TCP send buffers)
- cq_poll_batch_max = 128 (large completion batches)
- tcp_push_flag = false (better TCP batching)
- tcp_3t_rules = true (3-tuple steering for efficiency)
- select_poll_num = 0 (poll once, then sleep)
- select_skip_os_fd_check = 1000 (rarely check OS)
- timer_resolution_msec = 32, tcp_timer_resolution_msec = 256 (slower timers)
- progress_engine_interval_msec = 0 (disabled)
- rx_cq_wait_ctrl = true (efficient epoll integration)
- rx_poll_on_tx_tcp = true (poll RX during TX for TCP ACKs)
- distribute_cq_interrupts = true (spread interrupts across workers)
- memory_limit = ~3-4GB per worker (scaled by worker count)

**Performance Characteristics:**
+ Maximum HTTP throughput (connections/sec, requests/sec)
+ Efficient resource sharing across nginx workers
+ Large TCP buffers for high bandwidth
+ TSO reduces CPU overhead for large responses
+ Optimized for epoll-based event loops
+ Lower CPU utilization (sleeps when idle)
- Higher latency than latency profiles
- Not suitable for latency-critical workloads
- Requires nginx worker configuration

**When to Use:**
- nginx reverse proxy / load balancer
- HAProxy and similar HTTP proxies
- Web servers serving large responses
- CDN edge servers
- Any epoll-based HTTP server

**REQUIRED**: Set applications.nginx.workers_num=<N> to enable.
Example: profiles.spec=nginx applications.nginx.workers_num=4

═══════════════════════════════════════════════════════════════════════════════
"nginx_dpu" or 4 - NGINX ON NVIDIA DPU (BlueField)
═══════════════════════════════════════════════════════════════════════════════
Same as nginx profile but tuned for DPU's constrained resources.

**Key Differences from nginx:**
- memory_limit = 512MB-1GB per worker (reduced for DPU)
- buffer_batching_mode = NONE (simpler buffer management)
- No rx_poll_on_tx_tcp (reduced complexity)

**When to Use:**
- nginx running inside NVIDIA BlueField DPU
- DPU-offloaded network proxy scenarios

Example: profiles.spec=nginx_dpu applications.nginx.workers_num=4

═══════════════════════════════════════════════════════════════════════════════
"nvme_bf3" or 5 - NVMe-oF / SPDK ON BLUEFIELD-3
═══════════════════════════════════════════════════════════════════════════════
Optimized for storage workloads (NVMe over Fabrics, SPDK) on BlueField-3 DPU.

**Key Parameter Changes:**
- strq_stride_num_per_rwqe = 8192 (large strides for big I/O)
- enable_lro = ON (Large Receive Offload ENABLED)
- enable_tso = ON (TSO ENABLED)
- tx_num_wr = 1024, tx_num_wr_to_signal = 128 (large TX batches)
- rx_num_wr = 32 (smaller RX queue)
- tcp_abort_on_close = true (RST on close for fast cleanup)
- handle_fork = false (no fork support needed)
- cq_aim_interval_msec = DISABLED (no adaptive moderation)
- progress_engine_interval_msec = 0 (disabled)
- memory_limit = 256MB, memory_limit_user = 2GB
- gro_streams_max = 0 (GRO disabled, using LRO instead)
- ring_dev_mem_tx = 1KB

**Performance Characteristics:**
+ Optimized for large sequential I/O patterns
+ LRO aggregates incoming storage data efficiently
+ Large TX batches for high IOPS
+ Fast connection cleanup (RST on close)
+ Tuned for BlueField-3 hardware capabilities
- Not suitable for general-purpose networking
- Requires BlueField-3 DPU

**When to Use:**
- SPDK NVMe-oF target on BlueField-3
- Storage disaggregation solutions
- High-performance storage networking

Example: profiles.spec=nvme_bf3

═══════════════════════════════════════════════════════════════════════════════
PROFILE SELECTION GUIDE
═══════════════════════════════════════════════════════════════════════════════

| Workload Type              | Recommended Profile | CPU Usage | Latency | Throughput |
|---------------------------|--------------------:|----------:|--------:|-----------:|
| HFT / Market Data          | ultra_latency (1)   | 100%      | Lowest  | Lower      |
| Real-time Control          | ultra_latency (1)   | 100%      | Lowest  | Lower      |
| Mixed Latency-Sensitive    | latency (2)         | High      | Low     | Moderate   |
| HTTP Proxy / Load Balancer | nginx (3)           | Moderate  | Higher  | Highest    |
| nginx on BlueField DPU     | nginx_dpu (4)       | Moderate  | Higher  | High       |
| NVMe-oF / SPDK on BF3      | nvme_bf3 (5)        | Moderate  | Moderate| High       |
| General Purpose            | none (0)            | Moderate  | Moderate| Good       |

**Note**: Individual parameters can still be overridden after profile selection.
The profile sets initial values which can be fine-tuned as needed.
Default value is 0


XLIO Monitoring & Performance Counters
=====================================
The XLIO internal performance counters include information per user
sockets and a global view on select() and epoll_wait() usage by the application.

Use the 'xlio_stats' included utility to view the per socket information and
performance counters during run time.
Usage:
        xlio_stats [-p pid] [-k directory] [-v view] [-d details] [-i interval]

Defaults:
        find_pid=enabled, directory="/tmp/", view=1, details=1, interval=1,

Options:
  -p, --pid=<pid>               Show XLIO statistics for process with pid: <pid>
  -k, --directory=<directory>   Set shared memory directory path to <directory>
  -n, --name=<application>      Show XLIO statistics for application: <application>
  -f, --find_pid                Find and show statistics for XLIO instance running (default)
  -F, --forbid_clean            By setting this flag inactive shared objects would not be removed
  -i, --interval=<n>            Print report every <n> seconds
  -c, --cycles=<n>              Do <n> report print cycles and exit, use 0 value for infinite (default)
  -v, --view=<1|2|3|4|5|6>      Set view type:
                                1 - Basic info
                                2 - Extra info
                                3 - Full info
                                4 - Multicast groups
                                5 - Show as 'netstat -tunaep'
                                6 - Entity Context info
  -d, --details=<1|2>           Set details mode:
                                1 - Totals
                                2 - Deltas
  -z, --zero                    Zero counters
  -l, --log_level=<level>       Set XLIO log level to <level>(1 <= level <= 7)
  -S, --fd_dump=<fd> [<level>]  Dump statistics for fd number <fd> using log level <level>. use 0 value for all open fds
  -D, --details_level=<level>   Set XLIO log details level to <level>(0 <= level <= 3)
  -s, --sockets=<list|range>    Log only sockets that match <list> or <range>, format: 4-16 or 1,9 (or combination)
  -V, --version                 Print version
  -h, --help                    Print this help message


Use monitor.stats.file_path to get internal XLIO statistics like xlio_stats provide.
If this parameter is set and the user application performed transmit or receive
activity on a socket, then these values will be logs once the sockets are closed.

Below is a logout example of a socket performance counters.
Below the logout example there is some explanations about the numbers.

XLIO: [fd=10] Tx Offload: 455 KB / 233020 / 0 / 3 [bytes/packets/drops/errors]
XLIO: [fd=10] Tx OS info:   0 KB /      0 / 0 [bytes/packets/errors]
XLIO: [fd=10] Rx Offload: 455 KB / 233020 / 0 / 0 [bytes/packets/eagains/errors]
XLIO: [fd=10] Rx byte: max 200 / dropped 0 (0.00%) / limit 2000000
XLIO: [fd=10] Rx pkt : max 1 / dropped 0 (0.00%)
XLIO: [fd=10] Rx OS info:   0 KB /      0 / 0 [bytes/packets/errors]
XLIO: [fd=10] Rx poll: 0 / 233020 (100.00%) [miss/hit]

Looking good :)
- No errors on transmit or receive on this socket (user fd=10)
- All the traffic was offloaded. No packets transmitted or receive via the OS.
- Just about no missed Rx polls (see performance.polling.blocking_rx_poll_usec & 
 performance.polling.iomux.poll_usec), meaning
 the receiving thread did not get to a blocked state to cause a contexts
 switch and hurt latency.
- No dropped packets caused by socket receive buffer limit (see XLIO_RX_BYTES_MIN).
- No 'No buffers' errors in Buffer Pools.
- No 'HW RX Packets' drops in CQs.

Interrupt Moderation
====================
The basic idea behind interrupt moderation is that the HW will not generate
interrupt for each packet, but instead only after some amount of packets received
or after the packet was held for some time.

The adaptive interrupt moderation change this packet count and time period
automatically to reach a desired rate of interrupts.


1. Use performance.polling.blocking_rx_poll_usec=0 and 
    performance.polling.iomux.poll_usec=0 to work in interrupt driven mode.

2. Control the period and frame count parameters with:
    performance.completion_queue.interrupt_moderation.packet_count - hold #count frames before interrupt
    performance.completion_queue.interrupt_moderation.period_usec - hold #usec before interrupt

3. Control the adaptive algorithm with the following:
    performance.completion_queue.interrupt_moderation.adaptive_count - max possible #count frames to hold
    performance.completion_queue.interrupt_moderation.adaptive_period_usec - max possible #usec to hold
    performance.completion_queue.interrupt_moderation.adaptive_interrupt_per_sec - desired interrupt rate
    performance.completion_queue.interrupt_moderation.adaptive_change_frequency_msec - frequency of adaptation

4. Disable CQ moderation with performance.completion_queue.interrupt_moderation.enable=false
5. Disable Adaptive CQ moderation with 
   performance.completion_queue.interrupt_moderation.adaptive_change_frequency_msec=0

Install library from rpm or debian
=================================

Installing:
Install the package as any other rpm or debian package [rpm -i libxlio.X.Y.Z-R.rpm].
The installation copies the XLIO library to: /usr/lib[64]/libxlio.so
The XLIO monitoring utility is installed at: /usr/bin/xlio_stats
The XLIO extra socket API is located at: /usr/include/mellanox/xlio_extra.h

Upgrading:
Use rpm update procedure: # rpm -U libxlio.X.Y.Z-R.rpm
You can upgrade by uninstalling (rpm -e) the previously installed package
before starting to install the new library rpm.

Uninstalling:
When uninstalling remember to uninstall (rpm -e) the package before you uninstall DOCA.

Troubleshooting
===============

* High log level:

 XLIO WARNING: *************************************************************
 XLIO WARNING: * XLIO is currently configured with high log level          *
 XLIO WARNING: * Application performance will decrease in this log level!  *
 XLIO WARNING: * This log level is recommended for debugging purposes only *
 XLIO WARNING: *************************************************************

This warning message means that you are using XLIO with high log level:
monitor.log.level variable value is set to 4 or more.
In order to fix it - set monitor.log.level to it's default value: 3

* CAP_NET_RAW and root access

 XLIO_WARNING: ******************************************************************************
 XLIO_WARNING: * Interface <Interface Name> will not be offloaded.
 XLIO_WARNING: * Offloaded resources are restricted to root or user with CAP_NET_RAW privileges
 XLIO_WARNING: * Read the CAP_NET_RAW and root access section in the XLIO's User Manual for more information
 XLIO_WARNING: ******************************************************************************

This warning message means that XLIO tried to create a hardware QP resource
while the kernel requires this operation to be performed only by privileged
users. Run as user root or grant CAP_NET_RAW privileges to your user

* Huge pages out of resource:

 XLIO WARNING: ************************************************************
 XLIO WARNING: NO IMMEDIATE ACTION NEEDED!
 XLIO WARNING: Not enough suitable hugepages to allocate 2097152 kB.
 XLIO WARNING: Allocation will be done with regular pages.
 XLIO WARNING: To avoid this message, either increase number of hugepages
 XLIO WARNING: or switch to a different memory allocation type:
 XLIO WARNING:   core.resources.hugepages.enable=false
 XLIO INFO   : Hugepages info:
 XLIO INFO   :   1048576 kB : total=0 free=0
 XLIO INFO   :   2048 kB : total=0 free=0
 XLIO WARNING: ************************************************************

This warning message means that you are using XLIO with hugepages memory allocation,
but not enough huge pages resources are available in the system.
If you want XLIO to take full advantage of the performance benefits of huge pages then
you should restart the application after adding more hugepages resources in your
system or trying to free unused hupepages shared memory segments with the below script.

NOTE: Use 'ipcs -m' and 'ipcrm -m shmid' to check and clean unused shared memory segments.
Below is a short script to help you release XLIO unused huge pages resources:
    for shmid in `ipcs -m | grep 0x00000000 | awk '{print $2}'`;
    do echo 'Clearing' $shmid; ipcrm -m $shmid;
    done;

For more information, refer to the "HugeTLB Pages" documentation of the Linux kernel.

* Not supported Bonding Configuration:

 XLIO WARNING: ******************************************************************************
 XLIO WARNING: XLIO doesn't support current bonding configuration of bond0.
 XLIO WARNING: The only supported bonding mode is "802.3ad(#4)" or "active-backup(#1)"
 XLIO WARNING: with "fail_over_mac=1" or "fail_over_mac=0".
 XLIO WARNING: The effect of working in unsupported bonding mode is undefined.
 XLIO WARNING: Read more about Bonding in the XLIO's User Manual
 XLIO WARNING: ******************************************************************************

This warning message means that XLIO has detected bonding device which is configured
to work in mode which is not supported by XLIO, this means that XLIO will not support
high availability events for that interface.
XLIO currently supports just active-backup(#1) or 802.3ad(#4) and fail_over_mac = 1 or 0 mode.
In order to fix this issue please change the bonding configuration.

Example:

Lets assume that the bonding device is bond0, which has two slaves: ib0 and
ib1.

Shut down the bond0 interface:
#ifconfig bond0 down

Find all the slaves of bond0:
#cat sys/class/net/bond0/bonding/slaves
ib0 ib1

Free all the slaves:
#echo -ib0 > /sys/class/net/bond0/bonding/slaves
#echo -ib1 > /sys/class/net/bond0/bonding/slaves

Change the bond mode:
#echo active-backup > /sys/class/net/bond0/bonding/mode

Change the fail_over_mac mode:
#echo 1 > /sys/class/net/bond0/bonding/fail_over_mac

Enslave the interfaces back:
#echo +ib0 > /sys/class/net/bond0/bonding/slaves
#echo +ib1 > /sys/class/net/bond0/bonding/slaves

Bring up the bonding interface:
#ifconfig bond0 up
OR
#ifconfig bond0 <ip> netmask <netmask> up

* Not supported Bonding & VLAN Configuration:

 XLIO WARNING: ******************************************************************
 XLIO WARNING: bond0.10: vlan over bond while fail_over_mac=1 is not offloaded
 XLIO WARNING: ******************************************************************

This warning message means that XLIO has detected bonding device which is configured with
VLAN over it while fail_over_mac=1.
This means that the bond will not be offloaded.
In order to fix this issue please change the bonding configuration.
