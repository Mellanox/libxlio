{
    "$schema": "http://json-schema.org/draft-07/schema#",
    "title": "XLIO Configuration Schema",
    "description": "Schema for XLIO configuration, used to validate the configuration files.",
    "type": "object",
    "properties": {
        "core": {
            "type": "object",
            "title": "Core Configuration",
            "description": "Essential configuration affecting XLIO's basic operation",
            "properties": {
                "resources": {
                    "type": "object",
                    "description": "Memory and CPU resource allocation settings.",
                    "properties": {
                        "memory_limit": {
                            "oneOf": [
                                {
                                    "type": "integer",
                                    "minimum": 0,
                                    "default": 2147483648
                                },
                                {
                                    "type": "string",
                                    "default": "2GB",
                                    "pattern": "^[0-9]+[KMGkmg]?[B]?$"
                                }
                            ],
                            "title": "Memory limit (bytes)",
                            "description": "Maps to XLIO_MEMORY_LIMIT environment variable.\nPre-allocated memory limit for buffers.\nNote that the limit does not include dynamic memory allocation\nand XLIO memory consumption can exceed the limit.\n0 means unlimited memory allocation.\nSupports suffixes: B, KB, MB, GB.",
                            "x-memory-size": true
                        },
                        "hugepages": {
                            "type": "object",
                            "description": "Hugepages configuration for optimized memory usage.",
                            "properties": {
                                "enable": {
                                    "type": "boolean",
                                    "default": true,
                                    "title": "Enable hugepages",
                                    "description": "Maps to XLIO_MEM_ALLOC_TYPE environment variable.\nUse huge pages for data buffers when available to improve performance\nby reducing TLB misses.\nXLIO will try to allocate data buffers as configured:\nwhen false, using malloc.\nwhen true, using huge pages.\nXLIO also overrides accordingly these rdma-core parameters:\nMLX_QP_ALLOC_TYPE and MLX_CQ_ALLOC_TYPE."
                                },
                                "size": {
                                    "oneOf": [
                                        {
                                            "type": "integer",
                                            "minimum": 0,
                                            "default": 0
                                        },
                                        {
                                            "type": "string",
                                            "default": "0B",
                                            "pattern": "^[0-9]+[KMGkmg]?[B]?$"
                                        }
                                    ],
                                    "title": "Hugepage size (bytes)",
                                    "description": "Maps to XLIO_HUGEPAGE_SIZE environment variable.\nForce specific hugepage size for XLIO internal memory allocations.\n0 allows to use any supported and available hugepages.\nMust be a power of 2, or 0.\nThe size may be specified with suffixes such as KB, MB, GB.\nSupports suffixes: B, KB, MB, GB.",
                                    "x-memory-size": true,
                                    "x-power-of-2-or-zero": true
                                }
                            }
                        },
                        "external_memory_limit": {
                            "oneOf": [
                                {
                                    "type": "integer",
                                    "minimum": 0,
                                    "default": 0
                                },
                                {
                                    "type": "string",
                                    "default": "0B",
                                    "pattern": "^[0-9]+[KMGkmg]?[B]?$"
                                }
                            ],
                            "title": "External memory limit (bytes)",
                            "description": "Maps to XLIO_MEMORY_LIMIT_USER environment variable.\nMemory limit for external user allocator.\nThe user allocator can optionally be provided with XLIO extra API.\n0 makes XLIO use the core.resources.memory_limit value for user allocations.\nSupports suffixes: B, KB, MB, GB.",
                            "x-memory-size": true
                        },
                        "heap_metadata_block_size": {
                            "oneOf": [
                                {
                                    "type": "integer",
                                    "minimum": 0,
                                    "default": 33554432
                                },
                                {
                                    "type": "string",
                                    "default": "32MB",
                                    "pattern": "^[0-9]+[KMGkmg]?[B]?$"
                                }
                            ],
                            "title": "Heap metadata block size",
                            "description": "Maps to XLIO_HEAP_METADATA_BLOCK environment variable.\nSize of metadata block added to every heap allocation.\nSupports suffixes: B, KB, MB, GB.",
                            "x-memory-size": true
                        }
                    },
                    "additionalProperties": false
                },
                "quick_init": {
                    "type": "boolean",
                    "default": false,
                    "title": "Quick initialization",
                    "description": "Maps to XLIO_QUICK_START environment variable.\nAvoid expensive extra checks to reduce the initialization time.\nThis may result in failures in case of a system misconfiguration.\nFor example, if the parameter is enabled and hugepages are requested\nbeyond the cgroup limit, XLIO crashes due to an access to an unmapped page."
                },
                "exception_handling": {
                    "type": "object",
                    "description": "How XLIO handles exceptions and errors.",
                    "properties": {
                        "mode": {
                            "oneOf": [
                                {
                                    "type": "integer",
                                    "enum": [
                                        -2,
                                        -1,
                                        0,
                                        1,
                                        2,
                                        3
                                    ],
                                    "default": -1
                                },
                                {
                                    "type": "string",
                                    "enum": [
                                        "exit",
                                        "handle_debug",
                                        "log_debug_undo_offload",
                                        "log_error_undo_offload",
                                        "log_error_return_error",
                                        "log_error_abort"
                                    ],
                                    "default": "handle_debug"
                                }
                            ],
                            "title": "Exception handling mode",
                            "description": "Maps to XLIO_EXCEPTION_HANDLING environment variable.\nMode for handling missing support or error cases in Socket API or functionality by XLIO.\nUseful for quickly identifying XLIO unsupported Socket API or features.\nUse:\n   - \"exit\" or -2 - to exit() on XLIO startup failure.\n   - \"handle_debug\" or -1 - for handling at DEBUG severity.\n   - \"log_debug_undo_offload\" or 0 - to log DEBUG message and\n      try recovering via Kernel network stack (un-offloading the socket).\n   - \"log_error_undo_offload\" or 1 - to log ERROR message and\n      try recovering via Kernel network stack (un-offloading the socket).\n   - \"log_error_return_error\" or 2 - to log ERROR message and\n      return API respectful error code.\n   - \"log_error_abort\" or 3 - to log ERROR message and\n      abort application (throw xlio_error exception)."
                        }
                    },
                    "additionalProperties": false
                },
                "signals": {
                    "type": "object",
                    "description": "Signal handling configuration.",
                    "properties": {
                        "sigint": {
                            "type": "object",
                            "description": "SIGINT handling behavior.",
                            "properties": {
                                "exit": {
                                    "type": "boolean",
                                    "default": true,
                                    "title": "Exit on SIGINT",
                                    "description": "Maps to XLIO_HANDLE_SIGINTR environment variable.\nWhen enabled, the library handler will be called when interrupt signal\nis sent to the process.\nXLIO will also call the application handler if it exists."
                                }
                            },
                            "additionalProperties": false
                        },
                        "sigsegv": {
                            "type": "object",
                            "description": "SIGSEGV handling behavior.",
                            "properties": {
                                "backtrace": {
                                    "type": "boolean",
                                    "default": false,
                                    "title": "Print backtrace on SIGSEGV",
                                    "description": "Maps to XLIO_HANDLE_SIGSEGV environment variable.\nWhen enabled, print backtrace if segmentation fault happens."
                                }
                            }
                        }
                    },
                    "additionalProperties": false
                },
                "syscall": {
                    "type": "object",
                    "description": "System call handling behavior.",
                    "properties": {
                        "dup2_close_fd": {
                            "type": "boolean",
                            "default": true,
                            "title": "Support dup2 calls",
                            "description": "Maps to XLIO_CLOSE_ON_DUP2 environment variable.\nWhen this parameter is enabled, XLIO will handle the duplicate fd (oldfd)\nas if it was closed (clear internal data structures) and only then,\nwill forward the call to the OS.\nThis is, in practice, a very rudimentary dup2 support.\nIt only supports the case where dup2 is used to close file descriptors."
                        },
                        "fork_support": {
                            "type": "boolean",
                            "default": true,
                            "title": "Enable fork support",
                            "description": "Maps to XLIO_FORK environment variable.\nControl whether XLIO should support fork.\nSetting this flag on will cause XLIO to call ibv_fork_init() function.\nibv_fork_init() initializes libibverbs data structures to handle fork()\nfunction calls correctly and avoid data corruption.\nIf ibv_fork_init() is not called or returns a non-zero status, then libibverbs\ndata structures are not fork()-safe and\nthe effect of an application calling fork() is undefined."
                        },
                        "deferred_close": {
                            "type": "boolean",
                            "default": false,
                            "title": "Defer closing of file descriptors",
                            "description": "Maps to XLIO_DEFERRED_CLOSE environment variable.\nDefers closing of file descriptors until the socket is actually closed,\nuseful for multi-threaded applications."
                        },
                        "allow_privileged_sockopt": {
                            "type": "boolean",
                            "default": true,
                            "title": "Allow privileged socket options",
                            "description": "Maps to XLIO_ALLOW_PRIVILEGED_SOCK_OPT environment variable.\nPermit the use of privileged socket options that might require special permissions."
                        },
                        "avoid_ctl_syscalls": {
                            "type": "boolean",
                            "default": false,
                            "title": "Avoid system control calls on TCP",
                            "description": "Maps to XLIO_AVOID_SYS_CALLS_ON_TCP_FD environment variable.\nFor TCP fd, avoid system calls for the supported options of:\nioctl, fcntl, getsockopt, setsockopt.\nNon-supported options will go to OS."
                        },
                        "sendfile_cache_limit": {
                            "oneOf": [
                                {
                                    "type": "integer",
                                    "minimum": 0,
                                    "default": 10737418240
                                },
                                {
                                    "type": "string",
                                    "default": "10GB",
                                    "pattern": "^[0-9]+[KMGkmg]?[B]?$"
                                }
                            ],
                            "title": "Sendfile byte limit",
                            "description": "Maps to XLIO_ZC_CACHE_THRESHOLD environment variable.\nMemory limit for the mapping cache which is used by sendfile().\nSupports suffixes: B, KB, MB, GB.",
                            "x-memory-size": true
                        }
                    },
                    "additionalProperties": false
                },
                "daemon": {
                    "type": "object",
                    "description": "XLIO daemon configuration.",
                    "properties": {
                        "enable": {
                            "type": "boolean",
                            "default": false,
                            "title": "Enable XLIO daemon",
                            "description": "Maps to XLIO_SERVICE_ENABLE environment variable.\nEnable the XLIO daemon service for additional monitoring capabilities."
                        },
                        "dir": {
                            "type": "string",
                            "default": "/tmp/xlio",
                            "title": "Daemon working directory",
                            "description": "Maps to XLIO_SERVICE_NOTIFY_DIR environment variable.\nSet the directory path for XLIO to write files used by xliod.\nNote: when used xliod must be run with --notify-dir directing the same folder."
                        }
                    },
                    "additionalProperties": false
                }
            },
            "additionalProperties": false
        },
        "network": {
            "type": "object",
            "title": "Network Configuration",
            "description": "All network-related settings and protocol configurations",
            "properties": {
                "timing": {
                    "type": "object",
                    "description": "Network timing and timestamp settings.",
                    "properties": {
                        "hw_ts_conversion": {
                            "oneOf": [
                                {
                                    "type": "integer",
                                    "enum": [
                                        0,
                                        1,
                                        2,
                                        3,
                                        4,
                                        5
                                    ],
                                    "default": 3
                                },
                                {
                                    "type": "string",
                                    "enum": [
                                        "disable",
                                        "raw_hw",
                                        "best_possible",
                                        "system",
                                        "ptp",
                                        "rtc"
                                    ],
                                    "default": "system"
                                }
                            ],
                            "title": "Timestamp conversion mode",
                            "description": "Maps to XLIO_HW_TS_CONVERSION environment variable.\nDefines how hardware timestamps are converted to a comparable format.\nThe value of network.timing.hw_ts_conversion is determined by all devices -\ni.e if the hardware of one device does not support the conversion,\nthen it will be disabled for the other devices.\nUse:\n   - \"disable\" or 0 to disable\n   - \"raw_hw\" or 1\n      only convert the time stamp to seconds.nano_seconds time units\n      (or disable if hardware does not supports).\n   - \"best_possible\" or 2\n      uses the best possible - raw hw or system time\n      Sync to system time, then Raw hardware time\n      disable if none of them are supported by hardware.\n   - \"system\" or 3\n      Sync to system time - convert the time stamp to seconds.nano_seconds\n      time units comparable to receive software timestamp.\n      disable if hardware does not support.\n   - \"ptp\" or 4 - PTP Sync\n      convert the time stamp to seconds.nano_seconds time units.\n      in case it is not supported -\n      will apply option \"system\" (or disable if hardware does not supports).\n   - \"rtc\" or 5 - RTC Sync\n      convert the time stamp to seconds.nano_seconds time units.\n      in case it is not supported -\n      will apply option \"system\" (or disable if hardware does not support)."
                        }
                    },
                    "additionalProperties": false
                },
                "protocols": {
                    "type": "object",
                    "description": "Protocol-specific configurations",
                    "properties": {
                        "ip": {
                            "type": "object",
                            "description": "IP configuration settings.",
                            "properties": {
                                "mtu": {
                                    "type": "integer",
                                    "minimum": 0,
                                    "maximum": 9000,
                                    "default": 0,
                                    "title": "MTU size",
                                    "description": "Maps to XLIO_MTU environment variable.\nSize of each Rx and Tx data buffer (Maximum Transfer Unit).\nThis value sets the fragmentation size of the packets sent by the library.\nIf network.protocols.ip.mtu is 0 then for each interface\nXLIO will follow the actual MTU.\nIf network.protocols.ip.mtu is greater than 0 then this MTU value is\napplicable to all interfaces regardless of their actual MTU."
                                }
                            },
                            "additionalProperties": false
                        },
                        "tcp": {
                            "type": "object",
                            "description": "TCP protocol settings.",
                            "properties": {
                                "wmem": {
                                    "oneOf": [
                                        {
                                            "type": "integer",
                                            "minimum": 0,
                                            "default": 1048576
                                        },
                                        {
                                            "type": "string",
                                            "default": "1MB",
                                            "pattern": "^[0-9]+[KMGkmg]?[B]?$"
                                        }
                                    ],
                                    "title": "Write buffer size (bytes)",
                                    "description": "Maps to XLIO_TCP_SEND_BUFFER_SIZE environment variable.\nTCP send buffer size of LWIP. This controls the maximum amount of data\nthat can be queued for transmission before the sender receives ACKs.\nSupports suffixes: B, KB, MB, GB.\n\nPERFORMANCE TRADEOFFS:\n\nHigher values (e.g., 2MB+):\n   - Improves throughput on high bandwidth-delay product (BDP) networks\n     (e.g., high-speed links with latency > 1ms).\n   - Allows the application to queue more data without blocking or receiving\n     EAGAIN, reducing context switches.\n   - Better handles bursty workloads.\n   - CAUTION: Increases memory consumption per socket. With many concurrent\n     connections, this can add up significantly.\n   - CAUTION: May increase latency (bufferbloat) since more data is queued\n     before TCP flow control kicks in.\n\nLower values (e.g., 64KB-256KB):\n   - Reduces memory footprint per socket, beneficial for servers with\n     thousands of concurrent connections.\n   - Provides faster flow control feedback to the application.\n   - Better latency characteristics for latency-sensitive applications.\n   - CAUTION: May limit throughput on high-latency or high-bandwidth networks,\n     as the pipe cannot be kept full.\n   - CAUTION: Non-blocking sockets may receive more frequent EAGAIN errors.\n\nSIZING GUIDANCE:\n   - For optimal throughput, size >= bandwidth * round-trip-time (BDP).\n     Example: 10 Gbps link with 1ms RTT = 10 Gbps * 0.001s = 1.25 MB.\n   - For latency-sensitive applications, smaller is generally better.\n   - Can be overridden per-socket using SO_SNDBUF setsockopt.\n   - Default of 1MB is suitable for most datacenter and LAN environments.",
                                    "x-memory-size": true
                                },
                                "nodelay": {
                                    "type": "object",
                                    "description": "TCP_NODELAY behavior configuration.\n\nControls the Nagle algorithm, which batches small outgoing TCP segments\nto improve network efficiency. These settings allow fine-tuning the\nlatency vs. throughput tradeoff for your workload.",
                                    "properties": {
                                        "enable": {
                                            "type": "boolean",
                                            "default": false,
                                            "title": "Disable Nagle's algorithm",
                                            "description": "Maps to XLIO_TCP_NODELAY environment variable.\nWhen true, disables Nagle's algorithm for each TCP socket during initialization.\nThis means TCP segments are sent as soon as possible, even for small data.\n\nHOW NAGLE'S ALGORITHM WORKS:\nWith Nagle enabled (enable=false), TCP buffers small writes and waits to send until:\n   - All previously sent data has been acknowledged, OR\n   - Enough data accumulates to fill a Maximum Segment Size (MSS, typically ~1460 bytes).\nThis reduces the number of small packets ('tinygrams') sent over the network.\n\nPERFORMANCE TRADEOFFS:\n\nenable=true (Nagle DISABLED - low latency mode):\n   + Minimizes transmission latency - data sent immediately.\n   + Critical for latency-sensitive applications: financial trading, gaming,\n     real-time telemetry, interactive protocols (SSH, Telnet).\n   + Better for request-response patterns where you send a small request\n     and wait for a reply before sending more.\n   - Increases packet rate and network overhead (more TCP/IP headers per byte).\n   - May reduce throughput on high-latency links due to more round-trips.\n   - Higher CPU usage from processing more packets.\n\nenable=false (Nagle ENABLED - default, high throughput mode):\n   + Better throughput for bulk data transfers and streaming.\n   + Fewer packets means less network overhead and CPU usage.\n   + More efficient use of network bandwidth.\n   - Adds latency (up to ~200ms worst case) waiting to batch small writes.\n   - Can cause 'write-write-read' deadlocks in poorly designed protocols\n     where receiver waits for complete message while sender waits to batch.\n\nRECOMMENDATIONS:\n   - Bulk transfers, streaming, file copies: keep disabled (enable=false).\n   - Low-latency trading, gaming, interactive apps: enable (enable=true).\n   - If enabling, consider also enabling tcp.quickack for symmetric low latency.\n   - Can be overridden per-socket using setsockopt(TCP_NODELAY)."
                                        },
                                        "byte_threshold": {
                                            "type": "integer",
                                            "default": 0,
                                            "minimum": 0,
                                            "title": "Data threshold for flush",
                                            "description": "Maps to XLIO_TCP_NODELAY_TRESHOLD environment variable.\nEffective only if network.protocols.tcp.nodelay.enable is true.\nSets a minimum data size threshold before triggering immediate transmission.\n\nHOW IT WORKS:\nWhen nodelay.enable is true, this threshold adds a condition:\ndata is sent immediately only if unsent_data_bytes >= byte_threshold.\nSmaller writes are still batched until they exceed the threshold.\n\nPERFORMANCE TRADEOFFS:\n\nbyte_threshold=0 (default - pure TCP_NODELAY):\n   + Lowest possible latency - every write triggers immediate send.\n   + Standard TCP_NODELAY behavior expected by most applications.\n   - Maximum packet overhead - each tiny write becomes its own packet.\n   - Highest CPU and network overhead.\n\nLow values (1-100 bytes):\n   + Still very low latency for most practical message sizes.\n   + Filters out accidental tiny writes (single bytes, partial headers).\n   - Minimal practical benefit over threshold=0.\n\nMedium values (100-500 bytes):\n   + Good balance for mixed workloads with small and medium messages.\n   + Reduces packet rate while maintaining low latency for real messages.\n   + Useful when application sends small control messages mixed with data.\n   - May add slight latency if messages are smaller than threshold.\n\nHigh values (500-1460 bytes, approaching MSS):\n   + Approaches Nagle-like batching while still having nodelay flag.\n   + Useful for applications that want nodelay semantics but send\n     many small writes that should be coalesced.\n   - Defeats much of the purpose of enabling nodelay.\n   - Values >= MSS (~1460) make nodelay nearly ineffective.\n\nRECOMMENDATIONS:\n   - For classic TCP_NODELAY behavior, use 0 (default).\n   - For request-response with known message sizes, set slightly below\n     your typical message size to avoid fragmenting single messages\n     into multiple packets while still getting low latency.\n   - Typical values: 0 for trading/gaming, 64-256 for mixed workloads.\n   - Values above MSS (~1460 bytes) are rarely useful."
                                        }
                                    }
                                },
                                "quickack": {
                                    "type": "boolean",
                                    "default": false,
                                    "title": "Enable quick ACKs",
                                    "description": "Maps to XLIO_TCP_QUICKACK environment variable.\nControls TCP's delayed acknowledgement behavior. When true, disables delayed ACKs,\ncausing TCP to send an acknowledgement immediately after receiving each packet.\n\nHOW DELAYED ACKs WORK:\nBy default (quickack=false), TCP may delay sending ACKs:\n   - TCP waits up to tcp.timer_msec/2 (default ~50ms) before sending an ACK.\n   - This allows batching multiple ACKs together and piggybacking ACKs on data.\n   - Reduces ACK packet count by up to 50% compared to immediate ACKs.\n\nPERFORMANCE TRADEOFFS:\n\nquickack=true (Delayed ACKs DISABLED - low latency mode):\n   + Lower round-trip latency - sender receives ACKs immediately.\n   + Faster TCP congestion window growth during connection ramp-up,\n     since CWND increases with each ACK received.\n   + Better for latency-sensitive request-response protocols.\n   + Avoids Nagle-delayed ACK interaction: when sender uses Nagle algorithm\n     and receiver uses delayed ACKs, both sides wait for each other,\n     causing latency spikes of 40-200ms ('ACK starvation').\n   + Recommended when tcp.nodelay.enable=true for symmetric low latency.\n   - Increases packet rate (more ACK packets on the network).\n   - Higher CPU overhead processing more ACKs on sender side.\n   - More network bandwidth consumed by ACK traffic.\n\nquickack=false (Delayed ACKs ENABLED - default, efficient mode):\n   + Reduced ACK traffic - fewer packets means less network overhead.\n   + More efficient for bulk transfers and streaming workloads.\n   + Lower CPU usage from processing fewer ACK packets.\n   + ACKs can be piggybacked on response data, saving packets.\n   - Adds latency for the sender (up to tcp.timer_msec/2, default ~50ms).\n   - Slower congestion window growth during connection startup.\n   - Can interact poorly with Nagle algorithm on sender, causing delays.\n\nRECOMMENDATIONS:\n   - Financial trading, gaming, real-time apps: enable (true) with nodelay.\n   - Bulk transfers, streaming, high-throughput apps: keep disabled (false).\n   - If latency spikes occur with small messages, try enabling quickack.\n   - For asymmetric patterns (e.g., server sends data, client ACKs),\n     enable quickack on the ACK-heavy side (client).\n   - Can be overridden per-socket using setsockopt(TCP_QUICKACK)."
                                },
                                "push": {
                                    "type": "boolean",
                                    "default": true,
                                    "title": "Set TCP Push flag",
                                    "description": "Maps to XLIO_TCP_PUSH_FLAG environment variable.\nControls whether the TCP PUSH (PSH) flag is set on the last segment of each write operation.\n\nHOW THE PUSH FLAG WORKS:\nThe TCP PUSH flag is a signal from the sender to the receiver that this segment\ncontains data that should be delivered to the application immediately, rather\nthan waiting for more data to arrive. When enabled, XLIO sets the PSH flag on\nthe last segment of every tcp_write operation.\n\nSender behavior:\n   - When enabled (push=true): PSH flag is set on the last segment of each write.\n   - When disabled (push=false): No PSH flags are set; data is sent without urgency hint.\n\nReceiver behavior:\n   - PSH flag signals the TCP stack to deliver buffered data to the application.\n   - Without PSH, the receiver may buffer data waiting for more segments.\n\nPERFORMANCE TRADEOFFS:\n\npush=true (PUSH flag ENABLED - default, low latency mode):\n   + Receiver delivers data immediately upon receiving the segment.\n   + Lower end-to-end latency for request-response patterns.\n   + Essential for interactive protocols (telnet, SSH, real-time messaging).\n   + Prevents receiver from buffering when waiting for more data.\n   + Better for applications sending discrete messages/records.\n   - May slightly increase receiver CPU usage due to more frequent delivery events.\n   - Receiver cannot batch multiple segments before delivery.\n\npush=false (PUSH flag DISABLED - throughput mode):\n   + Receiver can buffer data more efficiently.\n   + Better for high-throughput bulk transfers (file copies, streaming).\n   + Reduces delivery events on the receiver, lowering CPU overhead.\n   + Used by NGINX profile for maximum throughput scenarios.\n   - Higher latency: receiver may wait for more data before delivering.\n   - Can cause delays in request-response patterns.\n   - May cause application-level timeouts if receiver waits too long.\n   - Not recommended for interactive or latency-sensitive applications.\n\nTSO (TCP Segmentation Offload) INTERACTION:\n   - The PSH flag does NOT prevent TSO segment merging.\n   - Segments with PSH+ACK flags can still be merged for TSO.\n   - TSO efficiency is preserved regardless of this setting.\n\nRECOMMENDATIONS:\n   - Interactive apps (trading, gaming, messaging): keep enabled (true).\n   - Request-response APIs (HTTP, RPC): keep enabled (true).\n   - Bulk transfers (file copy, backup, streaming): consider disabling (false).\n   - High-throughput servers (NGINX, web servers): consider disabling (false).\n   - When disabled, ensure application protocol handles message boundaries.\n   - If experiencing latency issues with push=false, re-enable this option.\n   - For mixed workloads, keep enabled (true) as latency impact is usually more\n     noticeable than the throughput benefit from disabling."
                                },
                                "linger_0": {
                                    "type": "boolean",
                                    "default": false,
                                    "title": "Abort TCP connections on close",
                                    "description": "Maps to XLIO_TCP_ABORT_ON_CLOSE environment variable.\nControls how XLIO terminates TCP connections when close() is called.\n\nHOW TCP CONNECTION TERMINATION WORKS:\nTCP defines two ways to close a connection:\n1. Graceful close (FIN): Four-way handshake ensuring all data is delivered.\n2. Abortive close (RST): Immediate termination, discarding pending data.\n\nThis parameter selects which method XLIO uses by default.\n\nlinger_0=true (RST/ABORTIVE CLOSE - fast teardown mode):\n   XLIO immediately sends a RST (Reset) segment and discards all TCP state.\n   The connection is terminated instantly without waiting for acknowledgments.\n\n   + Immediate connection teardown - no handshake delay.\n   + No TIME_WAIT state - socket resources freed instantly.\n   + Faster port reuse - ephemeral ports available immediately.\n   + Lower memory footprint - no lingering socket state.\n   + Prevents TIME_WAIT accumulation on high-churn servers.\n   + Better for servers handling thousands of short-lived connections.\n   + Used by nvme_bf3 profile for maximum storage throughput.\n   - Pending send buffer data is DISCARDED (potential data loss).\n   - Unacknowledged data is NOT retransmitted.\n   - Peer receives RST and may see ECONNRESET error.\n   - Non-standard TCP behavior (violates graceful shutdown RFC).\n   - Peer application may not process final data.\n   - Can cause issues with protocols requiring complete delivery.\n   - Not suitable when data integrity is critical.\n\nlinger_0=false (FIN/GRACEFUL CLOSE - reliable mode, default):\n   XLIO performs standard TCP graceful shutdown:\n   1. Sends any pending data in the send buffer.\n   2. Sends FIN segment to signal end of transmission.\n   3. Waits for peer's FIN+ACK (enters FIN_WAIT_1, FIN_WAIT_2 states).\n   4. Enters TIME_WAIT state for 2*MSL (typically 60-120 seconds).\n\n   + All pending data is transmitted and acknowledged.\n   + Reliable delivery - peer receives all data.\n   + Standard TCP behavior - follows RFC specifications.\n   + No data loss risk - graceful handshake guarantees delivery.\n   + Better debugging - clean connection state transitions.\n   + Required for protocols needing complete data delivery.\n   - TIME_WAIT state consumes socket resources for 60-120 seconds.\n   - High connection churn can exhaust ephemeral ports.\n   - Slower teardown - requires multiple round-trips.\n   - Memory overhead for TIME_WAIT socket tracking.\n   - May limit maximum connection rate on busy servers.\n\nADDITIONAL ABORT TRIGGERS:\nEven when linger_0=false, XLIO sends RST (abortive close) if:\n   - Application has unread data in the receive buffer when close() is called.\n   - SO_LINGER socket option is set with l_linger=0.\n   - Process is terminating (shutdown scenario).\n\nPERFORMANCE IMPACT:\n\nHigh-connection-rate servers (e.g., reverse proxies, load balancers):\n   - TIME_WAIT accumulation can become a bottleneck.\n   - With 10,000 connections/second and 60s TIME_WAIT: 600,000 sockets in TIME_WAIT.\n   - Each TIME_WAIT socket consumes ~300-400 bytes of memory.\n   - Enable linger_0=true if connection rate is limiting throughput.\n\nLatency-sensitive applications:\n   - linger_0=true eliminates FIN handshake latency (1-2 RTT savings).\n   - Useful when connection setup/teardown latency matters.\n\nData-critical applications:\n   - Keep linger_0=false to ensure all data is delivered.\n   - Required for transactional systems, databases, file transfers.\n\nRECOMMENDATIONS:\n   - High-throughput web servers (Nginx, HAProxy): consider enabling (true).\n   - Storage/NVMe applications: enable (true) for maximum throughput.\n   - Short-lived connection servers: enable (true) to prevent port exhaustion.\n   - Financial/trading systems: keep disabled (false) for data integrity.\n   - Database connections: keep disabled (false).\n   - File transfer applications: keep disabled (false).\n   - When unsure: keep disabled (false) - it's the safer default.\n   - If experiencing TIME_WAIT buildup: enable (true) or tune kernel settings."
                                },
                                "congestion_control": {
                                    "oneOf": [
                                        {
                                            "type": "integer",
                                            "enum": [
                                                0,
                                                1,
                                                2
                                            ],
                                            "default": 0
                                        },
                                        {
                                            "type": "string",
                                            "enum": [
                                                "lwip",
                                                "cubic",
                                                "disable"
                                            ],
                                            "default": "lwip"
                                        }
                                    ],
                                    "title": "TCP congestion control algorithm",
                                    "description": "Maps to XLIO_TCP_CC_ALGO environment variable.\nTCP congestion control algorithm.\nThe default algorithm coming with LWIP is a variation of Reno/New-Reno.\nThe new Cubic algorithm was adapted from FreeBSD implementation.\nUse:\n   - \"lwip\" or 0 for LWIP algorithm.\n   - \"cubic\" or 1 for Cubic algorithm.\n   - \"disable\" or 2 to disable the congestion algorithm."
                                },
                                "timestamps": {
                                    "oneOf": [
                                        {
                                            "type": "integer",
                                            "enum": [
                                                0,
                                                1,
                                                2
                                            ],
                                            "default": 0
                                        },
                                        {
                                            "type": "string",
                                            "enum": [
                                                "disable",
                                                "enable",
                                                "os"
                                            ],
                                            "default": "disable"
                                        }
                                    ],
                                    "title": "TCP timestamps mode",
                                    "description": "Maps to XLIO_TCP_TIMESTAMP_OPTION environment variable.\nIf set, enable TCP timestamp option.\nCurrently, LWIP is not supporting RTTM and PAWS mechanisms.\nSee RFC1323 for info.\nUse:\n   - \"disable\" or 0 to disable.\n   - \"enable\" or 1 to enable.\n   - \"os\" or 2 for OS follow up.\nNote that enabling causes a slight performance degradation."
                                },
                                "timer_msec": {
                                    "type": "integer",
                                    "minimum": 0,
                                    "default": 100,
                                    "title": "TCP timer interval (msec)",
                                    "description": "Maps to XLIO_TCP_TIMER_RESOLUTION_MSEC environment variable.\nControls the resolution of TCP's internal timers in milliseconds.\n\nTHIS TIMER DRIVES TWO CRITICAL TCP MECHANISMS:\n\n1. FAST TIMER (fires every timer_msec):\n   - Sends delayed ACKs when data has been received but not yet acknowledged.\n   - Maximum delayed ACK latency is timer_msec (e.g., 100ms default).\n\n2. SLOW TIMER (fires every timer_msec * 2):\n   - Retransmission timeout (RTO) detection - determines when lost packets\n     are retransmitted.\n   - Persist timer - sends zero-window probes when receiver advertises\n     zero window.\n   - Keepalive probes - checks if idle connections are still alive.\n   - Connection state timeouts - cleans up connections in FIN_WAIT,\n     SYN_RCVD, TIME_WAIT, and LAST_ACK states.\n   - Out-of-sequence data cleanup.\n\nPERFORMANCE TRADEOFFS:\n\nLow values (10-50ms) - LATENCY-OPTIMIZED:\n   + Faster detection of packet loss - retransmissions triggered sooner.\n   + Lower delayed ACK latency - sender receives ACKs faster, improving\n     throughput ramp-up and RTT measurements.\n   + More responsive connection state machine.\n   + Better for latency-sensitive applications (trading, gaming, real-time).\n   - Higher CPU overhead - timers checked more frequently.\n   - Increased context switching and power consumption.\n   - May trigger premature retransmissions on high-latency networks.\n\nHigh values (200-500ms) - CPU-OPTIMIZED:\n   + Lower CPU overhead - less frequent timer processing.\n   + Better batching of timer events.\n   + Reduced power consumption.\n   + More efficient for high-connection-count servers.\n   - Slower packet loss detection - longer time before retransmission.\n   - Higher delayed ACK latency - can slow down sender's congestion\n     window growth.\n   - Less responsive to network changes.\n   - Connection state transitions (close, timeout) take longer.\n\nRECOMMENDATIONS:\n   - Latency-critical applications: 10-50ms\n   - General-purpose / balanced: 100ms (default)\n   - High connection count with CPU constraints: 200-500ms\n   - Never exceed 500ms (RFC 1122 requires delayed ACK < 500ms)\n\nCONSTRAINTS:\n   - Minimum effective value: performance.threading.internal_handler.timer_msec\n     (if set lower, it will be clamped to the internal thread timer).\n   - Maximum recommended: 500ms per RFC 1122 Section 4.2.3.2."
                                },
                                "mss": {
                                    "type": "integer",
                                    "minimum": 0,
                                    "maximum": 8960,
                                    "default": 0,
                                    "title": "Maximum Segment Size",
                                    "description": "Maps to XLIO_MSS environment variable.\nDefines the maximum TCP payload size (in bytes) that can be sent in a single\nTCP segment without requiring IP fragmentation.\n\nVALUE INTERPRETATION:\n   - 0 (default): Auto-calculate MSS based on network.protocols.ip.mtu,\n     leaving 40 bytes for IP (20) + TCP (20) headers:\n     \"TCP MSS = network.protocols.ip.mtu - 40\"\n   - Non-zero: Forces MSS to that specific value regardless of MTU.\n\nCOMMON MSS VALUES:\n   - 1460 bytes: Standard Ethernet (MTU 1500 - 40 = 1460)\n   - 8960 bytes: Jumbo frames (MTU 9000 - 40 = 8960, maximum supported)\n   - 536 bytes: Conservative minimum for unknown network paths\n\nIMPACT ON CONGESTION CONTROL:\nMSS directly affects TCP congestion window (cwnd) growth, which determines\nhow much data can be in flight:\n\n   - Slow Start phase: cwnd increases by 1 MSS per ACK received.\n     Higher MSS = faster throughput ramp-up in bytes/second.\n   - Congestion Avoidance phase: cwnd increases by ~MSS²/cwnd per ACK,\n     roughly 1 MSS per RTT. Higher MSS = larger increments.\n   - ssthresh (slow start threshold) minimum is 2×MSS.\n   - After timeout: cwnd resets to 1 MSS.\n\nExample: To reach 10MB cwnd from 14KB ssthresh at 1ms RTT:\n   - With MSS=1460: ~7000 RTTs = ~7 seconds\n   - With MSS=8960: ~1100 RTTs = ~1.1 seconds\n\nPERFORMANCE TRADEOFFS:\n\nHigh MSS values (4000-8960 bytes, Jumbo frames):\n   + Faster congestion window growth - quicker throughput ramp-up.\n   + Higher throughput efficiency - more payload per packet.\n   + Better for bulk data transfers - fewer packets, less header overhead.\n   + Reduced CPU overhead - fewer packets to process per byte transferred.\n   + Enhanced TSO (TCP Segmentation Offload) efficiency - NIC aggregates\n     more data per super-packet.\n   - Requires network infrastructure supporting Jumbo frames end-to-end.\n   - Higher latency per packet - larger packets take longer to transmit.\n   - Larger retransmission penalty - losing one packet means losing more data.\n   - May cause IP fragmentation if path MTU is smaller (severely degrades\n     performance - fragments are reassembled in software).\n\nLow MSS values (536-1460 bytes, standard Ethernet):\n   + Compatible with all networks - standard 1500-byte MTU is universal.\n   + Lower per-packet latency - smaller packets transmit faster.\n   + Smaller retransmission cost - less data lost per dropped packet.\n   + Better for lossy networks - faster recovery from packet loss.\n   - Slower congestion window growth in bytes - longer throughput ramp-up.\n   - Higher overhead - more headers per byte of payload.\n   - More packets to process - higher CPU utilization per byte transferred.\n   - Less efficient TSO aggregation.\n\nVery low MSS values (<536 bytes):\n   - WARNING: Generally not recommended.\n   - Extremely high header overhead (40 bytes header for <536 payload).\n   - Very slow cwnd growth - may never reach line rate on high-BDP paths.\n   - Increased packet processing overhead.\n   - Only useful for highly specialized latency-critical applications\n     with tiny messages.\n\nTSO (TCP SEGMENTATION OFFLOAD) INTERACTION:\nWhen TSO is enabled (hardware_features.tcp.tso.enable), the NIC aggregates\nmultiple MSS-sized segments into larger super-packets for transmission.\n   - MSS defines the segment granularity for TSO aggregation.\n   - Higher MSS with TSO = fewer segments to manage, better efficiency.\n   - TSO max payload (hardware_features.tcp.tso.max_size) caps the total\n     aggregated size regardless of MSS.\n\nCRITICAL CONSTRAINT - FRAGMENTATION:\nMSS must not exceed (Path MTU - 40). If MSS is larger than what the network\npath can handle:\n   - Packets will be IP-fragmented (very bad for performance).\n   - Fragments require CPU reassembly, defeating hardware offload benefits.\n   - Packet loss of any fragment requires retransmitting entire original packet.\n   - Always ensure MSS ≤ min(MTU along path) - 40.\n\nRECOMMENDATIONS:\n   - Default (0): Best for most deployments - auto-adapts to interface MTU.\n   - Standard Ethernet: Use 0 or 1460 (for MTU 1500).\n   - Jumbo frame networks: Use 0 with network.protocols.ip.mtu=9000,\n     or explicitly set to 8960.\n   - Mixed environments: Use 0 to auto-adapt per interface.\n   - High-throughput bulk transfers: Maximize MSS with Jumbo frames if\n     network supports it.\n   - Latency-sensitive (trading, gaming): Standard 1460 is usually optimal;\n     avoid very low values as they slow cwnd growth.\n\nMAXIMUM VALUE: 8960 bytes (Jumbo MTU 9000 - 40 byte headers)."
                                }
                            },
                            "additionalProperties": false
                        }
                    },
                    "additionalProperties": false
                },
                "multicast": {
                    "type": "object",
                    "description": "Multicast traffic settings.",
                    "properties": {
                        "mc_loopback": {
                            "type": "boolean",
                            "default": true,
                            "title": "Enable multicast loopback",
                            "description": "Maps to XLIO_TX_MC_LOOPBACK environment variable.\nThis parameter sets the initial value used by XLIO internally\nto control the multicast loopback packets behavior during transmission.\nAn application that calls setsockopt() with IP_MULTICAST_LOOP will\nrun over the initial value set by this parameter.\nRead more in 'Multicast loopback behavior' in notes section below."
                        },
                        "mc_flowtag_acceleration": {
                            "type": "boolean",
                            "default": false,
                            "title": "Accelerate flowtag for multicast",
                            "description": "Maps to XLIO_MC_FORCE_FLOWTAG environment variable.\nForces the use of flow tag acceleration for multicast flows where\n(SO_REUSEADDR) is set.\nApplicable if there are no other sockets opened for the same flow in system."
                        },
                        "wait_after_join_msec": {
                            "type": "integer",
                            "default": 0,
                            "minimum": 0,
                            "title": "Delay after multicast join (msec)",
                            "description": "Maps to XLIO_WAIT_AFTER_JOIN_MSEC environment variable.\nThis parameter indicates the time of delay in milliseconds for the first packet\nsent after receiving the multicast JOINED event from the SM.\nThis is helpful to overcome loss of first few packets of an outgoing stream due to\nSM lengthy handling of MFT configuration on the switch chips."
                        }
                    },
                    "additionalProperties": false
                },
                "neighbor": {
                    "type": "object",
                    "description": "Network neighbor discovery and ARP settings.",
                    "properties": {
                        "update_interval_msec": {
                            "type": "integer",
                            "minimum": 0,
                            "default": 10000,
                            "title": "Neighbor update interval (msec)",
                            "description": "Maps to XLIO_NETLINK_TIMER environment variable.\nSets the interval in milliseconds between neighbor table updates."
                        },
                        "errors_before_reset": {
                            "type": "integer",
                            "minimum": 0,
                            "default": 1,
                            "title": "Errors before neighbor reset",
                            "description": "Maps to XLIO_NEIGH_NUM_ERR_RETRIES environment variable.\nNumber of retries to restart the neighbor state machine after receiving an ERROR event."
                        },
                        "arp": {
                            "type": "object",
                            "description": "ARP settings.",
                            "properties": {
                                "uc_retries": {
                                    "type": "integer",
                                    "minimum": 0,
                                    "default": 3,
                                    "title": "Unicast ARP retries",
                                    "description": "Maps to XLIO_NEIGH_UC_ARP_QUATA environment variable.\nNumber of unicast ARP retries before sending\nbroadcast ARP when neigh state is NUD_STALE."
                                },
                                "uc_delay_msec": {
                                    "type": "integer",
                                    "minimum": 0,
                                    "default": 10000,
                                    "title": "Unicast ARP delay (msec)",
                                    "description": "Maps to XLIO_NEIGH_UC_ARP_DELAY_MSEC environment variable.\nTime in milliseconds to wait between unicast ARP attempts."
                                }
                            },
                            "additionalProperties": false
                        }
                    },
                    "additionalProperties": false
                }
            }
        },
        "hardware_features": {
            "type": "object",
            "title": "Hardware Features",
            "description": "Hardware-specific configurations and offloads",
            "properties": {
                "striding_rq": {
                    "type": "object",
                    "description": "Striding Receive Queue settings for optimized packet processing.",
                    "properties": {
                        "enable": {
                            "type": "boolean",
                            "default": true,
                            "title": "Enable striding receive queues",
                            "description": "Maps to XLIO_STRQ environment variable.\nEnable/Disable Striding Receive Queues (STRQ).\n\nStriding RQ vs Regular RQ:\nWith regular RQ, each Work Queue Element (WQE) receives exactly one packet.\nWith Striding RQ, each WQE is divided into multiple strides (slots),\nallowing a single WQE to receive many packets. This significantly reduces\nWQE replenishment overhead under high packet rates.\n\nBenefits of Striding RQ (enabled):\n• Higher packet rates with lower CPU overhead for WQE management.\n• Better batching - fewer doorbell writes to the NIC.\n• Dynamic stride allocation from an expandable pool.\n• Recommended for most workloads, especially high packet-rate scenarios.\n\nWhen to disable:\n• Legacy compatibility requirements.\n• Specific debugging or troubleshooting scenarios.\n• Environments where Striding RQ is not supported by hardware.\n\nWQE buffer size = strides_num × stride_size.\nWith defaults (2048 × 64 bytes), each WQE can hold up to 2048 small packets\nor fewer large packets that span multiple strides."
                        },
                        "strides_num": {
                            "type": "integer",
                            "default": 2048,
                            "minimum": 512,
                            "maximum": 65536,
                            "title": "Number of strides per WQE",
                            "description": "Maps to XLIO_STRQ_NUM_STRIDES environment variable.\nThe number of strides (packet slots) in each receive Work Queue Element (WQE).\nMust be power of two and in range [512 - 65536].\n\nMemory Impact:\nWQE buffer size = strides_num × stride_size.\nWith default values (2048 × 64 bytes), each WQE uses 128 KB.\nTotal RX memory ≈ strides_num × stride_size × rx_num_wr.\n\nLower values (e.g., 512):\n• Smaller WQE buffers reduce memory footprint per ring.\n• Faster WQE buffer recycling since fewer packets need to complete.\n• Fewer packets per WQE means more frequent WQE replenishment,\n  which may become a bottleneck under very high packet rates.\n\nHigher values (e.g., 65536):\n• More packets can be received per WQE, reducing WQE management overhead.\n• Better for high packet-rate workloads (millions of packets/sec).\n• Larger WQE buffers increase memory consumption.\n• WQE buffer is held until all strides are consumed, potentially\n  increasing memory pressure under bursty traffic.\n\nConstraint: strides_num × rx_num_wr must not exceed 4,194,304 (CQE limit).\nIf exceeded, rx_num_wr is automatically reduced.",
                            "x-power-of-2-or-zero": true
                        },
                        "stride_size": {
                            "type": "integer",
                            "default": 64,
                            "minimum": 64,
                            "maximum": 8192,
                            "title": "Size of each stride (bytes)",
                            "description": "Maps to XLIO_STRQ_STRIDE_SIZE_BYTES environment variable.\nThe size, in bytes, of each stride in a receive WQE.\nMust be power of two and in range [64 - 8192].\n\nPackets larger than stride_size span multiple strides.\nFor example, a 1500-byte packet with stride_size=64 uses 24 strides.\n\nLower values (e.g., 64 - default):\n• Optimal memory efficiency for small packets (typical UDP, TCP ACKs).\n• Minimal internal fragmentation - less wasted space per packet.\n• More packets fit within each WQE.\n• Large packets (jumbo frames, LRO segments) consume many strides,\n  increasing metadata overhead per packet.\n\nHigher values (e.g., 2048-8192):\n• Better suited for jumbo frames (MTU 9000) or LRO-aggregated segments.\n• Fewer strides per large packet reduces per-packet overhead.\n• Fewer stride descriptor objects are allocated.\n• Significant memory waste when receiving small packets.\n  A 64-byte packet in an 8192-byte stride wastes 99% of the stride.\n\nRecommendations:\n• For typical mixed traffic with MTU 1500: use default (64 bytes).\n• For jumbo frames (MTU 9000): consider 256-512 bytes.\n• For NVMe-oF or storage workloads with large transfers: consider 2048-8192 bytes.\n\nNote: strides_num × stride_size must be at least MTU + 18 bytes (Ethernet + VLAN headers).",
                            "x-power-of-2-or-zero": true
                        }
                    },
                    "additionalProperties": false
                },
                "tcp": {
                    "type": "object",
                    "description": "TCP hardware offloads settings.",
                    "properties": {
                        "lro": {
                            "oneOf": [
                                {
                                    "type": "integer",
                                    "enum": [
                                        -1,
                                        0,
                                        1
                                    ],
                                    "default": -1
                                },
                                {
                                    "type": "string",
                                    "enum": [
                                        "auto",
                                        "disable",
                                        "enable"
                                    ],
                                    "default": "auto"
                                }
                            ],
                            "title": "Large Receive Offload policy",
                            "description": "Maps to XLIO_LRO environment variable.\nLarge Receive Offload (LRO) is a hardware technique for increasing inbound TCP throughput by reducing CPU overhead. The NIC aggregates multiple incoming TCP segments from the same stream into a single larger buffer before delivering it to the application, reducing the number of packets that must be processed.\n\nValues:\n   - \"auto\" or -1 (default)\n      Automatically enabled based on ethtool setting and adapter capability.\n      Check with: ethtool -k <interface> | grep large-receive-offload\n   - \"disable\" or 0\n      LRO is disabled. Each TCP segment is delivered individually.\n   - \"enable\" or 1\n      Force LRO enabled if the adapter supports it.\n\nPerformance Tradeoffs:\n\nEnabled (higher throughput, lower CPU):\n• Multiple TCP segments are coalesced into larger chunks (up to 64KB).\n• Dramatically reduces per-packet CPU overhead for bulk data transfers.\n• Ideal for high-bandwidth workloads: file transfers, backup, streaming, NVMe-oF.\n• Can achieve near line-rate throughput with lower CPU utilization.\n• Coalesced segments span multiple strides when using Striding RQ.\n• Memory efficiency: fewer buffer descriptors and less metadata per byte transferred.\n\nDisabled (lower latency, per-packet processing):\n• Each TCP segment is delivered immediately without waiting for aggregation.\n• Lower latency - critical for latency-sensitive applications.\n• Required when applications need per-packet timestamps or ordering guarantees.\n• Better for interactive protocols, trading, or real-time communication.\n• Higher CPU overhead at high packet rates (millions of pps).\n• Each packet consumes at least one stride in Striding RQ mode.\n\nInteraction with Striding RQ:\n• LRO max payload size = min(strides_num × stride_size, 64KB), rounded to 256 bytes.\n• With defaults (2048 × 64 = 128KB), LRO can coalesce up to 64KB segments.\n• Lower stride configurations may limit LRO aggregation size.\n• LRO-aggregated packets consume multiple strides proportional to their size.\n• For maximum LRO benefit, ensure strides_num × stride_size >= 64KB.\n\nRecommendations:\n• Bulk data workloads (file/object storage, backup): enable LRO.\n• Latency-sensitive workloads (trading, real-time): disable LRO.\n• Mixed workloads: use auto and let the system decide.\n• NVMe-oF with large I/O: enable LRO with larger stride_size (2048+)."
                        },
                        "tso": {
                            "type": "object",
                            "description": "TCP Segmentation Offload settings.",
                            "properties": {
                                "enable": {
                                    "oneOf": [
                                        {
                                            "type": "integer",
                                            "enum": [
                                                -1,
                                                0,
                                                1
                                            ],
                                            "default": -1
                                        },
                                        {
                                            "type": "string",
                                            "enum": [
                                                "auto",
                                                "disable",
                                                "enable"
                                            ],
                                            "default": "auto"
                                        }
                                    ],
                                    "title": "TCP segmentation offload policy",
                                    "description": "Maps to XLIO_TSO environment variable.\nTCP Segmentation Offload (TSO), also known as Large Send Offload (LSO),\nallows the TCP stack to pass buffers larger than the MTU to the network\nadapter. The NIC hardware then segments these large buffers into\nMTU-sized packets, calculating TCP sequence numbers, checksums, and\nheaders for each segment.\n\nHOW IT WORKS:\nWithout TSO, the CPU must create individual TCP segments (typically\n1460 bytes each for standard Ethernet). With TSO enabled, the application\ncan submit much larger buffers (up to hardware_features.tcp.tso.max_size),\nand the NIC handles segmentation in hardware.\n\nVALUE OPTIONS:\n   - \"auto\" or -1 (default):\n      Automatically enables TSO based on:\n      1. Adapter hardware capability (queried via device attributes)\n      2. ethtool settings: ethtool -k <interface> | grep tcp-segmentation-offload\n      Recommended for most deployments.\n   - \"disable\" or 0:\n      Forces TSO off regardless of hardware capability.\n   - \"enable\" or 1:\n      Forces TSO on if the adapter supports it.\n      Fails gracefully if hardware lacks TSO capability.\n\nPERFORMANCE BENEFITS OF TSO (ENABLED):\n   + Dramatically higher throughput - can achieve line-rate on 100Gbps+\n     links by reducing per-packet processing overhead.\n   + Significant CPU offload - segmentation work moves from CPU to NIC,\n     freeing CPU cycles for application processing.\n   + Fewer system calls per byte transferred - one send() can transmit\n     hundreds of KB instead of individual 1460-byte segments.\n   + Reduced memory bandwidth - fewer packet headers to construct.\n   + Better cache utilization - less per-packet metadata.\n   + Lower interrupt rate - fewer completion events per byte transferred.\n\nWHEN TO DISABLE TSO:\n   - Debugging network issues (TSO can mask certain problems).\n   - When using network monitoring tools that need to see actual packets.\n   - Extremely latency-sensitive applications where even tiny buffering\n     adds unacceptable delay (though this is rare - TSO latency impact\n     is typically negligible).\n   - Compatibility with certain virtualization or tunneling setups that\n     don't properly handle TSO packets.\n\nCRITICAL INTERACTION - CONGESTION WINDOW:\nTSO segment aggregation is limited by the TCP congestion window (cwnd):\n   - TSO max_payload = MIN(hardware_features.tcp.tso.max_size,\n                           available_window_space)\n   - If cwnd is small (e.g., during slow start), TSO cannot aggregate\n     large segments regardless of max_size setting.\n   - This is normal TCP behavior - TSO respects congestion control.\n\nRELATED PARAMETERS:\n   - hardware_features.tcp.tso.max_size: Caps aggregated segment size.\n   - network.protocols.tcp.wmem: TCP send buffer affects TSO efficiency.\n   - network.protocols.tcp.mss: Segment granularity for TSO aggregation.\n\nRECOMMENDATIONS:\n   - High-throughput bulk transfers: Enable (default \"auto\" is fine).\n   - Data center / cloud workloads: Enable for efficiency.\n   - Most production deployments: Use \"auto\" (default).\n   - Network debugging: Temporarily disable.\n\nVERIFICATION:\nCheck if TSO is active: ethtool -k <interface> | grep tcp-segmentation-offload\nXLIO logs TSO status at startup: \"ring attributes: m_tso = ...\""
                                },
                                "max_size": {
                                    "oneOf": [
                                        {
                                            "type": "integer",
                                            "default": 262144,
                                            "minimum": 1
                                        },
                                        {
                                            "type": "string",
                                            "default": "256KB",
                                            "pattern": "^[0-9]+[KMGkmg]?[B]?$"
                                        }
                                    ],
                                    "title": "Maximum TSO size",
                                    "description": "Maps to XLIO_MAX_TSO_SIZE environment variable.\nMaximum size in bytes of data that can be aggregated into a single TSO\n\"super-packet\" before being handed to the NIC for hardware segmentation.\n\nHOW MAX_SIZE AFFECTS TSO:\nWhen the application sends data, XLIO aggregates multiple TCP segments\ninto a single large buffer (up to max_size bytes) and submits it to the\nNIC as one work request. The NIC then segments this into individual\nMTU-sized packets for transmission.\n\nEffective TSO payload = MIN(max_size, hardware_capability, congestion_window)\n\nCOMMON VALUES:\n   - 64KB (65536):   Conservative, lower memory per connection.\n   - 256KB (262144): Default, balanced throughput and resource usage.\n   - 512KB-1MB:      Aggressive, maximum throughput optimization.\n\nHIGH VALUES (512KB - 1MB) - THROUGHPUT-OPTIMIZED:\n   + Maximum throughput potential - fewer NIC work requests per byte,\n     achieving line-rate on 100Gbps+ links more easily.\n   + Lowest CPU overhead per byte - amortizes send() syscall and\n     work request posting costs over more data.\n   + Fewest interrupts - one completion event for larger data chunks.\n   + Best efficiency for bulk/streaming workloads (file transfer,\n     backup, video streaming, database replication).\n   - Higher per-connection memory consumption - larger aggregation\n     buffers are allocated.\n   - Increased head-of-line blocking potential - if one large TSO\n     segment is delayed, all data within it waits.\n   - May interact poorly with very small congestion windows during\n     slow start phase (TSO aggregation capped by cwnd).\n   - Bursty traffic pattern - sending large chunks can cause\n     micro-bursts that may trigger switch buffer drops.\n\nLOW VALUES (32KB - 64KB) - LATENCY-OPTIMIZED:\n   + Lower per-connection memory footprint - important for servers\n     with thousands of concurrent connections.\n   + More granular congestion control response - smaller chunks\n     react faster to network feedback.\n   + Reduced head-of-line blocking - data sent in smaller units.\n   + Smoother traffic pattern - less bursty, potentially better for\n     shared network environments.\n   + Faster first-byte latency for new data - less buffering before\n     transmission.\n   - Lower peak throughput - more NIC work requests needed per byte.\n   - Higher CPU overhead - more frequent send operations.\n   - May not fully utilize available bandwidth on very high-speed\n     links (100Gbps+) where aggregation efficiency matters most.\n\nVERY LOW VALUES (<32KB) - SPECIAL CASES:\n   - Generally not recommended for bulk transfers.\n   - May be useful for latency-critical applications with small messages.\n   - Significantly increases CPU overhead for high-throughput workloads.\n   - Consider whether TSO provides benefit at all for such small sizes.\n\nHARDWARE CONSTRAINTS:\nThe actual TSO payload size is capped by the NIC's hardware capability:\n   - Queried from device attributes (xlio_ibv_tso_caps.max_tso).\n   - If max_size exceeds hardware capability, the hardware limit wins.\n   - XLIO logs a warning if hardware cap exceeds default 256KB,\n     suggesting you increase max_size to utilize full NIC potential.\n\nCONGESTION WINDOW INTERACTION:\nTSO segment aggregation is dynamically limited by TCP congestion control:\n   - Actual aggregation = MIN(max_size, available_cwnd_space)\n   - During slow start (small cwnd), TSO may not reach max_size.\n   - This is correct behavior - TSO respects congestion control.\n   - As cwnd grows, TSO aggregation automatically increases.\n\nINTERACTION WITH OTHER PARAMETERS:\n   - network.protocols.tcp.wmem: Send buffer should be >= max_size\n     for optimal TSO efficiency. If wmem < max_size, TSO is limited\n     by available buffer space.\n   - network.protocols.tcp.mss: TSO aggregates MSS-sized segments.\n     Higher MSS = fewer segments per TSO super-packet.\n   - performance.buffers.tx_buf_size: Per-buffer memory allocation;\n     TSO buffer size is MIN(tx_buf_size, max_size).\n\nMEMORY IMPACT:\nLarger max_size increases memory usage approximately proportional to:\n   - Number of concurrent TCP connections\n   - Amount of data in flight per connection\n   - Send buffer (wmem) sizing\n\nFor high-connection-count servers (10K+ connections), consider lower\nmax_size (64KB-128KB) to reduce total memory footprint.\n\nRECOMMENDATIONS:\n   - Bulk transfer / streaming (few connections, max throughput):\n     512KB - 1MB\n   - Balanced workloads (moderate connections, good throughput):\n     256KB (default)\n   - High connection count servers (10K+ connections):\n     64KB - 128KB\n   - Latency-sensitive applications: 32KB - 64KB\n   - Memory-constrained environments: 64KB\n\nTUNING APPROACH:\n1. Start with default 256KB.\n2. Monitor throughput with iperf3 or application benchmarks.\n3. If throughput is below line-rate, try increasing to 512KB or 1MB.\n4. If memory is constrained, reduce to 128KB or 64KB.\n5. Check XLIO startup logs for hardware capability warnings.\n\nSupports suffixes: B, KB, MB (e.g., \"512KB\", \"1MB\").",
                                    "x-memory-size": true
                                }
                            },
                            "additionalProperties": false
                        },
                        "tls_offload": {
                            "type": "object",
                            "description": "TLS offload settings for TCP connections.",
                            "properties": {
                                "tx_enable": {
                                    "type": "boolean",
                                    "default": true,
                                    "title": "Enable TLS TX offload",
                                    "description": "Maps to XLIO_UTLS_TX environment variable.\nControls whether TLS encryption is offloaded to the NIC hardware or performed by the CPU.\n\nHOW IT WORKS:\nWhen enabled, XLIO leverages the Linux kTLS API to offload TLS encryption to\nNVIDIA ConnectX NICs. The NIC's dedicated crypto engine performs AES-GCM\nencryption directly on outgoing packets, freeing the CPU from encryption work.\nXLIO manages hardware resources including TIS (Transport Interface Send)\ncontexts and DEK (Data Encryption Key) caches on the NIC.\n\nENABLED (true) - HARDWARE TLS OFFLOAD:\n   + Lower CPU utilization - NIC crypto engine handles encryption.\n   + Higher throughput ceiling - dedicated hardware accelerator.\n   + Lower latency - fewer CPU cycles in the data path.\n   + Better scalability - CPU freed for application logic.\n   + Zero-copy friendly - encrypted data sent directly without extra copies.\n   - Requires compatible NIC (ConnectX-6 DX or later with TLS offload).\n   - Requires TLS library with kTLS support (OpenSSL 3.0+, or patched versions).\n   - Consumes NIC resources (DEK cache entries, TIS contexts).\n   - Resync overhead when TCP retransmissions occur (NIC must replay state).\n\nDISABLED (false) - SOFTWARE TLS ENCRYPTION:\n   + Works on any hardware without TLS offload capability.\n   + No NIC resource consumption (DEK cache, TIS contexts freed).\n   + Simpler debugging - all encryption visible in CPU profiling.\n   + Useful when NIC resources are scarce with many TLS connections.\n   - Higher CPU utilization - CPU performs all AES-GCM encryption.\n   - Lower throughput ceiling - CPU becomes the encryption bottleneck.\n   - Higher latency - additional CPU cycles per packet.\n   - More memory bandwidth - data copied through CPU for encryption.\n\nPERFORMANCE IMPACT:\n   - With hardware offload: Expect 2-5x lower CPU usage for TLS workloads\n     and higher sustainable throughput (100Gbps+ achievable).\n   - Without hardware offload: CPU encryption at ~10-20 Gbps per core\n     (cipher-dependent), limiting total throughput.\n\nWHEN TO DISABLE:\n   - Hardware doesn't support TLS offload (pre-ConnectX-6 DX NICs).\n   - TLS library lacks kTLS support.\n   - Debugging TLS-related issues (software path is easier to trace).\n   - Very high connection count exhausting NIC DEK cache.\n\nPREREQUISITES:\n   - NVIDIA ConnectX-6 DX, ConnectX-7, or BlueField-2+ with TLS offload.\n   - TLS library compiled with kTLS support.\n   - Application must use kTLS-compatible TLS API (setsockopt SOL_TLS).\n   - XLIO built with --enable-utls configure option.\n\nINTERACTION WITH OTHER PARAMETERS:\n   - hardware_features.tcp.tls_offload.dek_cache_max_size: Controls DEK cache\n     size. Increase for high connection counts with frequent key rotations.\n   - hardware_features.tcp.tls_offload.rx_enable: Independent control for\n     receive-side TLS offload.\n\nDEFAULT RECOMMENDATION:\nKeep enabled (true) for best performance on supported hardware. XLIO\nautomatically falls back to software encryption if hardware is unavailable."
                                },
                                "rx_enable": {
                                    "type": "boolean",
                                    "default": false,
                                    "title": "Enable TLS RX offload",
                                    "description": "Maps to XLIO_UTLS_RX environment variable.\nControls whether TLS decryption is offloaded to the NIC hardware or performed by the CPU.\n\nHOW IT WORKS:\nWhen enabled, XLIO leverages the Linux kTLS API to offload TLS decryption to\nNVIDIA ConnectX NICs. The NIC's crypto engine performs AES-GCM decryption\ndirectly on incoming packets. XLIO manages TIR (Transport Interface Receive)\ncontexts and handles resync scenarios when packets arrive out of order.\n\nIMPORTANT: RX offload is more complex than TX due to packet reordering.\nThe NIC must handle out-of-order packets which may require software fallback\nfor authentication/decryption of certain records.\n\nENABLED (true) - HARDWARE TLS RX OFFLOAD:\n   + Lower CPU utilization - NIC crypto engine handles decryption.\n   + Higher receive throughput ceiling (under ideal conditions).\n   + Lower receive latency - fewer CPU cycles in the data path.\n   + Better scalability for receive-heavy TLS workloads (web servers).\n   - Requires compatible NIC (ConnectX-6 DX or later with TLS RX offload).\n   - Requires TLS library with kTLS RX support.\n   - Consumes NIC resources (TIR contexts, DEK entries).\n   - Resync overhead can degrade throughput under load (see below).\n   - More complex failure modes than TX offload.\n\nDISABLED (false) - SOFTWARE TLS DECRYPTION:\n   + Works on any hardware without TLS offload capability.\n   + Simpler packet handling - no hardware state to manage.\n   + Easier debugging - all decryption visible in CPU profiling.\n   + No NIC resource consumption for RX TLS.\n   + More predictable throughput under high load or packet loss.\n   + No resync contention with high connection counts.\n   - Higher CPU utilization - CPU performs all AES-GCM decryption.\n   - Lower receive throughput ceiling.\n   - Higher receive latency.\n\nCRITICAL: RESYNC OVERHEAD AND THROUGHPUT DEGRADATION\n\nRX TLS offload can experience significant throughput degradation under\ncertain conditions due to expensive resync operations:\n\n1. WHAT TRIGGERS RESYNC:\n   - Out-of-order packet delivery\n   - TCP retransmissions\n   - Packet loss and recovery\n   - Network congestion causing reordering\n\n2. WHY RESYNC IS EXPENSIVE:\n   - Resync requires Send Queue (SQ) credits from the TX path\n   - Each resync posts UMR WQEs to reprogram NIC TLS state\n   - Resyncs compete with TX traffic for SQ resources\n\n3. UNDER HIGH LOAD:\n   - When SQ is congested, resyncs are SKIPPED and retried later\n   - Skipped resyncs cause packets to fall back to software decryption\n   - Software fallback (OpenSSL EVP_Decrypt) is CPU-intensive\n   - This creates a negative feedback loop under load\n\n4. HIGH CONNECTION COUNT AMPLIFIES THE PROBLEM:\n   - Each connection can independently trigger resyncs\n   - All connections share limited SQ credits\n   - More connections = more resync contention = more SW fallback\n   - Throughput can degrade significantly vs software-only path\n\nRESYNC SCENARIOS (tracked in statistics):\n   - full_enc: Entire record encrypted, needs full SW decrypt\n   - head_enc: First buffers encrypted, partial SW decrypt\n   - tail_enc: Tail encrypted, needs SW re-encrypt + decrypt\n   - mix_enc: Mixed state, most expensive SW processing\n\nPERFORMANCE IMPACT:\n   - IDEAL CONDITIONS (low loss, few connections, light TX load):\n     Significant CPU savings, higher throughput.\n   - HIGH LOAD / MANY CONNECTIONS / PACKET LOSS:\n     May perform WORSE than software decryption due to resync overhead\n     and SQ contention. Throughput can drop 20-50% in extreme cases.\n\nWHY DEFAULT IS FALSE:\nRX offload is disabled by default because:\n   1. Resync overhead can cause unexpected throughput degradation.\n   2. Performance depends heavily on network conditions and load.\n   3. TX offload alone provides significant benefit for most servers.\n   4. Software decryption has more predictable performance.\n   5. Some TLS libraries have limited kTLS RX support.\n\nWHEN TO ENABLE:\n   - Low packet loss, stable network environment.\n   - Moderate connection count (not thousands of connections).\n   - High-throughput receive workloads with large payloads.\n   - TX path has spare SQ capacity (not heavily loaded).\n   - Testing confirms benefit for your specific workload.\n\nWHEN TO KEEP DISABLED:\n   - High connection count (1000+ TLS connections).\n   - Lossy network with frequent retransmissions.\n   - Heavy bidirectional traffic (TX competes for SQ credits).\n   - Workload shows high resync counts in XLIO statistics.\n   - Throughput testing shows degradation vs software path.\n\nMONITORING:\nUse XLIO statistics to monitor RX TLS health:\n   - n_tls_rx_resync: Total resync operations (should be low)\n   - n_tls_rx_records_full_enc: Records fully SW-decrypted\n   - n_tls_rx_records_*_enc: Partial SW decryption events\n   - n_rx_tls_auth_fail: Authentication failures\nHigh values indicate resync pressure; consider disabling RX offload.\n\nPREREQUISITES:\n   - NVIDIA ConnectX-6 DX, ConnectX-7, or BlueField-2+ with TLS RX offload.\n   - TLS library compiled with kTLS RX support.\n   - Application must use kTLS-compatible TLS API.\n   - XLIO built with --enable-utls configure option.\n\nINTERACTION WITH OTHER PARAMETERS:\n   - hardware_features.tcp.tls_offload.tx_enable: TX and RX offload share\n     SQ resources. Heavy TX load reduces RX resync capacity.\n   - hardware_features.tcp.tls_offload.dek_cache_max_size: Shared DEK cache\n     for both TX and RX operations.\n\nRECOMMENDATION:\nStart with TX offload only (default). Enable RX offload only after:\n   1. Confirming low packet loss in your environment.\n   2. Testing with realistic connection counts.\n   3. Monitoring resync statistics under production load.\n   4. Verifying throughput improvement vs software path."
                                },
                                "dek_cache_max_size": {
                                    "type": "integer",
                                    "default": 1024,
                                    "title": "DEK max cache size",
                                    "description": "Maps to XLIO_HIGH_WMARK_DEK_CACHE_SIZE environment variable.\nMaximum number of Data Encryption Key (DEK) objects to cache for TLS offload.\n\nHOW DEK CACHING WORKS:\nDEK objects contain TLS encryption keys loaded into the NIC's crypto engine.\nCreating new DEKs requires NIC firmware operations which have latency cost.\nXLIO maintains a cache of reusable DEK objects to amortize this cost.\n\nWhen a TLS connection closes, its DEK is returned to the 'put cache'.\nWhen a new TLS connection needs a DEK, XLIO first tries the 'get cache'.\nIf the get cache is empty, a crypto-sync operation swaps the caches.\nThis high watermark limits the maximum put cache size.\n\nHIGH VALUES (2048+):\n   + Better performance with many short-lived TLS connections.\n   + Fewer crypto-sync operations (less NIC firmware latency).\n   + Smoother latency distribution for connection setup.\n   - Higher NIC memory consumption for DEK storage.\n   - May exhaust system DEK resources with very high values.\n\nLOW VALUES (256-512):\n   + Lower NIC resource consumption.\n   + Suitable for long-lived connections (fewer DEK recycles).\n   - More frequent crypto-sync operations with many connections.\n   - Higher latency variance for connection setup.\n\nRECOMMENDATIONS:\n   - High connection churn (HTTP/1.1, many short requests): 2048-4096\n   - Moderate connection count with reuse (HTTP/2, gRPC): 1024 (default)\n   - Few long-lived connections: 256-512\n   - Very high connection count (10K+): Consider 4096+\n\nNOTE: This parameter only affects TLS hardware offload. Has no effect\nwhen tx_enable and rx_enable are both false."
                                },
                                "dek_cache_min_size": {
                                    "type": "integer",
                                    "default": 512,
                                    "title": "DEK min cache size",
                                    "description": "Maps to XLIO_LOW_WMARK_DEK_CACHE_SIZE environment variable.\nMinimum DEK cache threshold before creating new DEK objects.\n\nHOW IT WORKS:\nWhen the available DEK cache (get cache) is empty and the put cache size\nis below this low watermark, XLIO creates new DEK objects instead of\nperforming a crypto-sync operation to recycle existing DEKs.\n\nThis avoids throttling behavior where a single DEK is returned and\nimmediately fetched, causing frequent crypto-sync operations.\n\nHIGH VALUES (relative to max_size):\n   + More aggressive new DEK creation.\n   + Fewer crypto-sync operations.\n   + Better latency for connection setup bursts.\n   - More total DEK objects created over time.\n\nLOW VALUES:\n   + More DEK reuse via crypto-sync.\n   + Lower total DEK object count.\n   - More frequent crypto-sync operations.\n   - Higher latency variance during cache transitions.\n\nCONSTRAINT:\nMust be less than dek_cache_max_size. If set >= max_size, XLIO\nautomatically adjusts to max_size / 2.\n\nRECOMMENDATION:\nKeep at ~50% of dek_cache_max_size (default ratio). Adjust max_size\nfirst; min_size rarely needs manual tuning."
                                }
                            },
                            "additionalProperties": false
                        }
                    },
                    "additionalProperties": false
                }
            },
            "additionalProperties": false
        },
        "performance": {
            "type": "object",
            "title": "Performance Optimization",
            "description": "Settings that affect XLIO performance characteristics",
            "properties": {
                "steering_rules": {
                    "type": "object",
                    "description": "Steering rules settings.",
                    "properties": {
                        "tcp": {
                            "type": "object",
                            "description": "Steering TCP rules settings.",
                            "properties": {
                                "2t_rules": {
                                    "type": "boolean",
                                    "default": false,
                                    "title": "Enable 2-tuple rules",
                                    "description": "Maps to XLIO_TCP_2T_RULES environment variable.\nControls hardware flow steering rule granularity for TCP connections.\n\n**What it does:**\n- When false (default): Uses 5-tuple rules matching (protocol + local_ip + local_port + remote_ip + remote_port). Each TCP connection gets its own dedicated hardware steering rule.\n- When true: Uses 2-tuple rules matching only (protocol + local_ip). All TCP connections sharing the same local IP address share a single hardware steering rule.\n\n**When to enable (true):**\n- Applications with many outgoing TCP connections (clients calling connect()) where ephemeral source ports would exhaust hardware flow table.\n- Scenarios where hardware flow steering table size is a bottleneck.\n- High connection-count workloads where hardware rule scalability matters more than per-packet efficiency.\n\n**Performance implications when enabled:**\n- BENEFIT: Dramatically reduces hardware flow steering table usage. One rule covers unlimited connections per local IP.\n- BENEFIT: Overcomes adapter steering table limitations for high connection counts.\n- COST: Flow tags are disabled, removing hardware-assisted fast-path packet identification.\n- COST: Requires software demultiplexing to route packets to correct sockets, increasing CPU overhead.\n- COST: Requires unique local IP address per XLIO ring to ensure correct packet steering.\n\n**When to keep disabled (false, default):**\n- Server workloads using listen()/accept() - consider 3t_rules instead.\n- Low-to-moderate connection counts where hardware table is not exhausted.\n- Latency-sensitive applications benefiting from flow tag optimization.\n- Configurations where unique IP per thread is not feasible.\n\n**Requirements when enabled:**\nEach XLIO ring must bind sockets to a unique local IP address. In the default ring-per-thread configuration, each thread must use a thread-local IP address for its sockets."
                                },
                                "3t_rules": {
                                    "type": "boolean",
                                    "default": false,
                                    "title": "Enable 3-tuple rules",
                                    "description": "Maps to XLIO_TCP_3T_RULES environment variable.\nControls hardware flow steering rule granularity for incoming (accepted) TCP connections on server sockets.\n\n**What it does:**\n- When false (default): Uses 5-tuple rules matching (protocol + local_ip + local_port + remote_ip + remote_port). Each accepted TCP connection gets its own dedicated hardware steering rule with flow tag support.\n- When true: Uses 3-tuple rules matching only (protocol + local_ip + local_port). All accepted connections on the same listening socket share a single hardware steering rule.\n\n**Scope:** Only affects incoming TCP connections received via listen()/accept(). Outgoing connections established via connect() are not affected by this option.\n\n**When to enable (true):**\n- High-connection-count servers accepting thousands of concurrent TCP connections.\n- Servers where hardware flow table exhaustion is a concern.\n- Workloads prioritizing connection scalability over per-packet efficiency.\n- Applications where accept()/close() latency matters more than packet processing latency.\n\n**Performance implications when enabled:**\n- BENEFIT: Dramatically reduces hardware flow steering table usage. One rule handles unlimited accepted connections per listening socket.\n- BENEFIT: Faster accept() - no hardware rule creation overhead per new connection.\n- BENEFIT: Faster connection teardown - no hardware rule deletion per closed connection.\n- BENEFIT: Overcomes adapter flow table size limitations for servers with many clients.\n- COST: Flow tags are disabled. Hardware cannot identify which socket a packet belongs to.\n- COST: Software demultiplexing required. XLIO must perform a hash lookup to route each packet to the correct socket, increasing CPU cycles per packet.\n- COST: Slightly higher packet processing latency due to software lookup overhead.\n\n**When to keep disabled (false, default):**\n- Low-to-moderate connection count servers where hardware table is not exhausted.\n- Ultra-low-latency applications where per-packet CPU overhead is critical.\n- Servers benefiting from hardware flow tag optimization.\n- Workloads where connection rate is low but packet rate per connection is high.\n\n**Comparison with 2t_rules:**\n- 3t_rules: For servers (incoming connections via accept()). Shares rule per listening port.\n- 2t_rules: For clients (outgoing connections via connect()). Shares rule per local IP address. Requires unique IP per ring."
                                }
                            },
                            "additionalProperties": false
                        },
                        "udp": {
                            "type": "object",
                            "description": "UDP rule settings.",
                            "properties": {
                                "3t_rules": {
                                    "type": "boolean",
                                    "default": true,
                                    "title": "Enable 3-tuple rules",
                                    "description": "Maps to XLIO_UDP_3T_RULES environment variable.\nControls hardware flow steering rule granularity for connected UDP sockets.\n\n**What it does:**\n- When true (default): Uses 3-tuple rules matching only (protocol + local_ip + local_port). Remote IP/port are wildcarded. All connected UDP sockets bound to the same local IP:port share a single hardware steering rule.\n- When false: Uses 5-tuple rules matching (protocol + local_ip + local_port + remote_ip + remote_port). Each connected UDP socket gets its own dedicated hardware steering rule with flow tag support.\n\n**Scope:** This parameter is relevant for connected UDP sockets (sockets on which connect() was called). For non-connected UDP sockets, 3-tuple rules are always used regardless of this setting.\n\n**When to keep enabled (true, default):**\n- Applications with many UDP \"connections\" to different remote peers (e.g., game servers, video streaming servers).\n- Scenarios where hardware flow table exhaustion is a concern.\n- Workloads where connection setup/teardown rate matters more than per-packet efficiency.\n- Applications that do not require ultra-low-latency packet processing.\n\n**Performance implications when enabled (true):**\n- BENEFIT: Dramatically reduces hardware flow steering table usage. One rule can handle unlimited connected UDP sockets per local IP:port.\n- BENEFIT: Faster connect() - no hardware rule creation overhead per new connection.\n- BENEFIT: Faster socket teardown - no hardware rule deletion per closed connection.\n- BENEFIT: Overcomes adapter flow table size limitations for applications with many UDP peers.\n- COST: Flow tags are disabled. Hardware cannot identify which socket a packet belongs to.\n- COST: Software demultiplexing required. XLIO must perform a hash table lookup to route each incoming packet to the correct socket, increasing CPU cycles per packet.\n- COST: Slightly higher packet processing latency due to software lookup overhead.\n\n**When to disable (false):**\n- Low-to-moderate connection count applications where hardware table is not exhausted.\n- Ultra-low-latency applications (e.g., trading systems, real-time control) where per-packet CPU overhead is critical.\n- Applications with few UDP \"connections\" but very high packet rates per connection.\n- Workloads benefiting from hardware flow tag optimization for direct socket delivery.\n\n**Performance implications when disabled (false):**\n- BENEFIT: Flow tags work - hardware identifies the exact socket directly (fast path).\n- BENEFIT: Lower per-packet latency and fewer CPU cycles per packet.\n- BENEFIT: No software demultiplexing overhead - packets are delivered directly to the target socket.\n- COST: Each connected UDP socket consumes a hardware steering rule entry.\n- COST: Risk of exhausting hardware flow table with many concurrent connections.\n- COST: Slower connect/disconnect as each requires hardware rule creation/deletion operations.\n\n**Comparison with TCP steering rules:**\n- UDP 3t_rules: Affects connected UDP sockets. Default is true (3-tuple) to conserve resources.\n- TCP 3t_rules: Affects accepted TCP connections on server sockets. Default is false (5-tuple) for optimal latency.\n- TCP 2t_rules: For outgoing TCP connections. Requires unique IP per ring."
                                },
                                "only_mc_l2_rules": {
                                    "type": "boolean",
                                    "default": false,
                                    "title": "Use only L2 rules for multicast",
                                    "description": "Maps to XLIO_ETH_MC_L2_ONLY_RULES environment variable.\nControls hardware flow steering rule granularity for UDP multicast traffic.\n\n**What it does:**\n- When false (default): Creates specific hardware rules per (multicast_ip, port) pair.\n  Hardware matches on: Destination MAC + Destination IP + Destination Port + Protocol (UDP).\n  Each socket joining a multicast group on a different port gets its own dedicated hardware rule.\n- When true: Creates broad L2-only hardware rules per multicast MAC address.\n  Hardware matches on: Destination MAC + Protocol (UDP) only.\n  Multiple sockets on different ports sharing the same multicast IP share ONE hardware rule.\n  XLIO performs software filtering by port after packets arrive.\n\n**Technical detail:**\nMulticast MAC addresses are derived from multicast IP addresses (e.g., 239.1.1.1 maps to\n01:00:5e:01:01:01). When L2-only rules are enabled, all traffic to that derived MAC arrives\nat XLIO, regardless of port. Software then demultiplexes packets to the correct socket.\n\n**When to keep disabled (false, default):**\n- Applications with few multicast group/port combinations.\n- Ultra-low-latency multicast receivers where every microsecond counts.\n- High packet rate scenarios where CPU cycles per packet matter.\n- Environments with adequate hardware flow steering table capacity.\n\n**Performance implications when disabled (false):**\n- BENEFIT: Lower per-packet latency - hardware delivers packets directly to correct flow.\n- BENEFIT: Lower CPU overhead - no software port filtering required.\n- BENEFIT: Optimal for small-scale multicast (few groups, few ports per group).\n- COST: Each (multicast_ip, port) combination consumes one hardware steering rule entry.\n- COST: Risk of exhausting hardware flow steering table with many multicast subscriptions.\n- COST: More hardware rule create/delete operations when joining/leaving groups.\n\n**When to enable (true):**\n- Applications joining many ports on the same multicast group (e.g., 239.1.1.1:5000, 239.1.1.1:5001, ..., 239.1.1.1:5100).\n- Market data feed handlers receiving many instruments on different ports of the same multicast address.\n- Environments with limited hardware flow steering resources.\n- Applications hitting flow steering table capacity limits.\n- Multicast-heavy applications where conserving steering rules is critical.\n\n**Performance implications when enabled (true):**\n- BENEFIT: Dramatically reduces hardware flow steering resource usage.\n  One rule handles unlimited ports per multicast IP.\n- BENEFIT: Prevents flow table exhaustion for multicast-heavy workloads.\n- BENEFIT: Faster multicast group join/leave - fewer hardware rule operations.\n- COST: Higher CPU overhead per packet - software must filter by destination port.\n- COST: Increased latency due to additional software lookup.\n- COST: All multicast loopback traffic is handled by XLIO instead of OS\n  (relevant when IP_MULTICAST_LOOP is enabled).\n- COST: Packets for ports not subscribed (but same multicast IP) still arrive at XLIO\n  and must be filtered out in software.\n\n**Hardware resource context:**\nNetwork adapters have finite flow steering table capacity (varies by adapter model and\nfirmware). Each specific rule (5-tuple or 4-tuple) consumes one entry. Applications\nsubscribing to hundreds or thousands of multicast group/port combinations can exhaust\nthis capacity, causing steering rule creation failures.\n\n**Example scenario:**\n- Market data application subscribes to 500 instruments, each on a unique port of\n  multicast group 239.1.1.1.\n- With only_mc_l2_rules=false: 500 hardware steering rules required.\n- With only_mc_l2_rules=true: 1 hardware steering rule (matching MAC 01:00:5e:01:01:01).\n  Software filters packets to correct socket by port.\n\n**Latency vs resource tradeoff:**\n- false = Lower latency, higher resource usage. Best for latency-critical, low-scale multicast.\n- true = Higher latency, lower resource usage. Best for resource-constrained, high-scale multicast."
                                }
                            },
                            "additionalProperties": false
                        },
                        "disable_flowtag": {
                            "type": "boolean",
                            "default": false,
                            "title": "Disable hardware flow tag",
                            "description": "Maps to XLIO_DISABLE_FLOW_TAG environment variable.\nControls whether XLIO uses hardware flow tags for accelerated packet-to-socket mapping.\n\n**What it does:**\n- When false (default): XLIO uses hardware flow tags. Each socket gets a unique flow_tag_id embedded in its hardware steering rule. When packets arrive, the NIC includes this tag in the completion queue entry (CQE), enabling O(1) socket lookup without parsing packet headers.\n- When true: Disables flow tag functionality. All incoming packets go through the slow path requiring full header parsing (Ethernet, IP, TCP/UDP) and hash table lookups to find the destination socket.\n\n**Technical background:**\nFlow tags are a hardware feature on NVIDIA/Mellanox network adapters. When a steering rule is created, XLIO assigns a flow_tag_id (derived from socket file descriptor). The hardware attaches this tag to matching packets. On receive, XLIO reads the tag directly from the CQE and performs an instant array lookup to find the socket.\n\n**When to keep disabled (false, default):**\n- All latency-sensitive applications - flow tags provide the fastest packet delivery path.\n- High packet rate workloads where CPU cycles per packet matter.\n- Applications using 2-step socket migration (requires flow tags).\n- Most production deployments where performance is important.\n\n**Performance implications when disabled (false):**\n- BENEFIT: Fast path packet processing - direct socket lookup via hardware-provided tag.\n- BENEFIT: Minimal CPU overhead per packet - no header parsing required for socket identification.\n- BENEFIT: Lower and more consistent latency - fewer instructions in critical receive path.\n- BENEFIT: Enables 2-step socket migration feature for zero-copy socket handoff.\n- COST: Each flow with a unique flow tag consumes slightly more hardware resources.\n- COST: Flow tags are disabled automatically for sockets using SO_REUSEADDR/SO_REUSEPORT (unless mc_force_flowtag is set for multicast).\n\n**When to enable (true):**\n- Hardware doesn't support flow tags (XLIO checks capability and disables automatically if unsupported).\n- Debugging scenarios where you need to trace packet classification through software path.\n- Specific compatibility requirements with certain hardware configurations.\n- Testing or benchmarking the software classification path.\n\n**Performance implications when enabled (true):**\n- COST: All packets go through slow path - requires parsing Ethernet, IP, and TCP/UDP headers.\n- COST: Hash table lookup required for each packet to find destination socket.\n- COST: Higher CPU utilization per packet - more instructions in receive path.\n- COST: Increased latency variability due to hash lookups and cache behavior.\n- COST: 2-step socket migration feature becomes unavailable.\n- BENEFIT: Slightly fewer hardware resources used (no flow tag action in steering rules).\n- BENEFIT: Simpler packet flow for debugging purposes.\n\n**Feature interactions:**\n- Cannot be used together with network.multicast.mc_flowtag_acceleration (mc_force_flowtag).\n  If both are set, mc_force_flowtag is automatically disabled with a warning.\n- 2-step socket migration requires flow tags. Migration attempts will fail with ENOTSUP if disabled.\n- When performance.steering_rules.tcp.3t_rules or performance.steering_rules.tcp.2t_rules are enabled,\n  flow tags are automatically masked (FLOW_TAG_MASK) for those connections regardless of this setting.\n- Sockets with SO_REUSEADDR or SO_REUSEPORT have flow tags disabled unless mc_force_flowtag overrides for multicast.\n\n**Receive path comparison:**\n- Flow tags enabled: CQE -> read flow_tag_id -> array lookup -> socket -> dispatch (fast path)\n- Flow tags disabled: CQE -> parse ETH header -> parse IP header -> parse TCP/UDP header ->\n  extract 5-tuple -> hash lookup in flow map -> socket -> dispatch (slow path)\n\n**Recommendation:**\nKeep this disabled (false) for production deployments unless you have a specific reason to disable flow tags. The performance benefit of flow tags is significant for latency-sensitive workloads."
                        }
                    },
                    "additionalProperties": false
                },
                "rings": {
                    "type": "object",
                    "description": "XLIO ring configuration.",
                    "properties": {
                        "max_per_interface": {
                            "type": "integer",
                            "default": 0,
                            "title": "Maximum rings per interface",
                            "description": "Maps to XLIO_RING_LIMIT_PER_INTERFACE environment variable.\nLimits the number of rings that can be allocated per network interface.\nWhen 0 (default), unlimited rings are created based on the ring allocation logic\n(e.g., one ring per thread, per socket, or per core).\n\n**What is a ring?**\nA ring is a fundamental XLIO data structure containing:\n- Queue Pairs (QPs) for hardware-accelerated send/receive operations\n- Completion Queues (CQs) for operation completion notifications\n- Work Request Elements (WREs) - pre-allocated packet buffers\n- Dedicated per-ring locks (m_lock_ring_rx, m_lock_ring_tx) for thread-safe access\n- Local buffer caches for TX/RX operations\n\nEach ring consumes significant resources:\n- Memory: ~300+ MB per ring (varies by WRE configuration)\n- Hardware: Dedicated QP and CQ resources on the NIC\n- File descriptors: Completion event channels\n\n**How the limit works:**\nWhen a socket/thread requests a ring and the limit has been reached:\n1. The request is redirected to an existing ring with the minimum reference count (load balancing)\n2. Multiple sockets/threads share the same ring and its resources\n3. Shared rings use locking to ensure thread-safe access\n\n**Value of 0 (unlimited rings - default):**\nBenefits:\n- No per-ring lock contention - each thread/socket has its own dedicated ring\n- Full hardware parallelism - independent QPs and CQs per ring\n- Better data locality - each thread accesses only its own ring\n- Optimal for applications where threads rarely share resources\n\nDrawbacks:\n- High memory consumption - total memory scales linearly with ring count\n- Global buffer pool lock contention - all rings share global buffer pools\n  (g_buffer_pool_rx_ptr, g_buffer_pool_tx) protected by a single lock.\n  With many rings (e.g., 64+), threads compete heavily for buffer allocation.\n- Can cause scalability issues: 32+ workers may perform WORSE than 16 workers\n  due to buffer pool contention overwhelming the benefits of ring isolation.\n- Higher hardware resource usage (QPs, CQs, memory regions)\n\n**Low values (e.g., 4-16 rings):**\nBenefits:\n- Reduced memory footprint - fewer rings = less total memory\n- Less global buffer pool contention - fewer concurrent buffer allocators\n- Better scalability at high thread counts - fixes the \"more workers = worse performance\" problem\n- More efficient hardware resource utilization\n\nDrawbacks:\n- Per-ring lock contention - multiple threads compete for shared ring locks\n- Potential serialization - packet processing may serialize when threads share rings\n- Reduced hardware-level parallelism\n\n**Performance guidance:**\n\nFor event-driven applications (Nginx, HAProxy):\n- Workers spend most time in epoll, rarely accessing rings simultaneously\n- Ring sharing is nearly free due to low overlap\n- Recommended: Set to worker_count / 2 (e.g., 16 for 32 workers)\n- Expected improvement: 15-25% throughput increase at high worker counts\n\nFor CPU-intensive per-connection applications:\n- Threads actively process packets continuously\n- Ring sharing causes significant lock contention\n- Recommended: Keep at 0 (unlimited) or set equal to thread count\n\nFor memory-constrained environments:\n- Each ring uses ~300+ MB\n- Set to minimum viable value (4-8) to reduce footprint\n- Trade some latency consistency for memory savings\n\n**Dual-port / multi-interface scenarios:**\nThe limit applies PER INTERFACE. With 2 ports and limit=16:\n- Port 1: max 16 rings\n- Port 2: max 16 rings\n- Total: up to 32 rings\n\n**Tuning strategy:**\n1. Start with limit = number_of_threads / 2\n2. Benchmark throughput and latency\n3. If per-ring lock contention is observed (via xlio_stats), increase the limit\n4. If global buffer pool contention persists, decrease the limit\n5. Find the optimal balance for your specific workload\n\n**Monitoring:**\nUse xlio_stats to observe:\n- n_buffer_pool_no_bufs: Buffer exhaustion events (high = increase memory or reduce rings)\n- Per-ring packet counts: Verify load is balanced across shared rings\n- Ring lock contention: High contention suggests too few rings\n\n**Example configurations:**\n- 32 Nginx workers, dual-port: XLIO_RING_LIMIT_PER_INTERFACE=16\n- 8 CPU-intensive threads: XLIO_RING_LIMIT_PER_INTERFACE=0 (unlimited)\n- Memory-limited environment: XLIO_RING_LIMIT_PER_INTERFACE=4"
                        },
                        "tx": {
                            "type": "object",
                            "description": "Transmission ring settings.",
                            "properties": {
                                "allocation_logic": {
                                    "oneOf": [
                                        {
                                            "type": "integer",
                                            "enum": [
                                                0,
                                                1,
                                                10,
                                                20,
                                                30,
                                                31
                                            ],
                                            "default": 20
                                        },
                                        {
                                            "type": "string",
                                            "enum": [
                                                "per_interface",
                                                "per_ip_address",
                                                "per_socket",
                                                "per_thread",
                                                "per_cpuid",
                                                "per_core"
                                            ],
                                            "default": "per_thread"
                                        }
                                    ],
                                    "title": "TX ring allocation logic",
                                    "description": "Maps to XLIO_RING_ALLOCATION_LOGIC_TX environment variable.\nControls how transmission rings are allocated and separated across sockets, threads, or cores.\n\n**What is a Ring?**\nA ring is a fundamental XLIO data structure containing:\n- Queue Pairs (QPs) for hardware-accelerated send/receive operations\n- Completion Queues (CQs) for completion notifications\n- Work Request Elements (WREs) - pre-allocated packet buffers\n- Dedicated per-ring locks (m_lock_ring_tx) for thread-safe access\n- Local buffer caches for TX operations\n\nEach ring consumes significant resources:\n- Memory: ~300+ MB per ring (varies by WRE configuration)\n- Hardware: Dedicated QP and CQ resources on the NIC\n- File descriptors: Completion event channels\n\n**The Logic Options (Low to High values):**\n\n- \"per_interface\" or 0 - Ring per interface (FEWEST RINGS)\n  All sockets on the same network interface share a single TX ring.\n  Key: Always 0 (constant).\n\n- \"per_ip_address\" or 1 - Ring per IP address\n  Sockets binding to the same local IP share a ring.\n  Key: Hash of the IP address.\n\n- \"per_socket\" or 10 - Ring per socket (MOST RINGS)\n  Each socket file descriptor gets its own dedicated ring.\n  Key: Socket file descriptor number.\n\n- \"per_thread\" or 20 - Ring per thread (DEFAULT)\n  Each thread gets its own ring, shared by all sockets created in that thread.\n  Key: pthread_self() thread ID.\n\n- \"per_cpuid\" or 30 - Ring per CPU core\n  Ring determined by the CPU core executing the socket operation.\n  Key: sched_getcpu() at time of ring allocation.\n\n- \"per_core\" or 31 - Ring per core with thread pinning\n  Like per_cpuid, but XLIO also pins the thread to that CPU core\n  for consistent ring assignment. Uses cpu_manager to balance threads.\n\n**Performance Tradeoffs:**\n\n*Low values (0, 1 - per_interface, per_ip_address):*\nBenefits:\n- Minimal memory footprint (fewest rings = least memory)\n- Less global buffer pool contention (fewer rings competing for global buffer pools)\n- Can scale better at VERY HIGH thread counts (32+) where buffer pool lock contention dominates\n\nDrawbacks:\n- HIGH per-ring lock contention: multiple threads compete for the same ring lock\n- Serializes packet transmission: only one thread can send at a time per ring\n- Reduces hardware parallelism: single QP handles all traffic\n- May significantly hurt applications using select()/poll() (see warning below)\n\n*High values (10, 20, 30, 31 - per_socket, per_thread, per_core):*\nBenefits:\n- NO per-ring lock contention: each thread/socket has its own dedicated ring\n- Full hardware parallelism: independent QPs and CQs per ring\n- Better data locality: each thread accesses only its own ring structures\n- Lower latency variability: no waiting for ring locks\n\nDrawbacks:\n- HIGH memory consumption: total memory scales linearly with ring count\n  (e.g., 32 threads × 2 ports × 300MB = ~19GB)\n- Global buffer pool lock contention: ALL rings share global buffer pools\n  (g_buffer_pool_tx) protected by a single lock. With many rings (32+),\n  threads compete heavily for buffer allocation, potentially causing\n  WORSE performance than fewer threads.\n- Higher hardware resource usage (more QPs, CQs, memory regions)\n\n**Warning for select()/poll() applications:**\nApplications using select() or poll() require polling ALL rings to check for completions.\nWith many rings, global_ring_poll_and_process_element() iterates through all rings,\ncreating overhead proportional to ring count. Consider using per_interface (0) or\nlimiting rings via performance.rings.max_per_interface for such applications.\n\n**Recommended Configurations:**\n\nFor event-driven servers (Nginx, HAProxy with epoll):\n- Use per_thread (20) with performance.rings.max_per_interface set to worker_count/2\n- Workers spend most time in epoll, reducing ring access overlap\n- Expected: Better scalability at high worker counts\n\nFor CPU-intensive per-connection applications:\n- Use per_thread (20) or per_socket (10) with unlimited rings\n- Threads actively process packets continuously\n- Ring sharing would cause significant lock contention\n\nFor memory-constrained environments:\n- Use per_interface (0) or per_ip_address (1)\n- Trade latency consistency for memory savings\n\nFor NUMA-aware deployments:\n- Use per_cpuid (30) or per_core (31)\n- Ensures memory locality between ring buffers and processing cores\n- Combine with proper numactl binding for best results\n\n**Interaction with other parameters:**\n- performance.rings.max_per_interface: Limits total rings regardless of logic\n- performance.rings.tx.migration_ratio: Enables automatic ring migration for per_thread/per_core modes\n- TSO (hardware_features.tcp.tso.enable): Forces migration_ratio to -1 (disabled) when TSO is enabled"
                                },
                                "migration_ratio": {
                                    "type": "integer",
                                    "default": -1,
                                    "title": "TX ring migration ratio",
                                    "description": "Maps to XLIO_RING_MIGRATION_RATIO_TX environment variable.\nControls when to replace a socket's TX ring with the current thread's ring.\n\n**When Does Migration Apply?**\nMigration only works when allocation_logic is set to:\n- per_thread (20)\n- per_cpuid (30)\n- per_core (31)\n\nIt has no effect for per_interface, per_ip_address, or per_socket modes.\n\n**The Two-Phase Migration Algorithm:**\n\n1. **Detection Phase (every `migration_ratio` ring accesses):**\n   Every time your application sends data (send/sendto/writev), XLIO increments a counter.\n   When this counter reaches `migration_ratio`, XLIO checks if the current thread/CPU\n   differs from the ring's assigned thread/CPU.\n\n2. **Stability Phase (20 additional ring accesses):**\n   If a mismatch is detected, the new thread becomes a \"migration candidate\".\n   Migration does NOT happen immediately. The candidate must remain stable for\n   20 consecutive ring accesses. If the socket bounces back to the original thread\n   during these 20 accesses, the process resets.\n\n**Total accesses before migration = migration_ratio + 20**\nExample: If migration_ratio=100, at least 120 send operations are needed before\nthe socket actually migrates to a new ring.\n\n**What Happens During Migration (Expensive!):**\n- Acquiring locks on both old and new rings\n- Releasing the old ring (may deallocate if last user)\n- Reserving a new ring (may allocate new QP/CQ)\n- Reallocating Send Gather Element (SGE) arrays\n- Invalidating cached packet headers and state\n- Potential flow steering rule updates\n\n**Value Tradeoffs:**\n\n*Low values (1-100):*\n- Pros: Faster adaptation to thread affinity changes; socket quickly moves\n  to the \"correct\" ring when application migrates work between threads.\n- Cons: Higher per-packet overhead; XLIO checks thread ID more frequently.\n  May trigger unnecessary migrations in applications with transient thread\n  borrowing patterns.\n- Best for: Applications that permanently move sockets between threads\n  (e.g., connection hand-off patterns, work stealing schedulers).\n\n*High values (500-10000):*\n- Pros: Lower per-packet overhead; checks are infrequent.\n- Cons: Slower adaptation; socket operates on the \"wrong\" ring longer when\n  threads change. This causes cross-CPU cache misses and potential NUMA\n  penalties if the ring's data structures are in a remote CPU's cache.\n- Best for: Applications with mostly stable thread affinity but occasional\n  thread migrations. Balances adaptation capability with low overhead.\n\n*Value of -1 (disabled, DEFAULT):*\n- Pros: Zero overhead; no thread ID checking, no migration attempts.\n  Optimal for single-threaded apps or when sockets never change threads.\n- Cons: No adaptation capability. If a socket is created in thread A but\n  consistently used from thread B, it will always use thread A's ring,\n  causing cache misses and lock contention with thread A's other sockets.\n- Best for: Single-threaded applications, or applications where each socket\n  is exclusively used by its creating thread.\n\n**Performance Impact of Wrong Ring:**\nUsing a ring from a different thread/CPU than the current one causes:\n1. L1/L2 cache misses: Ring data structures (locks, pointers, buffers) are\n   cached on the ring owner's CPU, not the current CPU.\n2. NUMA penalties: Cross-socket memory access adds 50-100+ ns latency.\n3. False sharing: Multiple threads accessing the same ring's cache lines.\n4. Lock contention: Current thread competes with ring owner for ring lock.\n\n**Recommended Settings:**\n\n- Single-threaded app: -1 (disabled)\n- Thread-per-connection server: -1 (sockets stay with their thread)\n- Work-stealing scheduler: 100-500 (balance adaptation vs overhead)\n- Connection hand-off pattern: 50-100 (quick adaptation needed)\n- Debugging/testing: 10 (see migrations happen quickly)"
                                },
                                "max_on_device_memory": {
                                    "type": "integer",
                                    "minimum": 0,
                                    "maximum": 262144,
                                    "default": 0,
                                    "title": "Max TX memory on device (KB)",
                                    "description": "Maps to XLIO_RING_DEV_MEM_TX environment variable.\n\n**What This Does:**\nOn Device Memory (ODM) stores egress packet data directly on the NIC's internal\nmemory instead of keeping it in host memory for DMA. This eliminates PCIe\nround-trip latency for packets that exceed the WQE inline size (~204 bytes).\n\n**How Packet Transmission Works:**\nXLIO uses three paths for sending packet data, in order of preference:\n\n1. **WQE Inline** (≤ max_inline_size, ~204 bytes):\n   Packet data is embedded directly in the Work Queue Entry (WQE) in Send Queue\n   memory. The NIC reads data from the SQ - no separate fetch needed. Fastest path.\n\n2. **On Device Memory** (ODM enabled, buffer available):\n   Packet data is copied to NIC's internal memory buffer. WQE contains a pointer\n   to this ODM location. NIC reads from its local memory - no PCIe DMA needed.\n   Eliminates host-to-NIC PCIe latency.\n\n3. **Host DMA** (ODM disabled or buffer full):\n   WQE contains a pointer to host memory. NIC must DMA the data across PCIe.\n   Adds PCIe round-trip latency (~hundreds of nanoseconds).\n\n**Buffer Behavior:**\nODM operates as a circular buffer per TX ring. When the buffer is full\n(Out-of-Buffer/OOB event), XLIO gracefully falls back to host DMA - no packets\nare lost, just slightly higher latency for those packets. Monitor OOB events\nvia xlio_stats (n_tx_dev_mem_oob counter).\n\n**Hardware Limits:**\n- Single-port HCA: 256KB total shared across all TX rings\n- Dual-port HCA: 128KB total shared across all TX rings\n\n**Value Tradeoffs:**\n\n*Value of 0 (disabled, DEFAULT):*\n- All packets larger than inline size use standard PCIe DMA from host memory\n- No HCA memory consumption\n- Best for: Memory-constrained environments, small-packet workloads,\n  applications where latency variation is acceptable\n\n*Low values (1KB-4KB):*\n- Minimal ODM buffer, frequent fallback to host DMA under load\n- Conserves HCA memory for other rings\n- Best for: High ring count configurations, memory-conscious deployments,\n  steady low-rate traffic patterns\n\n*Medium values (8KB-16KB) - Recommended for latency-sensitive workloads:*\n- Good balance between buffer capacity and memory usage\n- Handles moderate bursts without OOB events\n- Used by ULTRA_LATENCY and LATENCY profiles (16KB)\n- Best for: Trading applications, real-time systems, request-response patterns\n\n*High values (32KB-64KB):*\n- Large burst absorption capacity, minimal OOB events\n- More HCA memory consumed per ring\n- Best for: Bursty traffic patterns, high-throughput single-ring configurations,\n  applications prioritizing consistent latency over memory efficiency\n\n*Maximum values (approaching HW limits):*\n- Maximizes burst handling for a single ring\n- May starve other rings of HCA memory\n- Best for: Single-ring scenarios with extreme burst requirements\n\n**When ODM Has No Effect:**\n- On VMs without BlueFlame support (ODM is disabled on such systems due to\n  observed performance issues with dm_copy operations)\n- When packets fit within WQE inline size (already using optimal path)\n- When HCA doesn't support device memory allocation\n\n**Tuning Recommendations:**\n- Start with 0 (disabled) and measure baseline latency\n- Enable with 16KB for latency-sensitive workloads\n- Monitor n_tx_dev_mem_oob stats - high OOB rates suggest increasing buffer\n- Calculate total: (value × number_of_tx_rings) must fit within HW limit\n- For multi-ring setups, divide HW limit appropriately across rings"
                                },
                                "ring_elements_count": {
                                    "type": "integer",
                                    "default": 32768,
                                    "minimum": 0,
                                    "title": "TX Send Queue depth (WREs per ring)",
                                    "description": "Maps to XLIO_TX_WRE environment variable.\n\n**What This Does:**\nControls the size of the TX Send Queue (SQ) ring buffer for each transmit QP.\nThe Send Queue is a circular buffer where Work Request Elements (WREs) are posted\nfor the NIC to transmit. Each WRE is a packet descriptor that occupies 1-8 WQEBBs\n(64 bytes each) depending on packet size and scatter-gather complexity.\n\n**How It Affects Transmission:**\nWhen an application sends data, XLIO posts WREs to the Send Queue. The NIC\nasynchronously processes these WREs and generates completions. The queue depth\ndetermines how many packets can be \"in flight\" (posted but not yet completed)\nat any given time.\n\nIf the Send Queue fills up (all credits consumed) before completions free space:\n- In non-blocking mode: packets are silently dropped (n_tx_dropped_wqes counter)\n- In blocking mode: the send call blocks until space becomes available\n\n**Memory Impact:**\nMemory per ring ≈ ring_elements_count × 64 bytes (WQEBBs) + tracking structures.\nWith ring-per-thread allocation (default), total memory scales with thread count.\nExample: 32768 WREs × 64 bytes × 16 threads × 2 interfaces ≈ 64MB for SQ alone.\n\n**Hardware Limits:**\nThe value is automatically capped by the NIC's max_qp_wr capability.\nIf you request more than hardware supports, XLIO logs a warning and reduces\nthe value. Typical ConnectX limits are 8K-32K depending on configuration.\n\n**Relationship with completion_batch_size:**\nThe system enforces: ring_elements_count > (completion_batch_size × 2).\nIf violated, ring_elements_count is automatically increased to satisfy this.\n\n**Value Tradeoffs:**\n\n*Low values (256-512) - Best for latency-sensitive applications:*\n- Faster queue drain time on shutdown/error recovery\n- Smaller completion batches = more predictable latency (lower std-dev)\n- Lower memory footprint per ring\n- Risk: Higher chance of SQ full under burst traffic = packet drops\n- Used by: ULTRA_LATENCY profile (256), LATENCY profile (256)\n- Best for: High-frequency trading, real-time systems prioritizing consistency\n\n*Medium values (1024-4096) - Balanced profiles:*\n- Good burst absorption with reasonable memory usage\n- Moderate queue drain times\n- Used by: NGINX profile (1024)\n- Best for: Web servers, request-response workloads, general purpose\n\n*High values (16384-32768) - Best for throughput:*\n- Large burst absorption capacity\n- More packets in flight = higher sustained throughput\n- Longer completion processing cycles\n- Higher memory consumption (especially with many rings)\n- Risk: Increased latency variance under heavy load\n- Used by: Default profile (32768)\n- Best for: Bulk data transfer, streaming, high-bandwidth applications\n\n**Tuning Recommendations:**\n- Monitor n_tx_dropped_wqes via xlio_stats - non-zero indicates SQ full events\n- For latency: Start low (256-512), increase only if seeing dropped WREs\n- For throughput: Use default (32768) unless memory constrained\n- Calculate total memory: value × 64 × num_rings × num_interfaces\n- Consider ring allocation logic - per-thread creates more rings than per-interface"
                                },
                                "completion_batch_size": {
                                    "type": "integer",
                                    "default": 64,
                                    "minimum": 1,
                                    "maximum": 64,
                                    "title": "TX WRE completion batch size",
                                    "description": "Maps to XLIO_TX_WRE_BATCHING environment variable.\n\n**What This Does:**\nControls how many TX Work Request Elements (WREs) are posted to the Send Queue\nbefore XLIO requests a Completion Queue Entry (CQE) from the NIC. When a CQE\narrives, XLIO must process ALL accumulated WREs since the last completion -\nreleasing buffers back to the pool, returning SQ credits, and invoking callbacks.\n\n**How It Works:**\nWhen sending packets, each WRE can optionally set the MLX5_WQE_CTRL_CQ_UPDATE flag\nto request a completion. XLIO uses a counter (m_n_unsignaled_count) to track how\nmany WREs were posted without requesting completion. When the counter reaches zero,\nthe next WRE requests a completion, and the counter resets to completion_batch_size-1.\n\n**What Happens at Completion Time:**\nWhen a CQE arrives, handle_sq_wqe_prop() iterates through a linked list of all WREs\nposted since the last completion. For each WRE it:\n- Returns the buffer descriptor to the TX buffer pool\n- Handles TI (Transport Interface) callbacks for TLS/NVMe offloads\n- Accumulates credits for batch return to the Send Queue\nThis batch processing means higher values cause longer completion processing bursts.\n\n**Special Behaviors:**\n- Zero-copy (ZCOPY) packets ALWAYS request immediate completion regardless of this\n  setting, so the application is notified that user buffers can be reused.\n- The first send operation always triggers a completion.\n- After each signaled WRE, XLIO polls the CQ to process completions inline.\n\n**Value Tradeoffs:**\n\n*High values (32-64) - Best for throughput:*\n- Higher PPS (packets per second) - less completion overhead per packet\n- Lower average latency - fewer CQ polls interrupt the send path\n- Better CPU efficiency - fewer interrupts and context switches\n- BUT: Higher latency variance (jitter) - when completion finally arrives,\n  many buffers must be processed in a burst, causing latency spikes\n- More TX buffers held in-flight before release\n- Longer completion processing time per CQE\n- Used by: Default profile (64), NVME_BF3 profile (128 via custom override)\n- Best for: Bulk data transfer, streaming, high-bandwidth applications\n\n*Low values (1-8) - Best for latency consistency:*\n- Lower latency standard deviation - more predictable, consistent timing\n- Smoother buffer release - resources returned more frequently\n- More responsive to backpressure - faster SQ credit recovery\n- BUT: Lower PPS - more completion overhead per packet\n- Higher CPU utilization - more frequent CQ polling\n- Slightly higher average latency due to polling overhead\n- Used by: ULTRA_LATENCY profile (4), LATENCY profile (4)\n- Best for: High-frequency trading, real-time systems, latency-sensitive apps\n\n**Performance Impact Summary:**\n| Metric              | High Value (64) | Low Value (4) |\n|---------------------|-----------------|---------------|\n| Packets/sec (PPS)   | Higher          | Lower         |\n| Average latency     | Lower           | Slightly higher |\n| Latency std-dev     | Higher (spiky)  | Lower (smooth) |\n| CPU utilization     | Lower           | Higher         |\n| Buffers in-flight   | More            | Fewer          |\n\n**Tuning Recommendations:**\n- For latency-sensitive applications: Use 1-8 (profiles use 4)\n- For throughput applications: Use default (64)\n- For balanced workloads: Try 16-32 as a middle ground\n- Monitor latency histograms, not just averages, to detect jitter\n- Consider alongside ring_elements_count - system enforces:\n  ring_elements_count > (completion_batch_size × 2)"
                                },
                                "max_inline_size": {
                                    "type": "integer",
                                    "default": 204,
                                    "minimum": 0,
                                    "maximum": 884,
                                    "title": "Max TX inline size",
                                    "description": "Maps to XLIO_TX_MAX_INLINE environment variable.\n\n**What This Does:**\nControls the maximum packet size (in bytes) that can be transmitted using the\nfast \"inline\" path, where packet data is embedded directly into the Work Queue\nEntry (WQE) rather than requiring a separate DMA operation.\n\n**How Packet Transmission Works:**\nXLIO uses three transmission paths, in order of preference:\n\n1. **WQE Inline Path** (packet ≤ max_inline_size):\n   Packet data is COPIED directly into the WQE in Send Queue memory. The NIC\n   reads data from the WQE itself - no separate memory fetch needed. This is\n   the fastest path because when BlueFlame is available, the first 8 bytes are\n   written directly to the NIC's BlueFlame register, triggering immediate send.\n   The 18-byte Ethernet header is always inlined regardless of this setting.\n\n2. **On-Device Memory Path** (max_on_device_memory > 0, buffer available):\n   For packets exceeding inline size, data can be copied to NIC's internal\n   memory. WQE contains a pointer to this ODM location. Eliminates PCIe DMA\n   latency but requires completion processing to release the ODM buffer.\n\n3. **Host DMA Path** (fallback for larger packets):\n   WQE contains POINTERS to data buffers in host memory. The NIC must perform\n   a PCIe DMA read to fetch the actual packet data, adding round-trip latency.\n\n**Value Tradeoffs:**\n\n*High Values (up to 884 bytes) - Optimize for latency:*\n+ More packets use the fast inline path (no DMA delay)\n+ Lower latency for small-to-medium packets\n+ Fewer PCIe transactions and DMA operations\n+ Better for latency-sensitive workloads (trading, real-time)\n- Larger WQEs consume more Send Queue space\n- Reduces maximum number of in-flight packets (queue depth)\n- More CPU cycles spent on memcpy() to copy data into WQE\n- At 884 bytes: ~3x fewer WQEs fit in SQ compared to default 204\n\n*Low Values (including 0 to disable) - Optimize for throughput:*\n+ Smaller WQEs = more WQEs fit in Send Queue\n+ Higher queue depth = more packets can be in-flight simultaneously\n+ Less CPU overhead on data path (no memcpy for large packets)\n+ Better for bulk transfer and high-throughput scenarios\n- More packets use DMA path = higher per-packet latency\n- More PCIe transactions\n- Only beneficial when typical packets exceed the inline threshold\n\n*Default Value (204 bytes) - Balanced:*\nThe default is carefully chosen to match the BlueFlame buffer capacity:\n- 1st WQEBB (64B): Control segment + Ethernet segment with 18B inline header\n- Remaining: 3 WQEBBs (192B) + 12B from 1st WQEBB = 204B for data\nThis fits most control plane packets (TCP ACKs, small UDP messages) inline\nwhile preserving reasonable queue depth for bulk data transfers.\n\n**Queue Depth Impact:**\nHigher inline size directly reduces max WQEs in Send Queue:\n- At 204B: ~6000+ WQEs can fit (formula: SQ_size / WQE_size)\n- At 884B: ~2000 WQEs can fit (3x reduction)\nThis affects how many packets can be outstanding before backpressure.\n\n**Tuning Recommendations:**\n- Latency-critical (HFT, real-time): Consider 400-884B if packets fit\n- High throughput (bulk transfer): Keep default 204B or lower\n- Mixed workloads: Default 204B is a good balance\n- Large packets (>500B typical): Lower values won't hurt, may help throughput\n- Small packets (<200B typical): Higher values reduce DMA overhead\n- Consider with max_on_device_memory: ODM can help packets just above inline\n\n**Note:** In older releases this parameter was called XLIO_MAX_INLINE."
                                },
                                "udp_buffer_batch": {
                                    "type": "integer",
                                    "default": 8,
                                    "minimum": 1,
                                    "title": "TX buffer batch size",
                                    "description": "Maps to TX_BUFS_BATCH_UDP environment variable.\n\n**What This Does:**\nControls how many TX buffers a UDP socket fetches at once from the ring's\nshared buffer pool. Each socket maintains a local cache of pre-fetched\nbuffers to reduce contention on the shared pool.\n\n**How Buffer Fetching Works:**\n1. When sending a UDP packet, the socket first checks its local buffer cache\n2. If the cache is empty, it acquires a lock on the ring's TX buffer pool\n   and fetches 'udp_buffer_batch' buffers in a single operation\n3. One buffer is used for the current packet, the rest remain cached locally\n4. After sending, if the cache is depleted, buffers are proactively pre-fetched\n5. When the socket is destroyed, any unused cached buffers return to the pool\n\n**Value Tradeoffs:**\n\n*High Values (8, 16, 32+) - Optimize for throughput and consistency:*\n+ Fewer lock acquisitions on the ring's shared buffer pool\n+ More consistent latency (refill operations happen less frequently)\n+ Better burst performance (buffers ready for rapid consecutive sends)\n+ Reduced CPU overhead from lock contention in multi-socket scenarios\n- Higher memory consumption per socket (each socket holds N buffers locally)\n- Buffers stay in socket's cache even when idle, reducing pool availability\n- May starve other sockets if many sockets cache large batches\n- Not ideal for applications with many sockets sending infrequently\n\n*Low Values (1, 2) - Optimize for memory efficiency and fairness:*\n+ Lower memory footprint per socket\n+ Better buffer sharing across many sockets\n+ More predictable memory usage\n+ Buffers return to shared pool faster, improving availability\n- More frequent lock acquisitions on the ring (higher contention)\n- Higher latency variance due to more frequent buffer fetching\n- May limit burst performance (must fetch buffers more often)\n- Each send may incur locking overhead\n\n*Value of 1 - Minimal caching:*\nFetches one buffer per send operation. Used in latency-sensitive profiles\nwhere predictable timing matters more than throughput. Also used when\nbuffer_batching_mode is set to 'disable' (BUFFER_BATCHING_NONE).\n\n**Default Value (8) - Balanced:**\nProvides a good balance between reducing lock contention and memory usage.\nSuitable for most UDP workloads with moderate sending rates.\n\n**Tuning Recommendations:**\n- High-throughput UDP streaming: Consider 16-32 for fewer pool accesses\n- Many sockets, infrequent sends: Use 1-4 to reduce per-socket memory\n- Latency-sensitive (low jitter): Use 1 to eliminate cache variability\n- Burst traffic patterns: Higher values ensure buffers are ready\n- Memory-constrained environments: Use lower values\n- Single socket, high rate: Higher values reduce locking overhead"
                                },
                                "tcp_buffer_batch": {
                                    "type": "integer",
                                    "default": 16,
                                    "minimum": 1,
                                    "title": "TCP buffer batch size",
                                    "description": "Maps to XLIO_TX_BUFS_BATCH_TCP environment variable.\n\n**What This Does:**\nControls how many TX buffers a TCP socket fetches at once from the ring's\nshared buffer pool. Each TCP connection maintains a local cache of pre-fetched\nbuffers to reduce contention on the shared pool.\n\n**How Buffer Fetching Works:**\n1. When sending TCP data, the socket first checks its local buffer cache\n2. If the cache is empty, it acquires a lock on the ring's TX buffer pool\n   and fetches 'tcp_buffer_batch' buffers in a single operation\n3. One buffer is used for the current segment, the rest remain cached locally\n4. TCP also supports zero-copy buffers with a separate cache (m_p_zc_mem_buf_desc_list)\n5. When the connection closes, any unused cached buffers return to the pool\n\n**TCP vs UDP Differences:**\n- TCP connections are long-lived, so cached buffers are reused across many sends\n- TCP supports zero-copy transmission with separate buffer management\n- TCP has additional segment batching (see tcp_segments.socket_batch_size)\n- Higher default (16 vs UDP's 8) reflects typical TCP streaming patterns\n\n**Value Tradeoffs:**\n\n*High Values (16, 32, 64+) - Optimize for throughput and streaming:*\n+ Fewer lock acquisitions on the ring's shared buffer pool\n+ More consistent latency (refill operations happen less frequently)\n+ Better sustained throughput for streaming workloads\n+ Reduced CPU overhead from lock contention in high-connection scenarios\n+ Well-suited for long-lived connections sending continuously\n- Higher memory consumption per connection\n- Buffers stay in connection's cache even during idle periods\n- May waste memory for short-lived or bursty connections\n- Can starve other connections if many cache large batches\n\n*Low Values (1, 2, 4) - Optimize for memory efficiency:*\n+ Lower memory footprint per connection\n+ Better buffer sharing across many connections\n+ More predictable memory usage\n+ Buffers return to shared pool faster\n+ Better for servers with thousands of mostly-idle connections\n- More frequent lock acquisitions on the ring (higher contention)\n- Higher latency variance due to more frequent buffer fetching\n- May limit streaming throughput\n\n*Value of 1 - Minimal caching:*\nFetches one buffer per send operation. Used in latency-sensitive profiles\nand when buffer_batching_mode is 'disable'. Also used when worker_threads > 0\nto minimize per-connection memory overhead in high-connection scenarios.\n\n**Default Value (16) - Optimized for TCP streams:**\nHigher than UDP default (8) because TCP connections typically send many\npackets over their lifetime, making the amortized cost of larger batches\nworthwhile. Suitable for most TCP workloads.\n\n**Tuning Recommendations:**\n- High-throughput streaming (file transfer, video): Consider 32-64\n- Web servers (many short-lived connections): Use 4-8\n- High-connection servers (10K+ connections): Use 1-4 to save memory\n- Latency-sensitive (trading, real-time): Use 1 for predictable timing\n- Long-lived persistent connections: Higher values (16-32) work well\n- Worker thread mode (worker_threads > 0): Keep at 1 (auto-configured)\n\n**Related Parameters:**\n- tcp_segments.socket_batch_size: Batching for TCP segment structures\n- tcp_segments.ring_batch_size: Ring-level segment batching\n- buffers.batching_mode: Master switch for buffer batching behavior"
                                },
                                "additionalProperties": false
                            }
                        },
                        "rx": {
                            "type": "object",
                            "description": "Reception ring settings.",
                            "properties": {
                                "allocation_logic": {
                                    "oneOf": [
                                        {
                                            "type": "integer",
                                            "enum": [
                                                0,
                                                1,
                                                10,
                                                20,
                                                30,
                                                31
                                            ],
                                            "default": 20
                                        },
                                        {
                                            "type": "string",
                                            "enum": [
                                                "per_interface",
                                                "per_ip_address",
                                                "per_socket",
                                                "per_thread",
                                                "per_cpuid",
                                                "per_core"
                                            ],
                                            "default": "per_thread"
                                        }
                                    ],
                                    "title": "RX ring allocation logic",
                                    "description": "Maps to XLIO_RING_ALLOCATION_LOGIC_RX environment variable.\nControls how reception rings are allocated and separated across sockets, threads, or cores.\n\n**What is a Ring?**\nA ring is a fundamental XLIO data structure containing:\n- Queue Pairs (QPs) for hardware-accelerated send/receive operations\n- Completion Queues (CQs) for completion notifications\n- Work Request Elements (WREs) - pre-allocated packet buffers\n- Dedicated per-ring locks (m_lock_ring_rx) for thread-safe access\n- Local buffer caches for RX operations\n\nEach ring consumes significant resources:\n- Memory: ~300+ MB per ring (varies by WRE configuration)\n- Hardware: Dedicated QP and CQ resources on the NIC\n- File descriptors: Completion event channels\n\n**The Logic Options (Low to High values):**\n\n- \"per_interface\" or 0 - Ring per interface (FEWEST RINGS)\n  All sockets on the same network interface share a single RX ring.\n  Key: Always 0 (constant).\n\n- \"per_ip_address\" or 1 - Ring per IP address\n  Sockets binding to the same local IP share a ring.\n  Key: Hash of the IP address.\n\n- \"per_socket\" or 10 - Ring per socket (MOST RINGS)\n  Each socket file descriptor gets its own dedicated ring.\n  Key: Socket file descriptor number.\n\n- \"per_thread\" or 20 - Ring per thread (DEFAULT)\n  Each thread gets its own ring, shared by all sockets created in that thread.\n  Key: pthread_self() thread ID.\n\n- \"per_cpuid\" or 30 - Ring per CPU core\n  Ring determined by the CPU core executing the socket operation.\n  Key: sched_getcpu() at time of ring allocation.\n\n- \"per_core\" or 31 - Ring per core with thread pinning\n  Like per_cpuid, but XLIO also pins the thread to that CPU core\n  for consistent ring assignment. Uses cpu_manager to balance threads.\n\n**Performance Tradeoffs:**\n\n*Low values (0, 1 - per_interface, per_ip_address):*\nBenefits:\n- Minimal memory footprint (fewest rings = least memory)\n- Less global buffer pool contention (fewer rings competing for global buffer pools)\n- Can scale better at VERY HIGH thread counts (32+) where buffer pool lock contention dominates\n\nDrawbacks:\n- HIGH per-ring lock contention: multiple threads compete for the same ring lock\n- Serializes packet reception: poll_and_process_element_rx() uses trylock()\n  which returns immediately if lock is held, potentially missing packets\n- Reduces hardware parallelism: single QP/CQ handles all traffic\n- May significantly hurt applications using select()/poll() (see warning below)\n\n*High values (10, 20, 30, 31 - per_socket, per_thread, per_core):*\nBenefits:\n- NO per-ring lock contention: each thread/socket has its own dedicated ring\n- Full hardware parallelism: independent QPs and CQs per ring\n- Better data locality: each thread accesses only its own ring structures\n- Lower latency variability: no waiting for ring locks\n- Better receive side scaling: packets distributed across multiple CQs\n\nDrawbacks:\n- HIGH memory consumption: total memory scales linearly with ring count\n  (e.g., 32 threads × 2 ports × 300MB = ~19GB)\n- Global buffer pool lock contention: ALL rings share global buffer pools\n  (g_buffer_pool_rx_ptr) protected by a single lock. With many rings (32+),\n  threads compete heavily for buffer allocation, potentially causing\n  WORSE performance than fewer threads.\n- Higher hardware resource usage (more QPs, CQs, memory regions)\n\n**Warning for select()/poll() applications:**\nApplications using select() or poll() require polling ALL rings to check for data.\nWith many rings, global_ring_poll_and_process_element() iterates through all rings,\ncreating overhead proportional to ring count. Consider using per_interface (0) or\nlimiting rings via performance.rings.max_per_interface for such applications.\n\n**Recommended Configurations:**\n\nFor event-driven servers (Nginx, HAProxy with epoll):\n- Use per_thread (20) with performance.rings.max_per_interface set to worker_count/2\n- Workers spend most time in epoll, reducing ring access overlap\n- Expected: Better scalability at high worker counts\n\nFor CPU-intensive per-connection applications:\n- Use per_thread (20) or per_socket (10) with unlimited rings\n- Threads actively process packets continuously\n- Ring sharing would cause significant lock contention\n\nFor memory-constrained environments:\n- Use per_interface (0) or per_ip_address (1)\n- Trade latency consistency for memory savings\n\nFor NUMA-aware deployments:\n- Use per_cpuid (30) or per_core (31)\n- Ensures memory locality between ring buffers and processing cores\n- Combine with proper numactl binding for best results\n\n**Interaction with other parameters:**\n- performance.rings.max_per_interface: Limits total rings regardless of logic\n- performance.rings.rx.migration_ratio: Enables automatic ring migration for per_thread/per_core modes\n  (allows sockets to move to the ring of the thread currently processing them)"
                                },
                                "migration_ratio": {
                                    "type": "integer",
                                    "default": -1,
                                    "title": "RX ring migration ratio",
                                    "description": "Maps to XLIO_RING_MIGRATION_RATIO_RX environment variable.\nControls when to replace a socket's RX ring with the current thread's ring.\n\n**When Does Migration Apply?**\nMigration only works when allocation_logic is set to:\n- per_thread (20)\n- per_cpuid (30)\n- per_core (31)\n\nIt has no effect for per_interface, per_ip_address, or per_socket modes.\n\n**The Two-Phase Migration Algorithm:**\n\n1. **Detection Phase (every `migration_ratio` ring accesses):**\n   Every time your application receives data (recv/recvfrom/recvmsg/poll),\n   XLIO increments a counter. When this counter reaches `migration_ratio`,\n   XLIO checks if the current thread/CPU differs from the ring's assigned\n   thread/CPU.\n\n2. **Stability Phase (20 additional ring accesses):**\n   If a mismatch is detected, the new thread becomes a \"migration candidate\".\n   Migration does NOT happen immediately. The candidate must remain stable for\n   20 consecutive ring accesses. If the socket bounces back to the original\n   thread during these 20 accesses, the process resets.\n\n**Total accesses before migration = migration_ratio + 20**\nExample: If migration_ratio=100, at least 120 recv/poll operations are needed\nbefore the socket actually migrates to a new ring.\n\n**What Happens During Migration (Expensive!):**\n- Acquiring locks on RX queue and migration lock\n- Releasing the old ring (may deallocate if last user)\n- Reserving a new ring (may allocate new QP/CQ)\n- Moving pending receive descriptors between rings\n- Re-registering flow steering rules with new ring\n- Updating socket's ring map entries\n\n**RX Migration is Particularly Impactful Because:**\n- Hardware flow steering rules must be updated (HW reconfiguration)\n- Pending packets in the old ring's CQ must be drained\n- Buffer ownership must be transferred between rings\n\n**Value Tradeoffs:**\n\n*Low values (1-100):*\n- Pros: Faster adaptation to thread affinity changes; socket quickly moves\n  to the \"correct\" ring when application migrates work between threads.\n  Incoming packets will be delivered to the current thread's CQ faster.\n- Cons: Higher per-packet overhead; XLIO checks thread ID on every poll.\n  May trigger unnecessary migrations and flow steering updates in apps\n  with transient thread borrowing patterns.\n- Best for: Applications that permanently move sockets between threads\n  (e.g., connection hand-off patterns, work stealing schedulers).\n\n*High values (500-10000):*\n- Pros: Lower per-packet overhead; checks are infrequent.\n- Cons: Slower adaptation; socket continues polling the \"wrong\" ring.\n  This causes cross-CPU cache misses when accessing CQ and buffer\n  structures, and NUMA penalties if ring is on remote socket.\n- Best for: Applications with mostly stable thread affinity but occasional\n  thread migrations. Balances adaptation capability with low overhead.\n\n*Value of -1 (disabled, DEFAULT):*\n- Pros: Zero overhead; no thread ID checking, no migration attempts.\n  No flow steering rule updates. Optimal for single-threaded apps or\n  when sockets never change threads.\n- Cons: No adaptation capability. If a socket is created in thread A but\n  consistently polled from thread B, it will always use thread A's ring.\n  Thread B must poll thread A's CQ, causing cache misses and potential\n  lock contention.\n- Best for: Single-threaded applications, or applications where each socket\n  is exclusively polled by its creating thread.\n\n**Performance Impact of Wrong Ring (RX specific):**\nUsing an RX ring from a different thread/CPU causes:\n1. CQ polling from remote CPU: Cache lines containing CQ entries must be\n   fetched from the ring owner's CPU cache.\n2. Receive buffer access latency: Packet data buffers are allocated from\n   the ring owner's buffer pool, potentially on a different NUMA node.\n3. Lock contention: Current thread competes with ring owner for ring lock\n   during poll operations.\n4. Reduced receive throughput: Cross-CPU synchronization overhead.\n\n**Recommended Settings:**\n\n- Single-threaded app: -1 (disabled)\n- Thread-per-connection server: -1 (sockets stay with their thread)\n- Work-stealing scheduler: 100-500 (balance adaptation vs overhead)\n- Connection hand-off pattern: 50-100 (quick adaptation needed)\n- epoll-based server with thread pool: Consider 200-500 if threads\n  dynamically pick up connections from the epoll set\n- Debugging/testing: 10 (see migrations happen quickly)"
                                },
                                "ring_elements_count": {
                                    "type": "integer",
                                    "default": 32768,
                                    "minimum": 0,
                                    "title": "RX Receive Queue depth (WREs per ring)",
                                    "description": "Maps to XLIO_RX_WRE environment variable.\n\n**What This Does:**\nControls the size of the RX Receive Queue (RQ) for each receive QP.\nThe Receive Queue is where Work Request Elements (WREs) are posted to receive\nincoming packets. Each WRE represents a buffer (or set of buffers with Striding RQ)\nthat the NIC can use to store incoming packet data.\n\n**Interaction with Striding RQ (Default: Enabled):**\nThe effective receive capacity depends on whether Striding RQ is enabled:\n\n*With Striding RQ (hardware_features.striding_rq.enable=true, DEFAULT):*\n- Default: 128 WREs\n- Each WRE contains multiple strides (default: 2048 strides per WRE)\n- Effective receive slots: ring_elements_count × strides_num = 128 × 2048 = 262,144\n- Memory per WRE: strides_num × stride_size = 2048 × 64 = 128KB\n\n*Without Striding RQ (hardware_features.striding_rq.enable=false):*\n- Default: 32768 WREs\n- Each WRE holds one packet\n- Memory per WRE: ~2KB (MTU-sized buffer)\n\n**How It Affects Reception:**\nWhen packets arrive, the NIC writes data into the next available receive buffer\nand generates a completion. If all WREs are consumed (queue is empty) before the\napplication replenishes them, the NIC has nowhere to put incoming packets.\n\nUnlike TX drops (which XLIO can track), RX overruns happen in hardware:\n- Packets are silently dropped by the NIC\n- No direct counter visible to application\n- Monitor via ethtool -S (rx_out_of_buffer, rx_discards_phy)\n\n**Memory Impact:**\nWith Striding RQ: Memory per ring ≈ ring_elements_count × strides_num × stride_size\nExample: 128 × 2048 × 64 = 16MB per ring\n\nWithout Striding RQ: Memory per ring ≈ ring_elements_count × buffer_size\nExample: 32768 × 2048 ≈ 64MB per ring\n\n**Relationship with post_batch_size:**\nThe system enforces: ring_elements_count > (post_batch_size × 2).\nIf violated, ring_elements_count is automatically increased.\n\n**Profile-Specific Values:**\n- Default (STRQ): 128 WREs (262K effective receive slots)\n- Default (no STRQ): 32768 WREs\n- Ultra-Latency (STRQ): 4 WREs (8K effective slots)\n- Ultra-Latency (no STRQ): 256 WREs\n- Nginx: 32 WREs (STRQ)\n\n**Value Tradeoffs:**\n\n*Low values (4-32 with STRQ, 256-512 without) - Best for latency:*\n- Smaller completion queue = faster CQ polling\n- Less memory per ring\n- Faster buffer recycling\n- Risk: May drop packets under burst traffic if application can't keep up\n- Used by: ULTRA_LATENCY profile, LATENCY profile\n- Best for: Low-latency trading, real-time systems\n\n*Medium values (64-256 with STRQ, 4096-8192 without) - Balanced:*\n- Good burst absorption\n- Reasonable memory usage\n- Best for: General purpose, web servers\n\n*High values (512+ with STRQ, 16384+ without) - Best for throughput:*\n- Large burst absorption capacity\n- Handles packet storms without drops\n- More memory consumed\n- Larger CQ to poll = potential latency variance\n- Used by: Default profile (128 STRQ / 32768 non-STRQ)\n- Best for: High-bandwidth streaming, bulk receivers\n\n**Tuning Recommendations:**\n- Check ethtool -S for rx_out_of_buffer - non-zero indicates queue overruns\n- For STRQ (default): Start with 128, reduce if memory constrained, increase if seeing drops\n- For non-STRQ: Start with 8192-16384, adjust based on packet rate and drop counters\n- Calculate total: value × (strides_num if STRQ) × stride_size × num_rings"
                                },
                                "spare_buffers": {
                                    "type": "integer",
                                    "default": 32768,
                                    "title": "Spare RX buffers",
                                    "description": "Maps to XLIO_QP_COMPENSATION_LEVEL environment variable.\n\n**What This Does:**\nControls the size of the local buffer cache (m_rx_pool) that each ring's Completion Queue\nManager maintains. This cache provides a ready supply of pre-allocated receive buffers\nto quickly replenish the hardware Receive Queue (RQ) after packets are processed.\n\n**How the Buffer Flow Works:**\n1. Hardware NIC receives packets into buffers posted to the Receive Queue (RQ)\n2. CQ Manager polls for completions and processes received packets\n3. Processed buffers are returned to m_rx_pool (or directly to global pool)\n4. When m_rx_pool is depleted, XLIO fetches spare_buffers from global pool (g_buffer_pool_rx_rwqe)\n5. When m_rx_pool exceeds 2×spare_buffers, excess buffers return to global pool\n\nThis batching mechanism reduces lock contention on the global buffer pool.\n\n**The Debt Mechanism:**\nXLIO tracks 'm_debt' - the number of buffers consumed from RQ that need replenishment.\nWhen debt reaches post_batch_size, buffers from m_rx_pool are posted back to the RQ.\nIf m_rx_pool is empty and global pool allocation fails:\n- With performance.completion_queue.keep_full=true: current packet is dropped to recycle its buffer\n- Without keep_full: debt accumulates until buffers become available\n- If debt equals entire RQ size: packets must be dropped to prevent RQ starvation\n\n**Memory Impact Per Ring:**\nThe local cache consumes: spare_buffers × buffer_size bytes per ring.\n- With STRQ (default): buffer_size ≈ strides_num × stride_size = 128KB per WQE buffer\n- Without STRQ: buffer_size ≈ MTU (~2KB for standard, ~9KB for jumbo frames)\n\nTotal memory tied in local caches = spare_buffers × buffer_size × number_of_rings\nExample (STRQ, 4 rings, default 128): 128 × 128KB × 4 = 64MB in local caches\n\n**High Values (larger spare_buffers) - Best for Burst Tolerance:**\n\nBenefits:\n- Fewer global pool lock acquisitions → less lock contention\n- More resilient to traffic bursts - buffers ready locally without global pool access\n- Lower latency variance - no waiting for global pool allocation during bursts\n- Better tolerance when application holds buffers longer (slow processing)\n- Handles scenarios where multiple packets arrive before any are fully processed\n\nDrawbacks:\n- Higher memory consumption per ring (memory tied up in local caches)\n- With many rings (many threads), can exhaust total memory budget faster\n- Memory is reserved even during idle periods\n\nRecommended for: High packet rates, bursty traffic, applications with variable processing times\n\n**Low Values (smaller spare_buffers) - Best for Memory Efficiency:**\n\nBenefits:\n- Lower memory footprint per ring\n- More memory available in global pool for other rings\n- Better memory utilization when traffic patterns vary across rings\n- Suitable when rings have predictable, steady traffic\n\nDrawbacks:\n- More frequent global pool access → more lock contention on g_buffer_pool_rx_rwqe\n- Higher latency variance during traffic bursts (must wait for global allocation)\n- Increased risk of packet drops under sudden traffic spikes\n- May see n_buffer_pool_no_bufs increments in statistics\n\nRecommended for: Memory-constrained environments, steady low-rate traffic\n\n**Relationship with Other Parameters:**\n- Default is ring_elements_count / 2 (half the RQ size)\n- Minimum enforced is post_batch_size (cannot be smaller than batch size)\n- Works with performance.completion_queue.keep_full to determine drop behavior\n- Interacts with ring allocation_logic - more rings = more total buffer memory\n\n**Monitoring and Diagnostics:**\nUse xlio_stats to monitor:\n- n_buffer_pool_len: current local cache size per CQ\n- n_rx_sw_pkt_drops: packets dropped due to buffer exhaustion\n- n_buffer_pool_no_bufs: global pool allocation failures\n\nIf n_rx_sw_pkt_drops is non-zero, consider:\n1. Increasing spare_buffers for better burst handling\n2. Increasing XLIO_MEMORY_LIMIT if global pool is exhausted\n3. Reducing number of rings if memory is constrained\n\n**Default Values:**\n- With Striding RQ (default): 128 (since fewer, larger WQE buffers)\n- Without Striding RQ: 32768 (half of default ring_elements_count)\n\n**Tuning Guidelines:**\n- Start with default (ring_elements_count / 2)\n- For latency-sensitive: ensure value handles expected burst size\n- For memory-constrained: reduce to post_batch_size × 2 minimum\n- For high-throughput: increase if seeing sw_pkt_drops or buffer_pool_no_bufs\n- Calculate total: spare_buffers × buffer_size × num_rings × 2 (accounting for pool range)"
                                },
                                "spare_strides": {
                                    "type": "integer",
                                    "default": 32768,
                                    "title": "Spare stride descriptor cache size",
                                    "description": "Maps to XLIO_STRQ_STRIDES_COMPENSATION_LEVEL environment variable.\n\n**What This Does:**\nControls the size of the local stride descriptor cache (_stride_cache) that each ring's Striding RQ Completion Queue Manager maintains. This parameter only applies when Striding RQ is enabled (hardware_features.striding_rq.enable=true, the default).\n\n**Understanding Stride Descriptors:**\nIn Striding RQ mode, received packets don't get individual buffers. Instead:\n- Large WQE buffers (strides_num × stride_size bytes each) are posted to the hardware RQ\n- Each packet occupies one or more 'strides' (slots) within a WQE buffer\n- A stride descriptor (mem_buf_desc_t, ~300-400 bytes) is a metadata object that points into a portion of the WQE buffer, tracking packet location, size, timestamps, etc.\n\nStride descriptors are lightweight metadata objects - they do NOT contain actual packet data. The data buffers are managed separately via spare_buffers and the RWQE buffer pool.\n\n**How the Stride Cache Works:**\n1. When a packet arrives, the CQ manager needs a stride descriptor to represent it\n2. Descriptors are fetched from the local _stride_cache (lock-free operation)\n3. If _stride_cache is empty, XLIO fetches spare_strides descriptors from the global pool (g_buffer_pool_rx_stride) in one batch - this requires acquiring a lock\n4. When a packet is fully processed, its stride descriptor returns to _stride_cache\n5. If _stride_cache exceeds 2×spare_strides, excess descriptors are returned to the global pool (with lock)\n\nThis batching mechanism minimizes lock contention on the global stride pool.\n\n**Memory Impact:**\nUnlike spare_buffers (which manages large data buffers), spare_strides manages small metadata objects:\n- Per-descriptor memory: ~300-400 bytes (sizeof(mem_buf_desc_t))\n- Per-ring cache memory: spare_strides × ~350 bytes (can grow to 2× before trimming)\n- With default 32768: ~11-23 MB per ring in stride descriptor metadata\n- Total metadata memory = spare_strides × ~350 bytes × number_of_rings × (up to 2)\n\nExample with 4 rings: 32768 × 350 × 4 × 2 = ~92 MB maximum in stride descriptors\n\n**High Values (32768 - default) - Best for High Packet Rates:**\n\nBenefits:\n- Fewer global pool lock acquisitions → less lock contention between rings/threads\n- Local cache almost never exhausted, even under packet bursts\n- Better batch efficiency - large batches amortize the cost of lock acquisition\n- Essential for high packet-per-second (PPS) workloads (millions of packets/sec)\n- More resilient to traffic bursts - descriptors ready locally without global pool access\n- Lower latency variance - no stalls waiting for global pool allocation\n\nDrawbacks:\n- Higher metadata memory consumption per ring (~23 MB per ring at 2× high watermark)\n- With many rings (many threads), metadata memory can add up significantly\n- Most descriptor memory sits idle during low-traffic periods\n- Initial pool expansion may cause brief latency spike at startup\n\nRecommended for:\n- High packet rate workloads (100K+ packets/sec per ring)\n- Multi-threaded applications where lock contention matters\n- Bursty traffic patterns (financial trading, gaming, real-time systems)\n- Systems with ample memory (servers with 32GB+ RAM)\n\n**Low Values (512-4096) - Best for Memory Efficiency:**\n\nBenefits:\n- Lower metadata memory footprint per ring (~180KB-3MB per ring)\n- More memory available for other purposes (application buffers, page cache)\n- Suitable when packet rates are moderate or predictable\n- Good for embedded or memory-constrained environments\n\nDrawbacks:\n- More frequent global pool accesses → increased lock contention\n- Under high packet rates, frequent lock acquisition becomes a bottleneck\n- Higher latency variance during traffic bursts (waiting for global pool)\n- Risk of descriptor exhaustion under sudden traffic spikes (XLIO panics if pool empty)\n- Performance degradation with many concurrent rings competing for descriptors\n\nRecommended for:\n- Memory-constrained environments\n- Low to moderate packet rates (<50K packets/sec per ring)\n- Single-threaded applications (no lock contention concern)\n- Predictable, steady traffic patterns\n\n**Relationship with Other Parameters:**\n- Only effective when hardware_features.striding_rq.enable=true (default)\n- Independent of spare_buffers (which manages actual data buffers, not descriptors)\n- The global stride pool is infinite (expandable), but expansion has latency cost\n- Initial pool size = 2 × spare_strides (pre-allocated at startup)\n- Works per-ring: more rings = more total descriptor memory\n\n**Interaction with Ring Allocation:**\nWith ring allocation_logic per_thread (default) and many threads:\n- Total descriptor memory = spare_strides × 350 bytes × num_threads × num_interfaces × 2\n- Example: 32768 × 350 × 16 threads × 2 interfaces × 2 = ~735 MB in stride descriptors\n- Consider reducing spare_strides if this becomes significant\n\n**Tuning Guidelines:**\n1. **Start with default (32768)** - optimal for most high-performance scenarios\n2. **For memory-constrained systems:**\n   - Reduce to 4096-8192 for moderate packet rates\n   - Reduce to 512-1024 for low packet rates (<10K pps)\n3. **For extreme high PPS:**\n   - Keep default or increase if seeing lock contention in profiling\n4. **For many-ring scenarios (32+ threads):**\n   - Consider reducing to 8192-16384 to balance memory vs contention\n   - Use performance.rings.max_per_interface to limit total ring count\n\n**Monitoring:**\nThe stride descriptor pool doesn't have direct xlio_stats counters, but:\n- If XLIO panics with 'Unable to retrieve strides from global pool', increase this value\n- Profile lock contention on g_buffer_pool_rx_stride if performance degrades\n- Monitor total memory usage to ensure descriptor memory doesn't crowd out data buffers"
                                },
                                "post_batch_size": {
                                    "type": "integer",
                                    "default": 1024,
                                    "minimum": 1,
                                    "maximum": 1024,
                                    "title": "RX WRE batch size",
                                    "description": "Maps to XLIO_RX_WRE_BATCHING environment variable.\n\n**What This Does:**\nControls how many receive buffers XLIO accumulates before posting them back to the hardware Receive Queue (RQ) in a single batch. This batching mechanism trades off between doorbell write frequency (CPU overhead) and buffer replenishment granularity (latency consistency).\n\n**How the Mechanism Works:**\n1. **Debt Tracking:** XLIO maintains a 'm_debt' counter that tracks how many buffers have been consumed from the RQ since the last batch post.\n2. **Accumulation Phase:** As packets arrive and are processed, buffers are consumed and m_debt increments. Processed buffers go to the local rx_pool cache.\n3. **Batch Trigger:** When m_debt reaches post_batch_size, XLIO triggers batch replenishment.\n4. **Batch Posting:** XLIO fetches buffers from rx_pool (or global pool if needed), populates the WRE array with buffer descriptors, and posts the entire batch to the hardware RQ.\n5. **Doorbell Write:** A single doorbell record update notifies the NIC that new receive buffers are available.\n\nThe key insight: one doorbell write posts post_batch_size buffers, amortizing the cost of the memory barrier and PCIe transaction across many buffers.\n\n**What Happens During Batch Posting:**\n- post_recv_buffers() iterates through buffers, populating m_ibv_rx_wr_array[]\n- Each buffer's address, length, and lkey are written to scatter-gather entries\n- When the array is full (post_batch_size entries), xlio_raw_post_recv() is called\n- xlio_raw_post_recv() writes descriptors to the RQ ring buffer\n- A write memory barrier (wmb) ensures descriptors are visible before doorbell\n- Doorbell record update: *m_rq_data.dbrec = htonl(m_rq_data.head & 0xffff)\n- NIC sees the doorbell and knows new receive buffers are available\n\n**Value Tradeoffs:**\n\n*High values (512-1024, default) - Best for throughput:*\n\nBenefits:\n- Fewer doorbell writes per packet → lower CPU overhead\n- Better amortization of memory barrier cost (wmb) across many buffers\n- Reduced PCIe transaction overhead (fewer doorbell writes)\n- Better CPU cache utilization - batch processing keeps data hot in L1/L2\n- Higher sustained packets-per-second (PPS)\n- Fewer lock acquisitions on buffer pools (fetch buffers in bulk)\n\nDrawbacks:\n- Higher latency variance (jitter): when replenishment happens, the processing burst is larger\n- Longer time between RQ replenishments → RQ level fluctuates more\n- Under light load, buffers may sit in rx_pool longer before recycling\n- If application polls infrequently, RQ may run low before replenishment triggers\n- The burst of work when posting can cause momentary latency spikes\n\nUsed by: Default profile (1024)\nBest for: High-bandwidth streaming, bulk data transfer, throughput-oriented workloads\n\n*Low values (1-16) - Best for latency consistency:*\n\nBenefits:\n- Lower latency standard deviation - more predictable, smoother timing\n- Smaller processing bursts → more consistent per-packet processing time\n- RQ level stays more stable - buffers replenished frequently in small batches\n- Buffers recycled more quickly - shorter time from packet processed to buffer available\n- Better for real-time applications that need predictable response times\n- Easier to reason about worst-case latency bounds\n\nDrawbacks:\n- More doorbell writes per packet → higher CPU overhead (memory barriers, PCIe writes)\n- More frequent buffer pool accesses → potential for more lock contention\n- Lower maximum throughput due to per-batch overhead\n- Each doorbell write incurs ~100-500ns overhead\n- At very high PPS, doorbell writes can become a bottleneck\n\nUsed by: ULTRA_LATENCY profile (4), LATENCY profile (4) - only when Striding RQ disabled\nBest for: High-frequency trading, real-time control systems, latency-sensitive applications\n\n**Performance Impact Summary:**\n| Metric                    | High Value (1024) | Low Value (4)    |\n|---------------------------|-------------------|------------------|\n| Doorbell writes/packet    | ~1/1024           | ~1/4             |\n| CPU overhead              | Lower             | Higher           |\n| Max throughput (PPS)      | Higher            | Lower            |\n| Latency mean              | Slightly lower    | Slightly higher  |\n| Latency std-dev (jitter)  | Higher            | Lower            |\n| RQ buffer level stability | More variable     | More stable      |\n| Worst-case latency        | Higher spikes     | More bounded     |\n\n**Interaction with Other Parameters:**\n\n- **ring_elements_count (RX_WRE):** System enforces: ring_elements_count > (post_batch_size × 2). If violated, ring_elements_count is automatically increased.\n- **spare_buffers (QP_COMPENSATION_LEVEL):** The local rx_pool cache must have enough buffers to satisfy batch requests. spare_buffers should be >= post_batch_size.\n- **Striding RQ mode:** With Striding RQ enabled (default), fewer WREs are posted overall because each WRE contains multiple strides. The batch size still applies to WRE posting, but effective buffer count per post is multiplied by strides_num.\n- **cq_poll_batch_max:** Controls how many CQEs are polled per iteration. If cq_poll_batch_max < post_batch_size, multiple poll iterations are needed before batch posting triggers.\n\n**Striding RQ Interaction:**\nWith Striding RQ enabled (default), the meaning shifts slightly:\n- Instead of posting individual packet buffers, you're posting multi-stride WQE buffers\n- Each WQE contains strides_num strides (default: 2048), each stride can hold a packet\n- Effective receive capacity per batch post = post_batch_size × strides_num\n- Default: 1024 × 2048 = 2 million+ receive slots per batch cycle\n- This makes high post_batch_size values even more efficient with Striding RQ\n\n**Tuning Recommendations:**\n\n1. **For latency-sensitive applications (HFT, real-time):**\n   - Use 4-16 to minimize jitter\n   - Accept the throughput tradeoff for predictable timing\n   - Consider disabling Striding RQ and using small ring_elements_count\n\n2. **For throughput applications (streaming, bulk transfer):**\n   - Keep default (1024) for optimal efficiency\n   - High batch size amortizes doorbell overhead effectively\n\n3. **For balanced workloads:**\n   - Try 64-256 as a middle ground\n   - Monitor both latency histograms and throughput metrics\n\n4. **For debugging latency spikes:**\n   - Reduce post_batch_size to identify if batch posting is causing jitter\n   - Use perf/tracing to measure time spent in post_recv_buffers()\n\n**Monitoring:**\n- xlio_stats doesn't directly show batch posting frequency\n- Profile compensate_qp_poll_success() and post_recv_buffers() with perf\n- Monitor RQ buffer levels via internal debugging if available\n- Latency histograms reveal jitter patterns caused by batch processing"
                                }
                            },
                            "additionalProperties": false
                        }
                    },
                    "additionalProperties": false
                },
                "threading": {
                    "type": "object",
                    "description": "Thread and locking behavior settings.",
                    "properties": {
                        "worker_threads": {
                            "type": "integer",
                            "minimum": 0,
                            "maximum": 512,
                            "default": 0,
                            "title": "XLIO Worker Threads number",
                            "description": "Maps to XLIO_WORKER_THREADS environment variable.\nControls which execution mode XLIO uses to handle networking and progress sockets.\nApplicable only to POSIX API. XLIO Ultra API is not supported with Worker Threads mode.\n\n**The Two Execution Modes:**\n\n**Run-to-Completion Mode (value = 0, default):**\nApplication threads directly process networking operations within socket API calls.\nXLIO code runs in the context of your application threads when they call socket functions\n(send, recv, epoll_wait, poll, select, etc.).\n\nBenefits:\n- Best raw performance: lowest latency, highest throughput\n- No additional CPU cores consumed by XLIO\n- No inter-thread communication overhead\n- Direct data path - packets processed immediately in calling thread\n- Optimal cache locality - data stays on the same CPU that processes it\n\nRequirements:\n- Application must be network-aware and well-designed:\n  - Avoid sharing sockets between threads\n  - Each thread should have its own listen socket\n  - Each thread should have its own epoll file descriptor\n  - Application must call socket APIs frequently enough to progress networking\n- Blocking in application code blocks networking progress\n\n**Worker Threads Mode (value > 0):**\nXLIO spawns N dedicated worker threads that continuously poll for and process network events\nindependently of the application. Application socket operations are delegated to these threads\nvia efficient job queues.\n\nBenefits:\n- Minimal network awareness required from application\n- Sockets can be safely shared between application threads\n- Single listen socket works (XLIO creates internal RSS children per worker)\n- Single epoll context can be used across threads\n- Works even if application rarely calls socket APIs\n- Better suited for legacy applications not designed for high-performance networking\n\nCosts:\n- Each worker thread consumes 100% of a CPU core (busy-polling loop)\n- Added latency from job queue communication between app threads and workers\n- Memory overhead per worker thread (see \"Resources per Worker Thread\" below)\n- TCP only - UDP sockets not currently supported in this mode\n- Non-blocking sockets only - blocking connect() is not supported\n\n**Resources Allocated per Worker Thread:**\nEach worker thread gets its own entity_context containing:\n- poll_group: Contains rings, completion queues, and hardware resources\n- job_queue: Lock-free queue for receiving socket operations from application\n- event_handler_manager_local: For timer and event management\n- tcp_timers_collection: Per-worker TCP timer handling\n- Statistics tracking structure\n\nNote: Rings are created per socket/thread/interface based on\nperformance.rings.rx.allocation_logic and performance.rings.tx.allocation_logic settings.\n\n**Choosing the Right Value:**\n\n*Value = 0 (Run-to-Completion):*\nUse when:\n- Application is designed for high-performance networking\n- Threads don't share sockets\n- Low latency is critical (microsecond-sensitive)\n- CPU efficiency matters (no cores dedicated to XLIO)\n- Using XLIO Ultra API\n\n*Value = 1:*\nMinimal worker threads mode. Use when:\n- Testing worker threads mode behavior\n- Application has minimal networking needs\n- Single-threaded applications that need background networking\n\n*Value = 2-8 (Low):*\nUse when:\n- Application has moderate connection counts\n- Want balanced resource usage\n- Typical web servers or proxy applications\n\n*Value = 8-32 (Medium):*\nUse when:\n- High connection counts with active traffic\n- Multi-socket servers\n- Want to match worker threads to CPU cores for NUMA locality\n\n*Value = 32-512 (High):*\nUse when:\n- Very high connection density (thousands of connections)\n- Each listen socket creates N RSS children, so high values enable\n  massive parallel accept scaling\n- WARNING: Each thread consumes a full CPU core - ensure sufficient\n  CPU resources are available\n\n**Scaling Considerations:**\n\n1. CPU Usage: N worker threads = N CPU cores at 100% utilization\n   (busy-polling). Plan hardware accordingly.\n\n2. Listen Socket Scaling: When a socket calls listen(), XLIO creates\n   N RSS child sockets (one per worker thread) with hardware flow\n   steering rules to distribute incoming connections across workers.\n   This enables parallel accept() processing.\n\n3. Socket Distribution: Outgoing connections are distributed round-robin\n   across worker threads via the entity_context_manager.\n\n4. Memory Impact: Higher values increase total memory usage due to\n   per-worker structures. Monitor with xlio_stats.\n\n**Parameter Interactions:**\n\nWhen worker_threads > 0, XLIO automatically adjusts:\n- tx_buf_size: Set to 256KB for efficient batched TX operations\n- tx_bufs_batch_tcp: Set to 1 for immediate TX processing\n- select_poll_num: Set to -1 (worker threads handle polling)\n- progress_engine_interval_msec: Set to 0 (workers poll continuously)\n\n**Monitoring:**\nUse xlio_stats to observe per-worker-thread statistics:\n- idle_time: Time spent with no work (should be low if workers are needed)\n- hit_poll_time: Time spent processing received packets\n- job_proc_time: Time processing application job requests\n- job_queue_size_max: Peak job queue depth (high values suggest backpressure)\n- socket_num_added/removed: Socket lifecycle tracking\n\n**Example Configurations:**\n\n1. Latency-critical trading application:\n   worker_threads = 0 (Run-to-Completion for minimal latency)\n\n2. Nginx with 16 workers:\n   worker_threads = 8 (half of worker count, efficient for event-driven apps)\n\n3. Legacy application with socket sharing:\n   worker_threads = 4-8 (provides execution context app doesn't provide)\n\n4. High-connection-count load balancer:\n   worker_threads = 16-32 (parallel accept and connection handling)"
                        },
                        "mutex_over_spinlock": {
                            "type": "boolean",
                            "default": false,
                            "title": "Use mutex instead of spinlocks",
                            "description": "Maps to XLIO_MULTILOCK environment variable.\nControls the locking primitive used for XLIO's internal synchronization in performance-critical paths.\n\n**What Locks Are Affected:**\nThis parameter controls the lock type for:\n- Ring RX locks (m_lock_ring_rx): Protect packet reception and CQ polling\n- Ring TX locks (m_lock_ring_tx): Protect packet transmission and buffer management\n- TCP connection locks (m_tcp_con_lock): Protect TCP state machine and send/receive paths\n- Socket receive locks (m_lock_rcv): Protect socket receive buffer access\n- Application socket locks (m_app_lock): Protect general socket operations\n- Network device locks: Protect device-level operations\n\n**How Locks Are Used in XLIO:**\nXLIO uses two locking patterns depending on the code path:\n\n1. **trylock pattern (hot path - non-blocking):**\n   ```\n   if (!m_lock_ring_rx.trylock()) {\n       process_packets();\n       m_lock_ring_rx.unlock();\n   }\n   // If trylock fails, operation is skipped (returns 0)\n   ```\n   This is critical: in the packet processing hot path, XLIO uses trylock() and\n   skips the operation if the lock is held. This avoids blocking in latency-sensitive code.\n\n2. **lock pattern (blocking path):**\n   Used when the operation must complete, such as buffer allocation or socket setup.\n\n**Value = false (Spinlock - DEFAULT):**\n\nSpinlocks use busy-waiting: the thread continuously checks the lock in a tight loop\nuntil it becomes available. No kernel involvement, pure userspace atomic operations.\n\n*Benefits:*\n- Lowest uncontended latency: ~5-20 nanoseconds for lock/unlock\n- Fastest trylock: Single atomic test-and-set instruction\n- No context switches: Thread keeps running on the CPU\n- No kernel involvement: Pure userspace operation\n- Ideal for short critical sections: XLIO's ring operations are typically <1μs\n- Best for dedicated cores: When threads have exclusive CPU access\n\n*Drawbacks:*\n- CPU waste on contention: Spinning thread consumes 100% CPU while waiting\n- Priority inversion: Low-priority spinning thread can prevent high-priority work\n- Poor for long waits: If lock holder is preempted, spinner wastes CPU cycles\n- Cache line bouncing: Multiple spinners cause cache coherency traffic\n- Bad for oversubscribed systems: More threads than cores = wasted spinning\n\n*Performance characteristics:*\n- Uncontended lock: ~5-20ns\n- Contended lock: Spins until acquired (variable, depends on holder)\n- trylock (uncontended): ~5-10ns\n- trylock (contended): ~5-10ns (returns immediately with failure)\n\n**Value = true (Mutex):**\n\nMutexes use blocking: if the lock is unavailable, the thread is put to sleep\nby the kernel and woken up when the lock becomes available.\n\n*Benefits:*\n- No CPU waste on contention: Sleeping thread consumes no CPU\n- Fair scheduling: OS scheduler manages thread priorities properly\n- Better for oversubscribed systems: Many threads sharing few cores\n- Power efficient: Sleeping threads reduce power consumption\n- Better for long critical sections: Appropriate when lock is held longer\n- Plays well with other processes: Doesn't monopolize CPU\n\n*Drawbacks:*\n- Higher uncontended latency: ~50-200ns due to kernel involvement\n- Context switch on contention: ~1-10μs when thread must sleep/wake\n- Higher latency variance: Scheduling introduces jitter (microseconds)\n- syscall overhead: Every lock/unlock involves kernel\n- Not ideal for short critical sections: Overhead dominates for tiny operations\n\n*Performance characteristics:*\n- Uncontended lock: ~50-200ns\n- Contended lock: ~1-10μs (context switch + wake time)\n- trylock (uncontended): ~50-100ns\n- trylock (contended): ~50-100ns (returns immediately with failure)\n\n**Performance Impact Summary:**\n| Scenario                        | Spinlock (false)  | Mutex (true)      |\n|---------------------------------|-------------------|-------------------|\n| Uncontended lock latency        | ~5-20ns           | ~50-200ns         |\n| Contended lock (light)          | ~100ns-1μs        | ~1-10μs           |\n| Contended lock (heavy)          | Wastes CPU        | Efficient sleep   |\n| CPU usage while waiting         | 100%              | 0%                |\n| Latency jitter                  | Lower             | Higher            |\n| Best thread:core ratio          | 1:1 or fewer      | Many:1            |\n\n**When to Keep Default (false - Spinlock):**\n- Latency-sensitive applications (HFT, real-time systems)\n- Dedicated CPU cores per thread (no oversubscription)\n- Low lock contention (per-thread rings, single-threaded sockets)\n- Busy-polling applications (already consuming CPU continuously)\n- Short critical sections (typical XLIO ring operations)\n- Maximum throughput is the primary goal\n- Predictable latency is critical (lower jitter)\n\n**When to Use true (Mutex):**\n- Highly oversubscribed systems (many more threads than CPU cores)\n- Shared/multi-tenant environments (cloud VMs, containers)\n- Power-sensitive deployments (reduce idle CPU power)\n- Applications with high lock contention\n- Legacy applications with unpredictable threading patterns\n- Mixed workloads where non-XLIO threads need CPU time\n- When lock holders may be preempted by the OS\n- Systems running multiple latency-tolerant applications\n\n**Interaction with Ring Allocation:**\nThe impact of this setting depends heavily on ring allocation:\n\n- With per_thread rings (default): Each thread has its own ring, minimal contention.\n  Spinlocks are ideal - uncontended performance matters most.\n\n- With per_interface rings: Multiple threads share rings, higher contention.\n  Mutex may help if threads frequently compete for the same ring.\n\n- With ring limits (XLIO_RING_LIMIT_PER_INTERFACE): Forced ring sharing.\n  Consider mutex if contention causes excessive CPU waste from spinning.\n\n**Interaction with Worker Threads:**\nWhen worker_threads > 0, XLIO worker threads are constantly polling.\nSpinlocks ensure minimal overhead for their frequent lock operations.\nMutex would add unnecessary latency to the worker polling loops.\n\n**Tuning Strategy:**\n1. Start with default (false - spinlock) for best latency\n2. Monitor CPU utilization during lock contention scenarios\n3. If CPU usage is high but throughput is low → threads are spinning\n4. Switch to mutex (true) and measure impact\n5. If latency increases unacceptably, investigate ring allocation instead\n\n**Profiling Lock Contention:**\nUse Linux perf to identify lock contention:\n```bash\nperf lock record -a -p <pid> -- sleep 30\nperf lock report\n```\nLook for high wait times on XLIO locks (ring locks, tcp_con_lock).\n\n**Example Configurations:**\n\n1. Ultra-low-latency trading (dedicated cores):\n   mutex_over_spinlock = false (default)\n   + per_thread rings + CPU pinning\n\n2. Web server on shared cloud VM:\n   mutex_over_spinlock = true\n   + limited rings + moderate worker count\n\n3. High-throughput proxy (dedicated server):\n   mutex_over_spinlock = false (default)\n   + per_thread rings + NUMA-aware pinning\n\n4. Containerized microservice (CPU limits):\n   mutex_over_spinlock = true\n   + per_interface rings + minimal memory footprint"
                        },
                        "cpu_affinity": {
                            "type": "string",
                            "default": "-1",
                            "title": "CPU affinity",
                            "description": "Maps to XLIO_INTERNAL_THREAD_AFFINITY environment variable.\nControl which CPU core(s) the XLIO internal thread is serviced on.\n\n**What is the Internal Thread?**\n\nXLIO spawns an internal event handler thread that runs independently of your application.\nThis thread is responsible for:\n\n1. **Timer Processing**: Handles all registered timers including:\n   - TCP retransmission timers (for reliable delivery)\n   - TCP keepalive timers (connection health monitoring)\n   - ARP/neighbor resolution timers (address translation)\n   - IP fragment reassembly timers\n   - Socket cleanup and garbage collection\n\n2. **Async Hardware Events**: Monitors and processes:\n   - IB Verbs async events (device errors, port state changes)\n   - RDMA CM events (connection management)\n   - Agent progress for daemon communication\n\n3. **Event Loop Behavior**:\n   - Wakes up periodically based on performance.threading.internal_handler.timer_msec (default: 10ms)\n   - Uses epoll_wait with timeout to sleep between timer expirations\n   - Processes registered event handlers when async events arrive\n\n**Input Format:**\n\nThe cpu set should be provided as *EITHER* a hexadecimal value representing a bitmask,\n*OR* as a comma-delimited list of values (ranges are supported).\nBoth formats are identical to what the taskset command supports.\n\nBitmask Examples:\n- 0x00000001 - Run on processor 0 only\n- 0x00000007 - Run on processors 0, 1, and 2\n- 0x0000000F - Run on processors 0-3\n- 0xFFFFFFFF - Run on processors 0-31\n\nComma-Delimited Examples:\n- 0          - Run on processor 0 only\n- 0,4,8      - Run on processors 0, 4, and 8\n- 0,1,7-10   - Run on processors 0, 1, 7, 8, 9, and 10\n- 0-7        - Run on processors 0 through 7\n\n**Value Tradeoffs:**\n\n*Value = \"-1\" (default) - No Affinity (OS Scheduler Decides):*\n\nBenefits:\n- Maximum flexibility - OS scheduler optimizes for overall system performance\n- Automatic load balancing across available cores\n- Works well for general-purpose deployments\n- No risk of pinning to an overloaded or offline core\n\nDrawbacks:\n- Thread may migrate between cores, causing cache invalidation\n- Less deterministic behavior - latency may vary as thread moves between CPUs\n- Cross-NUMA migrations can significantly increase timer processing latency\n- Harder to reason about worst-case performance\n\nBest for: Development, testing, or when you don't know the system topology.\n\n*Single Core (e.g., \"0\" or \"0x01\") - Dedicated Isolated Core:*\n\nBenefits:\n- Most predictable, lowest-jitter timer processing\n- No cache pollution from thread migration\n- Easier to isolate using isolcpus or cset for guaranteed resources\n- Consistent NUMA locality - all internal thread data stays in one memory domain\n- Recommended for latency-sensitive applications\n\nDrawbacks:\n- Wastes potential parallelism if internal thread has burst workloads\n- If pinned to a core also used by application threads, may cause contention\n- Must ensure the selected core isn't being used by other latency-critical components\n\nBest for: Latency-sensitive applications (trading, real-time control).\n\n*Multiple Cores (e.g., \"0-3\" or \"0x0F\") - Bounded Migration:*\n\nBenefits:\n- Allows some OS scheduling flexibility within a bounded set\n- Can specify cores within the same NUMA node for locality\n- Provides redundancy if one core becomes overloaded\n- Balances determinism and flexibility\n\nDrawbacks:\n- Thread may still migrate within the allowed set, causing some cache misses\n- More complex to configure correctly\n- Must ensure all specified cores are in the same NUMA node for best results\n\nBest for: Multi-socket systems where you want NUMA-aware flexibility.\n\n**NUMA Considerations:**\n\nFor multi-socket systems, NUMA topology significantly affects performance:\n\n- **Same NUMA Node as Application**: Internal thread and application share L3 cache.\n  Timer-triggered operations (like TCP retransmits) have lower latency accessing\n  socket data structures.\n\n- **Different NUMA Node**: Cross-NUMA memory access adds ~50-100ns latency per access.\n  TCP timer processing may take longer, slightly delaying retransmissions.\n  However, this isolates the internal thread's cache footprint from application threads.\n\nRecommendation: Pin to a core in the same NUMA node as your primary application threads,\nbut ideally on a different physical core to avoid contention.\n\nExample for a 2-socket system with cores 0-15 on NUMA 0 and 16-31 on NUMA 1:\n- If application runs on cores 0-7, pin internal thread to core 15 (same NUMA, isolated)\n- Configuration: cpu_affinity = \"15\" or cpu_affinity = \"0x8000\"\n\n**CPU Contention Considerations:**\n\n- Internal thread wakes up every timer_msec (default 10ms) to process timers\n- Each wakeup is brief (~microseconds) unless many timers expire simultaneously\n- Sharing a core with application threads causes context switches at timer intervals\n- For latency-critical apps, dedicate a core to internal thread using isolcpus\n\n**Interaction with Other Parameters:**\n\n- **performance.threading.cpuset**: If set, thread is first moved to that cpuset,\n  then cpu_affinity is applied within that cpuset. Ensure affinity cores exist in cpuset.\n\n- **performance.threading.internal_handler.timer_msec**: Controls how often internal thread\n  wakes up. Lower values = more frequent wakeups = more important to have low-latency\n  access to the pinned core.\n\n- **performance.threading.internal_handler.behavior**: When set to \"delegate\", TCP timers\n  are handled by application threads instead of the internal thread, reducing the\n  internal thread's workload and the importance of this affinity setting.\n\n- **performance.threading.worker_threads**: When > 0, worker threads handle socket\n  operations. Internal thread still handles async events and non-delegated timers.\n\n**Recommended Configurations:**\n\n1. **Latency-Critical (HFT, Real-Time Control):**\n   - Use isolcpus to reserve a dedicated core (e.g., core 1)\n   - cpu_affinity = \"1\"\n   - internal_handler.timer_msec = 10 or lower\n   - Ensure application threads are pinned to different cores\n\n2. **High-Throughput Server (Many Connections):**\n   - cpu_affinity = \"-1\" (let OS balance) or pin to a specific NUMA node\n   - internal_handler.timer_msec = 100 (reduce wakeup frequency)\n   - Consider internal_handler.behavior = \"delegate\" if connections are thread-local\n\n3. **Multi-Socket NUMA System:**\n   - Identify which NUMA node your application primarily uses\n   - Pin internal thread to a core on that NUMA node: cpu_affinity = \"<core_on_same_numa>\"\n   - Verify with: numactl --hardware and lstopo\n\n4. **Container/VM Environment:**\n   - cpu_affinity = \"-1\" (container runtime may restrict available cores)\n   - Or use values matching container's cpuset allocation\n\n**Monitoring:**\n\n- Use `top -H -p <pid>` to see internal thread CPU usage\n- Thread is named with PID visible in /proc/<pid>/task/\n- High CPU on internal thread suggests many timers expiring - consider tuning timer_msec\n\n**Profiles That Set This Value:**\n- LATENCY profile: \"0\" (pinned to core 0 for consistency)\n- ULTRA_LATENCY profile: \"0\" (pinned to core 0)\n- NGINX_ULTRA_LATENCY profile: \"0x01\" (pinned to core 0)\n- Default profile: \"-1\" (no affinity, OS decides)\n\nNOTE: Only hexadecimal values are supported for this parameter in XLIO_INLINE_CONFIG."
                        },
                        "cpuset": {
                            "type": "string",
                            "default": "",
                            "title": "CPU set path",
                            "description": "Maps to XLIO_INTERNAL_THREAD_CPUSET environment variable.\nSelect a cpuset for XLIO internal thread.\n\n**What is a cpuset?**\n\ncpuset is a Linux kernel mechanism (part of cgroups v1) that constrains which CPUs and memory nodes\na process or thread can use. Unlike cpu_affinity which is a per-thread hint that the scheduler\nmay override, cpuset provides hard isolation - the kernel will NEVER schedule the thread on CPUs\noutside its cpuset.\n\ncpusets are represented as filesystem directories under /dev/cpuset (or /sys/fs/cgroup/cpuset on\nmodern systems). Each cpuset directory contains:\n- cpuset.cpus: The CPUs this cpuset can use (e.g., \"0-3\" or \"0,2,4\")\n- cpuset.mems: The NUMA memory nodes this cpuset can allocate from\n- tasks: PIDs/TIDs of processes/threads in this cpuset\n\nWhen XLIO's internal thread starts, it writes its TID to <cpuset_path>/tasks to join the cpuset.\n\n**Value Options:**\n\n*Value = \"\" (empty string - DEFAULT):*\n\nThe internal thread inherits the cpuset of the process that loaded XLIO.\nThis is usually the root cpuset containing all CPUs, but may be restricted if:\n- The application was launched with cset, cpuset, or systemd-run --scope\n- Running in a container with CPU limits (Docker, Kubernetes)\n- The parent shell has cpuset restrictions\n\nBenefits:\n- No configuration required\n- Thread follows the application's resource constraints automatically\n- Works correctly in containerized environments\n\nDrawbacks:\n- No explicit control over CPU isolation\n- Thread competes with all application threads for the inherited CPUs\n\nBest for: Most deployments, especially containers.\n\n*Value = \"/path/to/cpuset\" (explicit cpuset path):*\n\nThe internal thread explicitly joins the specified cpuset, gaining its CPU and memory constraints.\n\nCommon paths:\n- /dev/cpuset/my_isolated_set (cgroups v1 style)\n- /sys/fs/cgroup/cpuset/my_isolated_set (cgroups v1 on modern systems)\n\nBenefits:\n- Hard CPU isolation - kernel guarantees thread runs only on specified CPUs\n- Can isolate internal thread from application threads for predictable latency\n- NUMA memory binding - thread allocates memory from specified NUMA nodes only\n- Integrates with system-level resource management (cgroups, Kubernetes)\n- Survives across thread priority changes and scheduler reconfigurations\n\nDrawbacks:\n- Requires cpuset to exist before XLIO initialization (created by admin/orchestrator)\n- More complex deployment - cpuset must be pre-configured\n- If cpuset's CPUs become offline, thread may be unable to run\n- Wrong configuration can severely limit performance\n\nBest for: Production systems with strict isolation requirements.\n\n**Interaction with cpu_affinity:**\n\nWhen BOTH cpuset and cpu_affinity are set, they are applied in order:\n\n1. FIRST: Thread joins the cpuset (moved to new CPU/memory constraints)\n2. SECOND: cpu_affinity is applied WITHIN that cpuset\n\nCRITICAL: cpu_affinity must specify CPUs that exist in the cpuset!\nIf cpu_affinity includes CPUs not in the cpuset, pthread_setaffinity_np() will fail\nand XLIO logs: \"Internal thread affinity failed. Did you try to set affinity outside of cpuset?\"\n\nExample:\n```\ncpuset: /dev/cpuset/xlio_set       # Contains CPUs 4,5,6,7\ncpu_affinity: 4                     # OK - CPU 4 is in the cpuset\ncpu_affinity: 0                     # FAIL - CPU 0 is not in the cpuset\ncpu_affinity: 4,5                   # OK - both CPUs are in the cpuset\n```\n\nRecommendation: If using cpuset, set cpu_affinity to \"-1\" (no affinity) and let the cpuset\ncontrol CPU placement, OR ensure cpu_affinity is a subset of the cpuset's CPUs.\n\n**cpuset vs cpu_affinity - When to Use Each:**\n\n| Feature                    | cpuset                         | cpu_affinity                    |\n|----------------------------|--------------------------------|---------------------------------|\n| Enforcement                | Hard (kernel guarantee)        | Soft (scheduler hint)           |\n| Persistence                | Survives reconfigurations      | Can be overridden               |\n| NUMA memory binding        | Yes (cpuset.mems)              | No                              |\n| Setup complexity           | Requires pre-created cpuset    | Just set the parameter          |\n| Container integration      | Native (cgroups)               | Per-thread only                 |\n| Recommended for            | System-level isolation         | Thread-level tuning             |\n\nFor most users: Use cpu_affinity alone (simpler, effective for latency tuning).\nFor production isolation: Use cpuset for hard isolation, optionally with cpu_affinity within it.\n\n**Performance Considerations:**\n\n*Benefits of Using cpuset:*\n- Guaranteed CPU isolation prevents unexpected thread migration\n- NUMA-aware memory allocation (set cpuset.mems to match cpuset.cpus NUMA nodes)\n- Predictable worst-case latency (no cross-NUMA surprises)\n- Integrates with Kubernetes CPU manager (static policy) for pod-level isolation\n\n*Costs of Using cpuset:*\n- Joining a cpuset requires filesystem write (one-time ~1ms at startup)\n- If cpuset is too restrictive, thread may be starved of CPU time\n- Configuration complexity and potential for misconfiguration\n\n**Creating a cpuset (for administrators):**\n\n```bash\n# Create cpuset for XLIO internal thread on CPUs 4-7, NUMA node 0\nmkdir -p /dev/cpuset/xlio_internal\necho 4-7 > /dev/cpuset/xlio_internal/cpuset.cpus\necho 0 > /dev/cpuset/xlio_internal/cpuset.mems\necho 1 > /dev/cpuset/xlio_internal/cpu_exclusive  # Optional: exclusive access\n```\n\nThen set: cpuset = \"/dev/cpuset/xlio_internal\"\n\n**Example Configurations:**\n\n1. **Default (no cpuset, use cpu_affinity only):**\n   - cpuset = \"\" (default)\n   - cpu_affinity = \"4\" (or desired core)\n   - Simple setup, effective for most use cases\n\n2. **Container environment (inherit container's cpuset):**\n   - cpuset = \"\" (default - inherits container's cgroup)\n   - cpu_affinity = \"-1\" (let container scheduler decide)\n   - Works with Docker --cpuset-cpus and Kubernetes CPU limits\n\n3. **Strict isolation with NUMA binding:**\n   - cpuset = \"/dev/cpuset/xlio_isolated\" (admin pre-creates with specific CPUs/NUMA)\n   - cpu_affinity = \"-1\" (let cpuset handle placement)\n   - For latency-critical deployments with system-level resource management\n\n4. **cpuset + specific affinity within cpuset:**\n   - cpuset = \"/dev/cpuset/xlio_set\" (contains CPUs 4-7)\n   - cpu_affinity = \"4\" (pin to one specific CPU within the cpuset)\n   - Maximum control - cpuset provides isolation, affinity provides precision\n\n**Kubernetes Integration:**\n\nWhen running in Kubernetes with CPU manager (static policy), pods with guaranteed QoS class\nget their own cpuset automatically. XLIO will inherit this cpuset if cpuset = \"\".\n\nFor exclusive CPU access in Kubernetes:\n```yaml\nresources:\n  requests:\n    cpu: \"2\"       # Request exactly 2 CPUs\n    memory: \"4Gi\"\n  limits:\n    cpu: \"2\"       # Limit to exactly 2 CPUs (creates exclusive cpuset)\n    memory: \"4Gi\"\n```\n\n**Troubleshooting:**\n\n- If XLIO fails to start with cpuset errors, verify the path exists and is readable/writable\n- Use `cat /dev/cpuset/your_set/cpuset.cpus` to check which CPUs are in the cpuset\n- Use `cat /dev/cpuset/your_set/tasks` to verify the thread joined successfully\n- If cpu_affinity fails with cpuset, ensure affinity cores are in cpuset.cpus"
                        },
                        "internal_handler": {
                            "type": "object",
                            "description": "Internal thread handler configuration.",
                            "properties": {
                                "timer_msec": {
                                    "type": "integer",
                                    "minimum": 0,
                                    "default": 10,
                                    "title": "Timer resolution (msec)",
                                    "description": "Maps to XLIO_TIMER_RESOLUTION_MSEC environment variable.\nControls the minimum wakeup interval (in milliseconds) for XLIO's internal thread.\n\n**What is the Internal Thread?**\nXLIO spawns a dedicated internal thread that handles:\n- Timer events (TCP timers, neighbor/ARP discovery, netlink route updates)\n- RDMA Connection Manager (CM) events for connection establishment\n- IBVerbs async events (hardware errors, port state changes, device events)\n- Netlink monitoring for routing table and neighbor cache updates\n\nThe internal thread uses epoll_wait() with a timeout. This parameter sets the\nMINIMUM sleep duration—the thread wakes up at least every timer_msec milliseconds,\neven if no events are pending.\n\n**Relationship with TCP Timer Resolution:**\nThis parameter is a LOWER BOUND for network.protocols.tcp.timer_msec.\nTCP timer resolution cannot be smaller than this value. The TCP timer collection\nuses buckets = (tcp_timer_msec / timer_msec), so this ratio affects how TCP\nconnections are grouped for timer processing.\n\n**Low Values (1-10 ms) - Latency-Optimized:**\nBenefits:\n- Faster reaction to timer expirations (retransmissions, keepalives)\n- More accurate TCP RTO calculations\n- Lower worst-case latency for handling hardware/connection events\n- Better responsiveness to network topology changes (link failures)\n\nDrawbacks:\n- Higher CPU utilization due to frequent epoll_wait() syscalls\n- More context switches between internal thread and application\n- May cause unnecessary wakeups when no work is pending\n- Can add scheduling noise affecting tail latencies\n\n**High Values (32-100+ ms) - Throughput/Efficiency-Optimized:**\nBenefits:\n- Lower CPU utilization from reduced syscall frequency\n- Less scheduler noise, better for CPU-bound applications\n- Allows longer sleep periods when idle, improving power efficiency\n- Better suited for high-throughput, batch-oriented workloads\n\nDrawbacks:\n- Delayed reaction to TCP retransmission timeouts under packet loss\n- Coarser timer granularity affects RTO accuracy\n- Slower detection of connection/hardware failures\n- May increase tail latency for timer-dependent operations\n- Keepalive and persist timer probes sent less precisely\n\n**TCP Operations Affected by Timer Granularity:**\n- Retransmission timeout (RTO) - triggers packet retransmission\n- Keepalive probes - detects dead connections\n- Persist timer - zero-window probes when receiver advertises 0 window\n- TIME_WAIT cleanup - releases connection resources after 2*MSL\n- Connection timeouts - SYN_RCVD, FIN_WAIT_2, LAST_ACK state cleanup\n- Out-of-sequence segment timeout - drops stale OOO queued data\n\n**Recommended Configurations:**\n\nFor latency-sensitive applications (trading, real-time):\n- Use 1-10 ms (default 10 ms is good for most cases)\n- Pair with low tcp.timer_msec (e.g., 10-50 ms)\n\nFor throughput-focused applications (web servers, proxies like Nginx):\n- Use 32-64 ms to reduce CPU overhead\n- XLIO uses 32 ms for NGINX profile internally\n\nFor idle/low-traffic scenarios:\n- Higher values (50-100 ms) reduce unnecessary wakeups\n- Consider pairing with higher tcp.timer_msec (100-256 ms)\n\n**Note:** This parameter affects ALL timer-based operations in XLIO,\nnot just TCP. Changes impact neighbor discovery, route monitoring,\nand hardware event handling as well."
                                },
                                "behavior": {
                                    "oneOf": [
                                        {
                                            "type": "integer",
                                            "enum": [
                                                0,
                                                1
                                            ],
                                            "default": 0
                                        },
                                        {
                                            "type": "string",
                                            "enum": [
                                                "disable",
                                                "delegate"
                                            ],
                                            "default": "disable"
                                        }
                                    ],
                                    "title": "TCP control flow behavior",
                                    "description": "Maps to XLIO_TCP_CTL_THREAD environment variable.\nControls how TCP control flows (timers, state machine operations) are executed.\n\n**What Are TCP Control Flows?**\nTCP control flows include timer-driven operations critical to TCP reliability:\n- Retransmission timers (RTO): Retransmit unacknowledged packets\n- Delayed ACK timers: Send ACKs for received data\n- Keepalive timers: Detect dead connections\n- Persist timers: Probe zero-window receivers\n- Connection state cleanup: TIME_WAIT, FIN_WAIT, etc.\n- Congestion control updates\n\n**The Two Modes:**\n\n**\"disable\" or 0 (DEFAULT) - Internal Thread Mode:**\n\nArchitecture:\n- XLIO's internal thread (event_handler_manager) processes ALL TCP timers\n- Global timer collection (g_tcp_timers_collection) tracks all TCP sockets\n- Application threads use the global event handler for timer registration\n- TCP sockets use REAL locks (spinlocks or mutexes per mutex_over_spinlock setting)\n\nHow it works:\n1. Internal thread wakes up every performance.threading.internal_handler.timer_msec (default: 10ms)\n2. Processes all registered TCP timers in the global collection\n3. Acquires TCP connection locks (trylock pattern) to process each socket\n4. If lock contention occurs, timer processing for that socket is deferred\n\nBenefits:\n+ Thread-safe: Sockets can be used from ANY thread at any time\n+ Application threads can block indefinitely (poll/select/sleep) without affecting TCP\n+ Timers fire reliably regardless of application activity\n+ Supports traditional multi-threaded socket patterns (accept in one thread, read/write in another)\n+ No restrictions on socket lifecycle or thread affinity\n\nDrawbacks:\n- Lock contention: Internal thread competes with application threads for TCP locks\n- Latency variance: Timer processing may be delayed if socket lock is held\n- Cross-thread coordination overhead between internal thread and application\n- Internal thread CPU cost (though typically minimal)\n\n**\"delegate\" or 1 - Application Thread Mode (Lock-Free):**\n\nArchitecture:\n- NO internal thread processing of TCP timers\n- Thread-local timer collection (g_thread_local_tcp_timers) per application thread\n- Thread-local event handler (g_event_handler_manager_local) per thread\n- TCP sockets use DUMMY locks (NO real locking - lock-free operation)\n\nHow it works:\n1. Each application thread has its own timer infrastructure (thread_local)\n2. Timers are processed when application calls socket operations:\n   - recv()/send() trigger do_tasks() to check timers\n   - epoll_wait() periodically wakes up to run timers\n   - Explicit polling in rx_wait_helper() runs timers\n3. No cross-thread communication for timer handling\n\nBenefits:\n+ ZERO lock contention: No locks on TCP connection operations\n+ Lower latency: No lock acquisition/release overhead\n+ Better CPU cache locality: All socket data stays on one CPU\n+ Eliminates internal thread CPU usage\n+ Ideal for busy-polling applications with dedicated threads\n\nDrawbacks:\n- STRICT thread affinity: Socket MUST be owned by ONE thread from creation to destruction\n- Socket migration between threads causes undefined behavior (corruption, crashes)\n- Timers only fire when application is active on the socket\n- INCOMPATIBLE with blocking poll()/select() - see below\n- Application must provide consistent execution context\n\n**CRITICAL: Blocking poll()/select() Incompatibility**\n\nWhen using \"delegate\" mode:\n- Blocking poll() and select() DO NOT process TCP timers internally\n- While blocked in poll()/select(), NO timers fire for delegate-mode sockets\n- This can cause:\n  * TCP retransmissions to be delayed indefinitely\n  * Keepalive probes to never be sent\n  * Connections to hang or timeout incorrectly\n  * Delayed ACKs to never be sent, stalling senders\n\nWhy epoll is OK:\n- XLIO's socket-level os_epoll_wait_with_tcp_timers() method periodically\n  wakes up (every tcp_timer_resolution_msec) and calls do_tasks()\n- This ensures timer processing during blocking waits\n- Only applies to per-socket internal epoll, not global epoll multiplexing\n\n**Performance Impact Summary:**\n\n| Aspect                  | disable (0)            | delegate (1)          |\n|-------------------------|------------------------|-----------------------|\n| Lock overhead           | Real locks acquired    | Zero (dummy locks)    |\n| Timer reliability       | Always fires on time   | Depends on app activity|\n| Thread flexibility      | Any thread can use     | Single owner thread   |\n| Blocking poll/select    | ✓ Fully supported      | ✗ DO NOT USE          |\n| Blocking epoll          | ✓ Supported            | ✓ Supported           |\n| Non-blocking operations | ✓ Supported            | ✓ Best performance    |\n| CPU overhead            | Internal thread cost   | Zero internal thread  |\n| Latency (uncontended)   | Lock overhead (~5-50ns)| Near-zero             |\n| Latency (contended)     | Variable (lock wait)   | N/A (no contention)   |\n\n**Forced Configuration Changes:**\nWhen \"delegate\" is enabled, XLIO automatically forces:\n- Ring allocation logic: RING_LOGIC_PER_THREAD (for both TX and RX)\n- Progress engine interval: DISABLED (MCE_CQ_DRAIN_INTERVAL_DISABLED)\n\nThese changes ensure per-thread isolation required for lock-free operation.\n\n**When to Use \"disable\" (0) - DEFAULT:**\n- Applications using blocking poll() or select()\n- Multi-threaded socket patterns (accept/read/write on different threads)\n- Unknown or legacy application threading models\n- Applications that may migrate sockets between threads\n- Mixed blocking and non-blocking socket usage\n- General-purpose, conservative configuration\n\n**When to Use \"delegate\" (1):**\n- Busy-polling applications (trading systems, packet processors)\n- Applications using epoll exclusively (not poll/select)\n- Single-threaded event loops (one thread per connection)\n- Applications with strict thread-to-socket affinity\n- Latency-critical workloads where lock overhead matters\n- Applications already using thread-per-connection models\n- When maximum lock-free performance is required\n\n**Example Configurations:**\n\n1. High-Frequency Trading (latency-critical, single-threaded):\n   behavior = \"delegate\"\n   + epoll-based event loop\n   + busy-polling (rx_poll = -1)\n   + CPU pinning per thread\n\n2. Web Server (nginx-like, blocking accept):\n   behavior = \"disable\" (default)\n   + May use blocking operations in various threads\n   + Connections may be handed between threads\n\n3. Redis-like Server (single-threaded event loop):\n   behavior = \"delegate\"\n   + Single thread handles all connections via epoll\n   + No blocking poll/select usage\n\n4. Multi-threaded Server (thread pool):\n   behavior = \"disable\" (default)\n   + Connection may be processed by different worker threads\n   + Thread safety required\n\n**Monitoring and Debugging:**\nWith delegate mode, if you observe:\n- TCP retransmissions delayed → Application not calling socket ops frequently enough\n- Connections hanging → Likely using blocking poll/select\n- Crashes or corruption → Socket being used from multiple threads\n\n**WARNING:** Violating the single-thread ownership rule in delegate mode causes\nundefined behavior including data corruption, crashes, and memory leaks.\nThere is NO runtime check - it is the application's responsibility to ensure\neach socket is only accessed from its owning thread."
                                }
                            },
                            "additionalProperties": false
                        }
                    },
                    "additionalProperties": false
                },
                "polling": {
                    "type": "object",
                    "description": "Network polling settings.",
                    "properties": {
                        "nonblocking_eagain": {
                            "type": "boolean",
                            "default": false,
                            "title": "Return EAGAIN on nonblocking send",
                            "description": "Maps to XLIO_TX_NONBLOCKED_EAGAINS environment variable.\n\n**What This Does:**\nControls how XLIO handles non-blocking UDP send operations when transmit buffers are temporarily unavailable. This parameter affects the return value and error reporting behavior when a non-blocking UDP socket attempts to send but XLIO cannot allocate TX buffers from its internal buffer pool.\n\n**Note:** This parameter only affects UDP sockets. TCP sockets have different buffer management and flow control mechanisms.\n\n**When TX Buffer Exhaustion Occurs:**\nTX buffer exhaustion happens when:\n- Application sends packets faster than the NIC can transmit them\n- Completion queue processing cannot keep up with recycling used buffers\n- Multiple rings compete for a limited global buffer pool\n- Burst traffic exceeds available pre-allocated buffers\n\n**Value: false (Default) - OS-Compatible Silent Drops:**\n\nBehavior:\n- Returns success (the number of bytes \"sent\") even though the packet was dropped\n- The datagram is silently discarded inside XLIO\n- Application receives no indication of the failure\n- Matches standard Linux/OS behavior for non-blocking UDP sockets\n\nBenefits:\n- Drop-in compatible with existing applications designed for OS sockets\n- No application code changes required\n- Simpler application logic - no need to handle EAGAIN on UDP sends\n- Works transparently with fire-and-forget UDP patterns\n- Lower latency - no retry overhead in send path\n\nDrawbacks:\n- Silent data loss with no feedback mechanism\n- Cannot implement application-level flow control or backpressure\n- Harder to detect and debug packet loss issues\n- Statistics may be misleading - packets counted as \"sent\" were actually dropped\n- No opportunity for application to make intelligent decisions about dropped data\n\nBest for:\n- Applications designed for UDP's unreliable semantics\n- Fire-and-forget messaging patterns\n- Applications where occasional packet loss is acceptable\n- Legacy applications that cannot be modified\n- Scenarios where upper layers handle retransmission (e.g., QUIC, custom reliability)\n\n**Value: true - Explicit EAGAIN on Buffer Exhaustion:**\n\nBehavior:\n- Returns -1 with errno set to EAGAIN when TX buffers are unavailable\n- Application is explicitly notified that the send operation could not be completed\n- Packet is not silently dropped - the failure is communicated\n\nBenefits:\n- Full visibility into send failures - application knows when drops occur\n- Enables application-level flow control and backpressure mechanisms\n- More accurate performance monitoring and statistics\n- Application can implement intelligent retry strategies (exponential backoff, etc.)\n- Can detect overload conditions and adapt sending rate\n- Better debugging and troubleshooting capabilities\n- Useful for applications that need delivery guarantees at the application layer\n\nDrawbacks:\n- Requires application to handle EAGAIN error on send()/sendto()/sendmsg()\n- May require application code modifications from standard UDP patterns\n- Adds complexity to error handling paths\n- Immediate retries can cause CPU spinning if not handled carefully\n- May increase latency if application busy-waits on EAGAIN\n\nBest for:\n- High-reliability UDP applications needing explicit failure feedback\n- Applications implementing custom flow control\n- Real-time systems where silent drops are unacceptable\n- Performance-critical applications that need to adapt to backpressure\n- Applications with their own retry/reliability layer that needs accurate feedback\n- Debugging and performance analysis scenarios\n\n**Example Application Handling for true:**\n```c\nssize_t ret = sendto(fd, buf, len, MSG_DONTWAIT, &addr, sizeof(addr));\nif (ret < 0 && errno == EAGAIN) {\n    // TX buffers exhausted - implement backpressure\n    // Option 1: Poll/select for POLLOUT before retrying\n    // Option 2: Exponential backoff\n    // Option 3: Drop packet with application-level logging\n}\n```\n\n**Interaction with Other Parameters:**\n- performance.rings.tx.ring_elements_count: Larger TX queues reduce frequency of buffer exhaustion\n- performance.rings.tx.udp_buffer_batch: Controls how many buffers are fetched per batch\n- performance.rings.tx.completion_batch_size: Affects how quickly buffers are recycled\n\n**Monitoring:**\nUse xlio_stats to monitor TX health:\n- n_tx_eagain: Count of EAGAIN returns (only meaningful when this option is true)\n- n_tx_dropped_wqes: Ring-level drops when Send Queue is full (separate from buffer exhaustion)\n- Ring TX buffer availability in real-time stats"
                        },
                        "rx_poll_on_tx_tcp": {
                            "type": "boolean",
                            "default": false,
                            "title": "Poll RX queues on transmit",
                            "description": "Maps to XLIO_RX_POLL_ON_TX_TCP environment variable.\n\n**What This Does:**\nWhen enabled, each TCP send() operation will opportunistically poll the RX completion queue to process incoming TCP ACKs before returning. This creates a tight feedback loop between transmission and acknowledgment processing, allowing send buffer space to be freed faster during data transmission.\n\n**Note:** This parameter only affects POSIX API sockets (standard send/recv). XLIO Ultra API sockets with entity_context have their own polling mechanism and ignore this setting.\n\n**Understanding the TCP Send Buffer Mechanism:**\nTCP maintains a send buffer (controlled by SO_SNDBUF / network.protocols.tcp.wmem) that limits how much unacknowledged data can be outstanding. When you call send():\n1. Data is copied into the send buffer and transmitted\n2. The send buffer space is consumed (reducing sndbuf_available())\n3. When the remote peer sends an ACK, that acknowledged data is released\n4. Only then is the send buffer space restored for new data\n\nWithout rx_poll_on_tx_tcp, ACKs are only processed during:\n- Explicit recv() calls\n- epoll_wait()/poll()/select() calls\n- XLIO internal timer events\n\nThis creates a potential bottleneck: if you're sending data in a tight loop without receiving, ACKs accumulate in the RX queue unprocessed, and your send buffer fills up.\n\n**Value: false (Default) - Standard TX/RX Separation:**\n\nBehavior:\n- send() only transmits data and returns immediately\n- ACK processing happens separately during RX operations\n- Send buffer is released only when application polls for RX events\n\nBenefits:\n- Lower per-call latency for send() operations\n- Predictable CPU usage - TX and RX are cleanly separated\n- No wasted CPU cycles polling when no ACKs are pending\n- Works well with event-driven architectures (epoll/poll)\n- Better for applications that naturally interleave TX and RX\n\nDrawbacks:\n- Send buffer can fill up faster in TX-heavy workloads\n- Blocking sends may stall waiting for send buffer space\n- Congestion window growth may be slower (ACKs drive cwnd increase)\n- May require explicit RX polling to maintain TX throughput\n\nBest for:\n- Event-driven applications using epoll/poll for both TX and RX\n- Applications with natural request/response patterns\n- Low-latency scenarios where per-call overhead matters\n- Applications already polling RX frequently\n- XLIO Ultra API applications (this setting is ignored anyway)\n\n**Value: true - Opportunistic ACK Processing on TX:**\n\nBehavior:\n- Each send() call first polls the RX completion queue (CQ)\n- Incoming ACKs are processed immediately, freeing send buffer space\n- Then the actual transmission proceeds\n- Creates a feedback loop: TX triggers ACK processing triggers more TX capacity\n\nBenefits:\n- Faster send buffer release during streaming workloads\n- Better throughput for bulk data transfer scenarios\n- Faster TCP congestion window (cwnd) growth - ACKs drive cwnd increase\n- Reduced blocking time for blocking sockets when send buffer is full\n- Self-sustaining TX flow - no need for separate RX polling to maintain TX\n- Particularly effective with performance.threading.internal_handler.behavior=delegate\n\nDrawbacks:\n- Increased per-call latency for send() - each call does extra RX polling\n- Additional CPU cycles even when no ACKs are pending\n- Redundant work if application already polls RX frequently\n- May increase jitter in send latency due to variable RX processing\n- Extra lock contention on the RX ring during TX operations\n\nBest for:\n- Streaming/bulk data transfer applications\n- TX-heavy workloads with minimal RX polling\n- Blocking socket applications sending large volumes\n- Applications using performance.threading.internal_handler.behavior=delegate mode\n- Scenarios where maximizing throughput is more important than per-call latency\n- Applications not using epoll for event-driven I/O\n\n**Performance Tradeoffs:**\n\n| Scenario | false (Default) | true |\n|----------|-----------------|------|\n| Per-send() latency | Lower | Higher (adds RX poll overhead) |\n| Bulk transfer throughput | May stall if not polling RX | Better sustained throughput |\n| CPU efficiency | Better (no redundant polling) | May waste cycles polling empty CQ |\n| Send buffer utilization | May fill up in TX-heavy loops | Stays freed via ACK processing |\n| cwnd growth rate | Depends on RX poll frequency | Faster (immediate ACK processing) |\n\n**Interaction with Other Parameters:**\n- performance.threading.internal_handler.behavior: With \"delegate\" mode, enabling rx_poll_on_tx_tcp is often recommended\n- network.protocols.tcp.wmem: Larger send buffers reduce the impact of delayed ACK processing\n- performance.polling.skip_cq_on_rx: Affects overall RX polling behavior\n\n**Typical Use Cases:**\n\n1. **High-throughput file transfer:** Enable (true) - Sustained TX benefits from immediate ACK processing\n2. **Low-latency trading:** Disable (false) - Per-call latency matters more than throughput\n3. **Event-driven web server:** Disable (false) - epoll already handles ACK processing\n4. **Blocking socket bulk sender:** Enable (true) - Prevents send buffer stalls\n5. **Request/response pattern:** Disable (false) - Natural TX/RX interleaving handles ACKs"
                        },
                        "rx_cq_wait_ctrl": {
                            "type": "boolean",
                            "default": false,
                            "title": "RX completion queue wait control",
                            "description": "Maps to XLIO_RX_CQ_WAIT_CTRL environment variable.\nControls when CQ (Completion Queue) channel file descriptors are added to socket internal epoll descriptors.\n\n**Background: How Socket Blocking Works in XLIO**\n\nEach XLIO socket has its own internal epoll file descriptor (m_rx_epfd) used for blocking operations. When a socket performs a blocking receive (recv, recvmsg, etc.), it eventually calls epoll_wait() on this internal epfd to sleep until data arrives.\n\nTo wake up sleeping sockets when packets arrive, XLIO uses CQ (Completion Queue) channel FDs. These are kernel file descriptors that become readable when completions are available on the hardware completion queue. By adding these CQ channel FDs to socket epoll descriptors, sleeping sockets get woken when packets arrive on the shared ring.\n\n**The Problem at Scale**\n\nWhen multiple sockets share the same ring (common with per_interface or per_thread allocation), they all need to watch the same CQ channel FD. The default behavior adds this CQ-fd to EVERY socket's internal epfd permanently.\n\nWith many connections (e.g., 350,000), this creates a serious scalability problem:\n- A single CQ event triggers the kernel to iterate through ALL epfds watching that CQ-fd\n- The kernel performs a linear scan of the waiter list for each event\n- This causes O(N) overhead where N is the number of sockets sharing the ring\n- Results in high CPU usage in kernel space and increased latency\n\n**Value: false (Default) - Permanent CQ-fd Registration**\n\nBehavior:\n- When a ring is associated with a socket, the CQ channel FD is immediately added to that socket's internal epfd via epoll_ctl(EPOLL_CTL_ADD)\n- The CQ-fd remains registered for the lifetime of the socket-ring association\n- All sockets sharing a ring permanently watch the same CQ-fd\n\nBenefits:\n- Simpler code path with fewer system calls\n- No epoll_ctl overhead on each blocking operation\n- Slightly lower latency for individual blocking calls\n- Works well for low to moderate connection counts\n\nDrawbacks:\n- Kernel overhead scales linearly with connection count\n- At 100K+ connections, significant CPU time spent in kernel epoll wakeup code\n- Can cause latency spikes when CQ events wake many waiters\n- Poor scalability for high-connection-count servers\n\nBest for:\n- Applications with fewer than 10,000 connections\n- Scenarios where blocking socket operations are rare\n- Applications using primarily non-blocking I/O with epoll\n- Low connection count with frequent blocking operations\n\n**Value: true - Dynamic CQ-fd Registration (On-Demand)**\n\nBehavior:\n- CQ channel FD is NOT added when ring is associated with socket\n- Instead, CQ-fd is added via epoll_ctl(EPOLL_CTL_ADD) only when socket is about to sleep (epoll_wait)\n- Immediately after waking up, CQ-fd is removed via epoll_ctl(EPOLL_CTL_DEL)\n- Only sockets that are ACTUALLY sleeping watch the CQ-fd at any given time\n\nBenefits:\n- Excellent scalability for high connection counts (100K+)\n- Kernel wakeup overhead proportional to sleeping sockets, not total sockets\n- Much lower kernel CPU usage under high connection loads\n- Dramatically reduced latency at scale\n- Critical optimization for event-driven servers (Nginx, Envoy, HAProxy)\n\nDrawbacks:\n- Two additional system calls per blocking wait (epoll_ctl ADD + DEL)\n- Slight overhead for each blocking operation\n- Overhead is wasted if only a few connections exist\n- Only beneficial when sockets actually perform blocking waits\n\nBest for:\n- High connection count servers (10K+ connections)\n- Event-driven architectures (Nginx, Envoy, HAProxy)\n- Applications where most sockets are idle/not sleeping\n- Scenarios where many sockets share few rings (per_interface, per_thread with limited rings)\n\n**Performance Tradeoffs Summary:**\n\n| Metric | false (Default) | true |\n|--------|-----------------|------|\n| Syscalls per blocking wait | 1 (epoll_wait only) | 3 (ADD + wait + DEL) |\n| Kernel wakeup overhead | O(total_sockets) | O(sleeping_sockets) |\n| 1K connections | Negligible difference | Slight overhead |\n| 100K connections | High kernel CPU, latency spikes | Excellent scalability |\n| 350K connections | Severe performance degradation | Linear performance |\n| Non-blocking sockets | No impact | No impact |\n\n**Automatic Enablement:**\n\nThis parameter is automatically set to true when using:\n- NGINX mode (XLIO_SPEC=nginx)\n- Envoy mode (XLIO_SPEC=envoy)\n\nThese application profiles recognize the high-connection-count nature of these workloads and enable this optimization by default.\n\n**Interaction with Other Parameters:**\n\n- performance.rings.rx.allocation_logic: With per_interface (0) or per_ip_address (1), more sockets share each ring, making this optimization more impactful\n- performance.rings.max_per_interface: Limiting rings increases socket-to-ring sharing ratio\n- performance.polling.count: If polling count is high and sockets rarely sleep, this setting has minimal impact\n\n**When to Change This Setting:**\n\nConsider enabling (true) when:\n- Running a server with 10,000+ concurrent connections\n- Seeing high kernel CPU usage (%sy in top) with many idle connections\n- Experiencing latency spikes correlated with connection count\n- Using ring allocation that results in many sockets per ring\n- Profiling shows time spent in epoll wakeup paths\n\nKeep disabled (false) when:\n- Running with fewer than 10,000 connections\n- Each connection has its own ring (per_socket allocation)\n- Blocking socket operations are very frequent\n- Application doesn't use blocking I/O patterns\n- Per-operation latency is more critical than scalability"
                        },
                        "skip_cq_on_rx": {
                            "oneOf": [
                                {
                                    "type": "integer",
                                    "enum": [
                                        0,
                                        1,
                                        2
                                    ],
                                    "default": 0
                                },
                                {
                                    "type": "string",
                                    "enum": [
                                        "disable",
                                        "enable",
                                        "enable_epoll_only"
                                    ],
                                    "default": "disable"
                                }
                            ],
                            "title": "Skip completion queue checks on RX",
                            "description": "Maps to XLIO_SKIP_POLL_IN_RX environment variable.\nControls whether TCP socket recv() operations skip direct Completion Queue (CQ) polling.\n\n**Background: How Packet Reception Works in XLIO**\n\nWhen a TCP socket calls recv()/read()/recvmsg(), XLIO needs to check for available packets. By default, this involves:\n1. Checking the socket's ready packet list (m_rx_pkt_ready_list)\n2. If no packets ready, polling the hardware Completion Queue (CQ) via poll_and_process_element_rx()\n3. Processing any completed packets from the CQ and moving them to socket ready lists\n4. If still no packets and blocking, sleeping until CQ notification or timeout\n\nCQ polling involves acquiring ring locks, accessing hardware CQ memory, and potentially processing multiple packets. This adds CPU cycles to every recv() call.\n\n**Value: \"disable\" or 0 (Default) - Always Poll CQ in recv()**\n\nBehavior:\n- Every recv()/read() call on a TCP socket polls the CQ directly\n- Socket gets the most up-to-date packet information immediately\n- The is_readable() check (used by select/poll/epoll) also polls the CQ\n\nBenefits:\n- Lowest latency from packet arrival to application delivery\n- Works correctly without epoll or other CQ polling mechanisms\n- Self-contained: each socket handles its own packet reception\n- Best for applications doing direct recv() without event loops\n\nDrawbacks:\n- Higher CPU overhead per recv() call (CQ polling, lock acquisition)\n- Redundant work if epoll is also polling the same CQ\n- Lock contention when multiple sockets share a ring\n- More syscall-like overhead even when packets are already ready\n\nBest for:\n- Applications not using epoll (direct blocking recv patterns)\n- Low-frequency recv() calls where per-call overhead doesn't matter\n- Scenarios requiring lowest possible latency to first byte\n- Applications with few sockets per ring (per_socket allocation)\n- Request-response patterns where each recv() is meaningful\n\n**Value: \"enable\" or 1 - Always Skip CQ Polling in recv()**\n\nBehavior:\n- recv()/read() calls NEVER poll the CQ directly\n- Only checks if packets are already in the ready queue (m_rx_pkt_ready_list)\n- If no packets ready, immediately sets errno=EAGAIN and returns -1 (for non-blocking)\n- For blocking sockets, goes directly to sleep waiting for CQ notification\n- The is_readable() check also skips CQ polling, returning false immediately if no ready packets\n- REQUIRES external CQ polling mechanism (epoll_wait, worker threads, or internal thread)\n\nBenefits:\n- Fastest recv() path: minimal overhead, no CQ polling or lock acquisition\n- Eliminates redundant CQ polling when using epoll\n- Reduces CPU usage for high-frequency recv() calls\n- Better performance for epoll-based event loops\n- Lower lock contention on shared rings\n\nDrawbacks:\n- Packets may not be visible until epoll_wait() or similar polls the CQ\n- Slightly higher latency from packet arrival to availability in recv()\n- Can cause recv() to return EAGAIN even when packets exist in hardware CQ\n- INCORRECT BEHAVIOR if used without epoll: recv() may spin returning EAGAIN\n- Breaks applications that rely on recv() to drive packet processing\n\nBest for:\n- Event-driven servers using epoll_wait() as primary event loop\n- High-frequency recv() patterns (many small reads)\n- Applications where epoll already polls CQs centrally\n- Worker thread mode where background threads poll CQs\n- Reducing CPU usage in recv-heavy workloads\n\n**Value: \"enable_epoll_only\" or 2 - Skip CQ Polling Only When Socket is in epoll**\n\nBehavior:\n- When socket is NOT in any epoll instance: behaves like \"disable\" (0) - polls CQ in recv()\n- When socket IS added to an epoll instance (epoll_ctl ADD): switches to skip CQ polling\n- When socket is removed from epoll (epoll_ctl DEL): switches back to polling CQ\n- Dynamically adapts based on socket's epoll membership\n\nBenefits:\n- Best of both worlds: auto-detects the right behavior\n- Safe default for mixed-mode applications\n- Works correctly even if socket is sometimes used without epoll\n- No need to know application architecture in advance\n- Provides optimization when epoll is used, correctness when not\n\nDrawbacks:\n- Slightly more complex behavior to understand\n- State changes when socket enters/leaves epoll\n- Minor overhead tracking epoll membership\n- May not be optimal for all edge cases\n\nBest for:\n- Applications with unknown or mixed I/O patterns\n- Libraries that don't control how sockets are used\n- Gradual migration from blocking to event-driven I/O\n- When correctness is more important than squeezing last bit of performance\n- Production deployments where safety matters\n\n**Performance Tradeoffs Summary:**\n\n| Scenario | disable (0) | enable (1) | enable_epoll_only (2) |\n|----------|-------------|------------|----------------------|\n| recv() CPU overhead | Higher (CQ poll) | Lowest (no CQ poll) | Adaptive |\n| Latency to first byte | Lowest | Higher (waits for epoll) | Adaptive |\n| Works without epoll | Yes | NO - breaks | Yes |\n| Works with epoll | Yes (redundant work) | Optimal | Optimal |\n| Lock contention | Higher | Lower | Adaptive |\n| Safety | Always correct | Requires epoll | Always correct |\n\n**Detailed Performance Characteristics:**\n\n*CPU Cycles per recv() call (approximate):*\n- disable (0): ~200-500 cycles (CQ poll + lock + memory access)\n- enable (1): ~50-100 cycles (just ready list check)\n- enable_epoll_only (2): Same as (0) or (1) depending on epoll state\n\n*Latency Impact:*\n- With disable (0): Packet available immediately when CQ has completion\n- With enable (1): Packet available after next epoll_wait() polls CQ\n  - If epoll_wait is called frequently (< 1ms): minimal impact\n  - If epoll_wait has long timeout: packets delayed until timeout or other events\n\n*Throughput Impact:*\n- High-frequency small recv() patterns: enable (1) can improve throughput 10-30%\n- Bulk recv() patterns: minimal difference (CQ poll overhead amortized)\n\n**Interaction with Other Parameters:**\n\n- performance.polling.rx_cq_wait_ctrl: Complementary optimization for CQ-fd registration\n- performance.rings.rx.allocation_logic: With per_thread/per_core, fewer sockets share rings, reducing lock benefit\n- performance.threading.worker_threads: Worker threads can poll CQs, making enable (1) safer\n- performance.threading.internal_handler.behavior: delegate mode provides CQ polling\n\n**Recommended Configurations:**\n\n*For Nginx/Envoy/HAProxy (event-driven):*\n- Use \"enable\" (1) or \"enable_epoll_only\" (2)\n- These applications use epoll_wait as their main event loop\n- Benefit from reduced recv() overhead\n\n*For custom applications with epoll:*\n- Use \"enable_epoll_only\" (2) for safety\n- Or \"enable\" (1) if you're certain all sockets are in epoll\n\n*For applications without epoll:*\n- Use \"disable\" (0) - the only correct option\n- Or \"enable_epoll_only\" (2) which auto-detects\n\n*For latency-critical trading applications:*\n- Use \"disable\" (0) to ensure immediate packet visibility\n- Every microsecond of latency from packet arrival to recv() matters\n\n*For high-throughput servers with many connections:*\n- Use \"enable\" (1) with epoll to minimize CPU usage\n- Combine with rx_cq_wait_ctrl=true for best scalability"
                        },
                        "blocking_rx_poll_usec": {
                            "type": "integer",
                            "minimum": -1,
                            "maximum": 100000000,
                            "default": 100000,
                            "title": "RX poll duration (µsec)",
                            "description": "Maps to XLIO_RX_POLL environment variable.\nControls how many times XLIO busy-polls the hardware Completion Queue (CQ) for incoming packets before transitioning to interrupt-driven sleep mode.\n\n**How Packet Reception Works in XLIO:**\n\nWhen an application calls recv(), read(), recvfrom(), or recvmsg() on a blocking socket:\n1. XLIO enters a busy-poll loop, checking the hardware CQ for completed packets\n2. Each poll iteration is very fast (~100-500 nanoseconds) but consumes CPU cycles\n3. If data arrives during polling: immediate return with lowest possible latency\n4. After exhausting the poll budget: XLIO arms CQ interrupts and sleeps via epoll_wait()\n5. When a packet arrives, hardware generates an interrupt, waking the thread\n\nFor non-blocking sockets: Only 1 poll is performed regardless of this setting, then returns -1/EAGAIN if no data is available.\n\n**Value Interpretation:**\n\n- **-1 (Infinite Polling):**\n  Never sleep; pure busy-wait until data arrives.\n  CPU core runs at 100% utilization spinning on CQ.\n  Provides absolute lowest latency (~sub-microsecond to packet availability).\n  Best for: Latency-critical applications with dedicated CPU cores (HFT, real-time systems).\n  Warning: Disables CQ interrupt moderation as interrupts are never needed.\n\n- **0 (Minimal Polling):**\n  Internally treated as 1 - performs at least one CQ poll before sleeping.\n  Effectively interrupt-driven with minimal busy-wait.\n  Lowest CPU usage but highest latency (context switch + interrupt overhead).\n  Best for: CPU-constrained environments, background services.\n\n- **Low Values (1-1,000):**\n  Brief busy-poll window before sleeping.\n  Quick transition to interrupt mode if no data.\n  Latency: ~10-100 microseconds typical (varies by interrupt delivery time).\n  CPU: Very low - thread sleeps most of the time.\n  Best for: Applications with infrequent receives, mixed workloads.\n\n- **Medium Values (1,000-100,000):**\n  Balanced approach between latency and CPU usage.\n  Default (100,000) provides good latency for most applications.\n  With typical poll rate of ~1-10 million polls/second, 100,000 polls ≈ 10-100ms of busy-waiting.\n  Best for: General-purpose servers, web applications.\n\n- **High Values (100,000-100,000,000):**\n  Extended busy-poll window, rarely sleeping.\n  Near-minimum latency with high CPU usage.\n  Thread may appear CPU-bound during idle periods.\n  Best for: Latency-sensitive applications without dedicated cores.\n\n**Performance Tradeoffs:**\n\n| Value Range | Latency | CPU Usage | Interrupt Overhead | Use Case |\n|-------------|---------|-----------|-------------------|----------|\n| -1 (infinite) | Lowest (~sub-μs) | 100% | None | HFT, real-time |\n| 0-100 | Highest (~50-500μs) | Minimal | Every packet | Background tasks |\n| 100-10,000 | High (~10-100μs) | Low | Frequent | Mixed workloads |\n| 10,000-100,000 | Medium (~1-10μs) | Medium | Occasional | General servers |\n| 100,000+ | Low (~sub-μs to μs) | High | Rare | Latency-sensitive |\n\n**Understanding Poll Hit Rate:**\n\nXLIO tracks polling effectiveness via statistics (visible with xlio_stats):\n- n_rx_poll_hit: Packets found during busy-poll phase\n- n_rx_poll_miss: Polling exhausted, had to sleep\n\nHigh poll hit rate (>90%) indicates optimal latency - packets arrive during polling.\nLow poll hit rate (<50%) suggests over-polling - wasting CPU waiting for infrequent data.\n\n**Latency Breakdown by Mode:**\n\n*During Busy-Poll (poll hit):*\n- Packet arrival → recv() returns: ~0.1-1 microsecond\n- Dominated by: CQ poll latency, memory access, packet processing\n\n*After Sleep (poll miss):*\n- Packet arrival → recv() returns: ~10-100+ microseconds typical\n- Dominated by: Interrupt delivery, kernel wakeup, context switch, CQ poll\n- Components: HW interrupt (~1-5μs) + kernel handling (~2-10μs) + context switch (~1-5μs) + epoll_wait return (~1-3μs) + CQ poll (~0.1-0.5μs)\n\n**CPU Utilization Patterns:**\n\n*With -1 (infinite polling):*\n- Thread runs at 100% CPU continuously\n- No context switches during receive wait\n- Ideal: Pin thread to dedicated core with isolcpus\n\n*With high values (>100,000):*\n- CPU usage correlates with receive frequency\n- Frequent receives: Low CPU (data arrives during early polls)\n- Infrequent receives: High CPU (most of poll budget consumed)\n\n*With low values (<1,000):*\n- CPU usage very low regardless of receive frequency\n- Thread spends most time sleeping\n- Context switches on every receive burst\n\n**Interaction with Other Parameters:**\n\n- performance.polling.iomux.poll_usec: Similar control for select()/poll()/epoll_wait()\n- performance.polling.yield_on_poll: Yields CPU periodically during polling (for multi-threaded sharing)\n- performance.polling.rx_cq_wait_ctrl: Optimizes epoll fd registration for sleeping\n- performance.polling.skip_cq_on_rx: Can skip CQ polling entirely if using epoll\n- performance.completion_queue.interrupt_moderation.*: Controls HW interrupt coalescing when sleeping\n\n**Recommended Configurations:**\n\n*Ultra-Low Latency (HFT, Real-time):*\n  blocking_rx_poll_usec = -1\n  + Dedicate CPU cores (isolcpus, taskset)\n  + Disable interrupt moderation\n  + Consider: yield_on_poll = 0\n\n*Low Latency Server (Trading, Gaming):*\n  blocking_rx_poll_usec = 1000000 (1M)\n  + Provides near-infinite polling with eventual sleep\n  + Good balance if cores aren't fully dedicated\n\n*General Purpose Server (Web, API):*\n  blocking_rx_poll_usec = 100000 (default)\n  + Good latency for active connections\n  + Reasonable CPU usage during idle\n\n*CPU-Constrained Environment:*\n  blocking_rx_poll_usec = 1000\n  + Quick transition to interrupt mode\n  + Minimal CPU waste during idle\n  + Accept higher latency tradeoff\n\n*Throughput-Optimized (Bulk Transfer):*\n  blocking_rx_poll_usec = 10000\n  + Latency less critical than efficiency\n  + Let interrupt coalescing batch packets\n\n**Tuning Methodology:**\n\n1. Start with default (100,000)\n2. Monitor with xlio_stats: check poll hit/miss ratio\n3. If poll hit rate >95% and latency acceptable: value is appropriate\n4. If poll hit rate <50%: reduce value to save CPU\n5. If latency too high despite high hit rate: check other bottlenecks\n6. If latency critical: increase toward -1, monitor CPU\n\n**Warning:**\nSetting -1 (infinite) disables CQ interrupt moderation. If your application has periods of no traffic, the polling thread will consume 100% of its CPU core indefinitely. Ensure this is acceptable for your deployment."
                        },
                        "iomux": {
                            "type": "object",
                            "description": "Select/epoll operation settings.",
                            "properties": {
                                "poll_usec": {
                                    "type": "integer",
                                    "minimum": -1,
                                    "maximum": 100000000,
                                    "default": 100000,
                                    "title": "Select/poll duration (µsec)",
                                    "description": "Maps to XLIO_SELECT_POLL environment variable.\nControls how long (in microseconds) XLIO busy-polls the hardware Completion Queue (CQ) for incoming packets during I/O multiplexing calls (select(), poll(), epoll_wait()) before transitioning to interrupt-driven sleep mode.\n\n**How I/O Multiplexing Works in XLIO:**\n\nWhen an application calls select(), poll(), or epoll_wait() with offloaded sockets:\n1. **Scenario 1 (No offloaded FDs):** Goes directly to OS wait - this parameter has no effect.\n2. **Scenario 2 (Polling phase):** XLIO enters a busy-poll loop, checking the hardware CQ for completed packets. The loop continues for up to poll_usec microseconds.\n3. **Scenario 3 (Sleep phase):** If polling exhausts without finding data, XLIO arms CQ interrupts, goes to sleep, and waits for hardware to signal packet arrival.\n\nThe max polling duration is bounded by the lesser of: this parameter OR the timeout provided by the application to select()/poll()/epoll_wait().\n\n**Value Interpretation:**\n\n- **-1 (Infinite Polling):**\n  Never sleep; pure busy-wait until data arrives or application timeout expires.\n  CPU core runs at 100% utilization spinning on CQ during wait.\n  Provides absolute lowest latency (~sub-microsecond to packet detection).\n  Best for: Ultra-low-latency applications with dedicated CPU cores (HFT, real-time trading).\n  Warning: Thread will consume 100% CPU during any wait period.\n  Profile: Used by LATENCY profile for minimum latency.\n\n- **0 (Minimal Polling):**\n  Performs exactly one CQ poll before immediately transitioning to sleep mode.\n  Essentially interrupt-driven operation with minimal busy-wait overhead.\n  Lowest CPU usage but higher latency (interrupt + context switch overhead).\n  Best for: High-concurrency servers (like Nginx) where thousands of connections share few workers, and CPU efficiency matters more than per-connection latency.\n  Profile: Used by NGINX profile for maximum scalability.\n\n- **Low Values (1-10,000 µsec):**\n  Brief busy-poll window (1µs to 10ms) before sleeping.\n  Quick transition to interrupt mode if no immediate data.\n  Latency: ~10-100 microseconds typical (dominated by interrupt delivery when sleeping).\n  CPU: Very low - thread sleeps most of the time during idle periods.\n  Best for: Applications with sporadic traffic, mixed workloads, background services.\n\n- **Medium Values (10,000-100,000 µsec):**\n  Balanced approach between latency and CPU usage.\n  Default (100,000 = 100ms) provides good latency for typical server applications.\n  Allows polling for 10-100ms before giving up and sleeping.\n  Best for: General-purpose servers, web applications, API servers.\n\n- **High Values (100,000-100,000,000 µsec):**\n  Extended busy-poll window (100ms to 100 seconds), rarely sleeping.\n  Near-minimum latency with high CPU usage during idle waits.\n  Thread may appear CPU-bound when waiting with no traffic.\n  Best for: Latency-sensitive applications that can tolerate some CPU waste.\n\n**Performance Tradeoffs:**\n\n| Value Range | Latency (poll hit) | Latency (poll miss) | CPU During Wait | Use Case |\n|-------------|-------------------|---------------------|-----------------|----------|\n| -1 (infinite) | ~0.1-1 µs | N/A (never miss) | 100% | HFT, real-time |\n| 0 (minimal) | ~0.1-1 µs | ~50-500 µs | ~0% | Nginx, high-concurrency |\n| 1-10,000 | ~0.1-1 µs | ~10-100 µs | Very Low | Mixed workloads |\n| 10K-100K | ~0.1-1 µs | ~1-10 µs | Medium | General servers |\n| 100K+ | ~0.1-1 µs | Rare | High | Latency-sensitive |\n\n**Poll Hit vs Poll Miss:**\n\nXLIO tracks polling effectiveness via statistics (visible with xlio_stats):\n- **n_iomux_poll_hit:** Data found during busy-poll phase - lowest latency path\n- **n_iomux_poll_miss:** Polling exhausted without data, had to sleep\n\n*Poll Hit (best case):*\n- Packet arrived during polling phase\n- select()/poll()/epoll_wait() returns in ~0.1-1 microseconds\n- No interrupt overhead, no context switch, no kernel involvement\n\n*Poll Miss (worse case):*\n- Polling budget exhausted without finding data\n- XLIO arms CQ interrupt and goes to sleep via OS epoll_wait()\n- When packet arrives: HW interrupt (~1-5µs) + kernel handling (~2-10µs) + context switch (~1-5µs)\n- Total latency: ~10-100+ microseconds\n\n**CPU Utilization Patterns:**\n\n*With -1 (infinite polling):*\n- Thread runs at 100% CPU continuously during any select()/poll()/epoll_wait() call\n- No context switches during wait periods\n- Ideal: Pin thread to dedicated core with isolcpus/taskset\n- All other threads on the system compete for remaining cores\n\n*With 0 (minimal polling):*\n- Thread sleeps immediately after single CQ poll\n- CPU usage near zero during idle periods\n- High context switch rate when traffic is bursty\n- Excellent for high worker count scenarios (e.g., Nginx with 32+ workers)\n\n*With medium/high values:*\n- CPU usage inversely correlates with traffic rate:\n  - High traffic: Low CPU (data found early in polling)\n  - Low/no traffic: High CPU (most of poll budget consumed before sleeping)\n\n**Interaction with Related Parameters:**\n\n- **performance.polling.blocking_rx_poll_usec:** Similar control but for direct recv()/read() calls. Both should typically be tuned together.\n- **performance.polling.iomux.poll_os_ratio:** How often to poll non-offloaded OS file descriptors during the busy-poll loop. Value of 0 means never poll OS FDs during polling phase.\n- **performance.polling.iomux.skip_os:** How often to skip OS FD checking when offloaded data is found.\n- **performance.completion_queue.interrupt_moderation.*:** Controls HW interrupt coalescing when in sleep mode - affects poll-miss latency.\n\n**Recommended Configurations:**\n\n*Ultra-Low Latency (HFT, Real-time Trading):*\n  poll_usec = -1\n  + Dedicate CPU cores (isolcpus, taskset)\n  + Set blocking_rx_poll_usec = -1 as well\n  + Consider disabling interrupt moderation\n\n*High-Concurrency Server (Nginx, HAProxy):*\n  poll_usec = 0\n  + Maximize worker count efficiency\n  + Reduce CPU waste on idle connections\n  + Accept slightly higher latency for better scalability\n\n*Low Latency Server (Trading, Gaming):*\n  poll_usec = 100000 to 1000000 (100ms-1s)\n  + Good balance of latency and CPU\n  + Rarely sleeps during active trading hours\n  + Sleeps during truly idle periods\n\n*General Purpose Server (Web, API):*\n  poll_usec = 100000 (default)\n  + Good latency for active connections\n  + Reasonable CPU usage during idle\n\n*CPU-Constrained Environment:*\n  poll_usec = 1000 to 10000 (1-10ms)\n  + Quick transition to interrupt mode\n  + Minimal CPU waste during idle\n  + Accept higher latency tradeoff\n\n**Warning for select()/poll() Applications:**\nApplications using select() or poll() (rather than epoll) may have different performance characteristics. XLIO must iterate through all FDs, which scales poorly with FD count. Consider migrating to epoll for better scalability, or use lower poll_usec values to reduce CPU overhead.\n\n**Tuning Methodology:**\n\n1. Start with default (100,000)\n2. Monitor with xlio_stats: check n_iomux_poll_hit / n_iomux_poll_miss ratio\n3. If poll hit rate >95% and latency acceptable: value is appropriate\n4. If poll hit rate <50%: reduce value to save CPU (you're wasting cycles polling when data is rarely ready)\n5. If latency too high during active periods: increase value toward -1\n6. Monitor CPU utilization during both active and idle periods"
                                },
                                "poll_os_ratio": {
                                    "type": "integer",
                                    "minimum": 0,
                                    "default": 10,
                                    "title": "OS file descriptor polling ratio",
                                    "description": "Maps to XLIO_SELECT_POLL_OS_RATIO environment variable.\nControls the ratio between hardware-accelerated offloaded socket polling (via Completion Queue)\nand OS kernel polling (via select/poll/epoll syscalls) for non-offloaded file descriptors.\n\n**How It Works:**\nDuring select()/poll()/epoll_wait(), XLIO enters a polling loop. A countdown counter starts\nat this value and decrements with each CQ poll. When it reaches 0, XLIO performs a\nzero-timeout OS syscall to check non-offloaded FDs, then resets the counter.\n\nThe effective ratio is: 1 OS poll for every N CQ polls, where N = poll_os_ratio.\n\n**Value Meanings:**\n\n- **0 (Disable OS polling):**\n  Only offloaded sockets are polled. Non-offloaded FDs are completely ignored during the\n  polling phase. Use this for pure-offloaded workloads where all important FDs are accelerated.\n  WARNING: Non-offloaded FDs will only be checked when entering blocking mode, causing\n  significant delays or starvation for non-offloaded traffic.\n  Used in MCE_SPEC_29WEST (ultra-low latency) mode.\n\n- **Low values (1-10):**\n  Frequent OS polling. OS FDs are checked often, providing responsive handling of\n  non-offloaded traffic at the cost of more syscall overhead.\n  - Value 1: OS polled every iteration (highest overhead, best non-offloaded responsiveness)\n  - Value 10 (default): OS polled every 10 CQ polls (balanced tradeoff)\n\n- **High values (50-100+):**\n  Infrequent OS polling. Prioritizes offloaded traffic with minimal syscall overhead.\n  - Value 100: OS polled once every 100 CQ polls (used in MCE_SPEC_LATENCY mode)\n  Non-offloaded FDs experience longer delays between checks.\n\n**Performance Tradeoffs:**\n\n*Lower Values (more OS polling):*\n- Better responsiveness for non-offloaded FDs (timers, signals, non-RDMA sockets)\n- Higher syscall overhead (each OS poll is a kernel transition)\n- Slightly higher latency variance for offloaded traffic\n- Better for mixed workloads with both offloaded and non-offloaded FDs\n\n*Higher Values (less OS polling):*\n- Lower syscall overhead, more CPU time spent in userspace\n- Better latency consistency for offloaded traffic\n- Non-offloaded FDs may experience delays proportional to the ratio\n- Better for workloads dominated by offloaded traffic\n\n**Related Parameters:**\n- performance.polling.iomux.skip_os: Controls how often to give OS immediate priority\n  (resets the countdown to 0 every N select/poll calls)\n- performance.polling.iomux.poll_usec: Duration of the entire polling phase\n\n**Recommended Configurations:**\n\n- Mixed workloads (offloaded + non-offloaded FDs): Use default (10) or lower\n- Latency-sensitive pure offloaded workloads: Use 100 or higher\n- Ultra-low latency, offloaded-only: Use 0 (disable OS polling entirely)"
                                },
                                "skip_os": {
                                    "type": "integer",
                                    "minimum": 0,
                                    "default": 4,
                                    "title": "Skip OS polling frequency",
                                    "description": "Maps to XLIO_SELECT_SKIP_OS environment variable.\nControls how often the OS kernel gets priority polling at the START of select()/poll()/epoll_wait() calls.\n\n**How It Works:**\nXLIO maintains a global counter that decrements with each iomux call (select/poll/epoll_wait).\nWhen this counter reaches 0, the OS is polled FIRST (given priority) before checking offloaded\nsockets. The counter then resets to this parameter's value.\n\nOn calls where the counter is > 0, OS polling is deferred according to poll_os_ratio.\n\n**Interaction with poll_os_ratio:**\nThese two parameters work together at different levels:\n- skip_os (this parameter): Controls WHEN OS gets priority across multiple iomux calls\n- poll_os_ratio: Controls the CQ-to-OS polling ratio WITHIN a single polling loop\n\nExample with skip_os=4 and poll_os_ratio=10:\n- Call 1: OS deferred (countdown starts at 10)\n- Call 2: OS deferred (countdown starts at 10)\n- Call 3: OS deferred (countdown starts at 10)\n- Call 4: OS gets priority (countdown starts at 0), then resets to 4\n\n**Value Meanings:**\n\n- **0 (OS always gets priority):**\n  OS is checked FIRST on every iomux call before checking offloaded sockets.\n  Best for: Workloads where non-offloaded FD responsiveness is critical.\n  Tradeoff: Adds syscall overhead on every call, even when offloaded data is waiting.\n\n- **Low values (1-3):**\n  OS gets priority frequently.\n  - Value 1: OS priority every other call\n  - Value 2: OS priority every 3rd call\n  Good for: Mixed workloads with important non-offloaded FDs (timers, signals, pipes).\n  Tradeoff: More frequent OS syscalls at loop start; slightly higher latency for\n  offloaded-only workloads.\n\n- **Default (4):**\n  Balanced approach: OS gets priority every 5th iomux call.\n  Good for: General-purpose workloads with both offloaded and non-offloaded FDs.\n  Provides reasonable responsiveness for non-offloaded FDs without excessive overhead.\n\n- **High values (10-100+):**\n  OS priority is rare.\n  - Value 10: OS priority every 11th call\n  - Value 100: OS priority every 101st call\n  Good for: Workloads dominated by offloaded traffic where non-offloaded FDs are\n  not latency-sensitive.\n  Tradeoff: Non-offloaded FDs may experience noticeable delays between priority checks.\n\n**Performance Tradeoffs:**\n\n*Lower Values (frequent OS priority):*\n- Better initial responsiveness for non-offloaded FDs\n- Higher syscall overhead at the start of iomux calls\n- May add latency when offloaded packets are ready but OS is checked first\n- Reduces the benefit of hardware acceleration for latency-sensitive applications\n\n*Higher Values (infrequent OS priority):*\n- Lower syscall overhead, better offloaded path latency\n- Offloaded sockets are checked first most of the time\n- Non-offloaded FDs may wait longer before being checked with priority\n- Better for pure offloaded workloads\n\n**Use Cases:**\n\n- Event-driven servers with timers and signals: Use low values (2-4)\n- Pure TCP offloaded workloads: Use high values (10+) or rely solely on poll_os_ratio\n- Mixed UDP multicast with control channels: Use default (4)\n- Ultra-low latency trading: Use higher values to minimize syscall jitter\n\n**Related Parameters:**\n- performance.polling.iomux.poll_os_ratio: Fine-grained control within polling loops\n- performance.polling.iomux.poll_usec: Total duration of the polling phase"
                                }
                            }
                        },
                        "yield_on_poll": {
                            "type": "integer",
                            "default": 0,
                            "title": "Yield CPU during RX polling loop",
                            "description": "Maps to XLIO_RX_POLL_YIELD environment variable.\nControls voluntary CPU yielding during the busy-polling phase of receive operations (recv, read, recvfrom, recvmsg), enabling cooperative multitasking when multiple threads share CPU cores.\n\n**Background: The Polling Loop**\n\nWhen XLIO performs a blocking receive operation, it enters a busy-poll loop controlled by performance.polling.blocking_rx_poll_usec. During this loop, the thread continuously polls hardware completion queues (CQs) for incoming packets. This polling is very fast (~100-500ns per iteration) and provides the lowest possible latency when packets arrive during the poll window.\n\nHowever, this busy-polling monopolizes the CPU core. In multi-threaded applications where multiple threads share cores (common when thread count exceeds core count), this can cause thread starvation - other threads waiting for CPU time cannot make progress, potentially missing their own incoming packets.\n\n**How yield_on_poll Works**\n\nThis parameter inserts periodic sched_yield() calls into the polling loop, voluntarily releasing the CPU to allow other threads to run.\n\n*For UDP sockets:*\nYields every N iterations, specifically on iterations N-1, 2N-1, 3N-1, etc.\nExample with value=10: yields on poll iterations 9, 19, 29, 39...\n\n*For TCP sockets:*\nYields on EVERY iteration after the threshold is exceeded.\nExample with value=10: no yield for iterations 0-10, then yields on 11, 12, 13...\n\n**Value Meanings**\n\n- **0 (Disabled - Default):**\n  No yielding occurs during the entire polling phase.\n  \n  *Best for:*\n  - Dedicated cores per thread (thread count <= core count)\n  - Latency-critical applications where every microsecond matters\n  - Applications using CPU affinity (taskset, numactl) to pin threads\n  - Real-time trading, low-latency messaging systems\n  \n  *Characteristics:*\n  - Lowest latency variance (no context switch jitter)\n  - Maximum CPU utilization per thread during polling\n  - Will starve other threads on shared cores\n  - Thread holds CPU for full blocking_rx_poll_usec duration\n\n- **Low values (1-100):**\n  Frequent yielding, maximum fairness.\n  \n  *Best for:*\n  - Heavily oversubscribed systems (many threads per core)\n  - Mixed workloads where non-XLIO threads need CPU time\n  - Development/testing environments with limited cores\n  - Containerized deployments with CPU limits\n  \n  *Characteristics:*\n  - Very frequent context switches during polling\n  - Excellent fairness between threads\n  - Higher latency variance due to context switch overhead (~1-10μs per switch)\n  - Reduced CPU monopolization\n  - Value of 1: yields on nearly every poll iteration (maximum sharing)\n  - Value of 10: yields every ~10 iterations\n\n- **Medium values (100-1000):**\n  Balanced approach between latency and fairness.\n  \n  *Best for:*\n  - Moderately oversubscribed systems (2-4x threads per core)\n  - Applications with mixed latency requirements\n  - General-purpose server workloads\n  \n  *Characteristics:*\n  - Occasional context switches during polling\n  - Good balance between latency and fairness\n  - Value of 100: yields every ~100 iterations\n  - Value of 1000: yields every ~1000 iterations\n\n- **High values (1000+):**\n  Infrequent yielding, latency-focused with minimal fairness.\n  \n  *Best for:*\n  - Near-dedicated core scenarios with occasional sharing needs\n  - Applications where latency is primary but fairness occasionally matters\n  \n  *Characteristics:*\n  - Rare context switches during polling\n  - Most of polling phase runs without interruption\n  - Minimal fairness improvement over disabled (0)\n\n**Performance Impact Analysis**\n\n*Latency Impact:*\nEach sched_yield() can add 1-10 microseconds of delay (varies by OS scheduler, CPU load, and whether another thread is ready to run). With blocking_rx_poll_usec=100000 (default):\n- value=10: up to ~10,000 yields possible, potentially adding 10-100ms total\n- value=100: up to ~1,000 yields possible, potentially adding 1-10ms total\n- value=1000: up to ~100 yields possible, potentially adding 0.1-1ms total\n\n*Throughput Impact:*\nYielding reduces effective polling density, meaning fewer CQ checks per unit time. This can reduce packet processing throughput in high-rate scenarios.\n\n*CPU Impact:*\nYielding reduces per-thread CPU usage, potentially lowering overall system utilization when many threads are polling simultaneously.\n\n**Interaction with Other Parameters**\n\n- **performance.polling.blocking_rx_poll_usec:**\n  Controls total polling iterations. yield_on_poll only has effect while within this polling budget. If blocking_rx_poll_usec=0 (interrupt-driven), yield_on_poll has no effect.\n\n- **performance.rings.rx.allocation_logic:**\n  With per_thread or per_socket ring allocation, each thread polls its own ring. Yielding allows other threads to poll their rings. With per_interface, threads share rings and may benefit more from yielding to reduce lock contention.\n\n- **performance.threading.worker_threads:**\n  Worker thread mode has different polling semantics. This parameter primarily affects POSIX API application threads.\n\n**Recommended Configurations**\n\n| Scenario | Recommended Value |\n|----------|-------------------|\n| Dedicated cores, ultra-low latency | 0 (disabled) |\n| Thread count = core count | 0 (disabled) |\n| Thread count = 2x core count | 500-1000 |\n| Thread count = 4x core count | 100-500 |\n| Thread count >> core count | 10-100 |\n| Containerized with CPU limits | 50-200 |\n| Mixed XLIO/non-XLIO threads | 100-500 |\n\n**Example Scenarios**\n\n*Scenario 1: 8 XLIO threads on 8 cores*\nUse value=0. Each thread has its own core, no contention.\n\n*Scenario 2: 32 XLIO threads on 8 cores*\nUse value=100-500. 4 threads compete per core; yielding prevents starvation while maintaining reasonable latency.\n\n*Scenario 3: Trading application with strict latency SLA*\nUse value=0 with thread pinning (numactl/taskset). Accept CPU monopolization for minimum latency variance.\n\n*Scenario 4: Microservices in Kubernetes with 2 CPU limit*\nUse value=50-100. Aggressive yielding ensures fair CPU sharing within the container's allocation."
                        },
                        "offload_transition_poll_count": {
                            "type": "integer",
                            "default": 0,
                            "minimum": -1,
                            "maximum": 100000000,
                            "title": "Offload transition poll count",
                            "description": "Maps to XLIO_RX_POLL_INIT environment variable.\nControls CQ busy-polling iterations for UDP sockets during the 'transition phase' - the period before flow steering rules are attached via multicast group membership (ADD_MEMBERSHIP) or before the socket has any attached rings/CQs.\n\n**Background: UDP Socket Lifecycle and Polling Modes**\n\nWhen a UDP socket is created, XLIO marks it as potentially offload-capable but doesn't immediately attach hardware flow steering rules. The socket enters a 'transition phase' where:\n1. No completion queue (CQ) is attached yet\n2. The socket may receive unicast traffic or be waiting for ADD_MEMBERSHIP\n3. This parameter controls polling behavior during this phase\n\nOnce ADD_MEMBERSHIP is called (for multicast) or flow steering is established:\n- Hardware CQs are attached to the socket\n- The socket transitions to using performance.polling.blocking_rx_poll_usec for polling\n- This parameter no longer applies\n\n**Important:** For UDP unicast applications that never call ADD_MEMBERSHIP, this parameter controls the ENTIRE lifetime polling behavior, making it critical for unicast UDP performance tuning.\n\n**How the Polling Loop Works**\n\nDuring blocking recv()/recvfrom() calls, XLIO enters a polling loop:\n1. Each iteration polls the hardware CQ via is_readable()\n2. If packets are found, return immediately (lowest latency)\n3. If no packets and iterations exhausted, fall back to interrupt-based wait (epoll_wait on CQ notification channel)\n4. The fallback adds latency but saves CPU\n\nThis parameter sets the iteration count for step 2 during the transition phase.\n\n**Value Meanings**\n\n- **0 (Disabled - Default):**\n  No busy-polling during transition phase. Socket immediately goes to interrupt-driven wait.\n  \n  *Behavior:*\n  - recv() calls skip CQ polling entirely during transition\n  - Falls back directly to epoll_wait() on the CQ notification channel\n  - Packet arrival triggers interrupt -> epoll wakeup -> packet delivery\n  \n  *Best for:*\n  - UDP multicast applications where transition phase is brief (ADD_MEMBERSHIP called early)\n  - Applications that don't expect traffic before multicast group join\n  - CPU-constrained environments where polling overhead is unacceptable\n  - Systems where power efficiency matters more than latency\n  \n  *Characteristics:*\n  - Zero CPU usage during transition phase when waiting\n  - Higher latency for packets arriving during transition (~50-200μs interrupt overhead)\n  - No wasted cycles polling an empty CQ\n  - Ideal when transition phase is just startup/initialization\n\n- **-1 (Infinite Polling):**\n  Busy-poll indefinitely during transition phase. Never fall back to interrupt wait.\n  \n  *Behavior:*\n  - recv() spins continuously polling CQ until packet arrives\n  - Thread never voluntarily yields CPU while waiting\n  - Will poll forever if no packet arrives (use with timeout or non-blocking)\n  \n  *Best for:*\n  - UDP unicast applications requiring absolute minimum latency\n  - Dedicated cores where 100% CPU usage is acceptable\n  - Real-time systems where interrupt latency is unacceptable\n  - Market data feeds, high-frequency trading UDP receivers\n  \n  *Characteristics:*\n  - 100% CPU usage while waiting for packets (one core fully consumed)\n  - Lowest possible latency (~100-500ns poll overhead vs ~50-200μs interrupt)\n  - Can starve other threads/processes on same core\n  - Must use timeout-based recv or risk indefinite blocking\n\n- **Low Values (1-1000):**\n  Brief polling window before falling back to interrupts.\n  \n  *Behavior:*\n  - Quick check if packet is immediately available\n  - Falls back to interrupt wait relatively quickly\n  - Captures 'almost ready' packets with low latency\n  \n  *Best for:*\n  - Applications where packets sometimes arrive during transition\n  - Moderate latency requirements with CPU efficiency\n  - Development/testing environments\n  \n  *Characteristics:*\n  - Low CPU overhead (~0.1-1ms of polling per recv call)\n  - Catches packets that arrive 'just in time' without interrupt delay\n  - Quick fallback for packets that take longer\n  - Value of 100: ~10-50μs of polling before interrupt fallback\n  - Value of 1000: ~100-500μs of polling before interrupt fallback\n\n- **Medium Values (1000-100000):**\n  Substantial polling window, balancing latency and CPU.\n  \n  *Best for:*\n  - UDP unicast applications with moderate traffic rates\n  - Applications where most packets arrive within polling window\n  - Balancing latency optimization with system resource sharing\n  \n  *Characteristics:*\n  - Moderate CPU usage during polling phase\n  - Value of 10000: ~1-5ms of polling\n  - Value of 100000: ~10-50ms of polling (matches default blocking_rx_poll_usec)\n  - Good compromise for general UDP applications\n\n- **High Values (100000-100000000):**\n  Extended polling, approaching infinite behavior.\n  \n  *Best for:*\n  - High-throughput UDP applications with bursty traffic\n  - Scenarios where interrupt latency is problematic but infinite polling is too aggressive\n  - Applications that need 'almost always polling' with eventual timeout\n  \n  *Characteristics:*\n  - High CPU usage during waiting periods\n  - Value of 100000000: ~10-50 seconds of polling (practically infinite for most applications)\n  - Provides emergency fallback while maintaining low-latency focus\n\n**Performance Tradeoffs**\n\n| Value | Latency (transition) | CPU Usage | Best Use Case |\n|-------|---------------------|-----------|---------------|\n| 0 | High (~50-200μs) | Minimal | MC apps, brief transition |\n| 1-100 | Medium-High | Very Low | Quick checks only |\n| 100-1000 | Medium | Low | Occasional transition traffic |\n| 1000-10000 | Low-Medium | Moderate | UDP unicast, balanced |\n| 10000-100000 | Low | High | UDP unicast, latency focus |\n| -1 | Lowest (~100-500ns) | Maximum | Ultra-low latency unicast |\n\n**UDP Unicast vs Multicast Considerations**\n\n*For UDP Multicast Applications:*\n- Transition phase is typically brief (socket created -> ADD_MEMBERSHIP called)\n- Default value of 0 is usually optimal\n- Once ADD_MEMBERSHIP called, blocking_rx_poll_usec takes over\n- Consider non-zero only if expecting traffic between socket() and ADD_MEMBERSHIP\n\n*For UDP Unicast Applications:*\n- This parameter controls polling for the ENTIRE socket lifetime\n- No ADD_MEMBERSHIP is called, so blocking_rx_poll_usec never activates\n- **Critical:** Set this to match your latency requirements\n- Recommended: Match this to your blocking_rx_poll_usec setting for consistency\n- For low-latency unicast, use -1 or high values (100000+)\n\n**Interaction with Other Parameters**\n\n- **performance.polling.blocking_rx_poll_usec:**\n  Takes over after rings are attached (post-ADD_MEMBERSHIP). This parameter only applies during transition.\n\n- **Blocking vs Non-blocking sockets:**\n  For non-blocking sockets, XLIO always performs exactly 1 poll iteration regardless of this setting. This parameter only affects blocking sockets.\n\n- **performance.polling.yield_on_poll:**\n  During the polling loop controlled by this parameter, yield_on_poll can cause periodic CPU yielding to other threads.\n\n**Recommended Configurations**\n\n| Application Type | Recommended Value |\n|-----------------|-------------------|\n| UDP Multicast (normal) | 0 (default) |\n| UDP Multicast (traffic before join) | 1000-10000 |\n| UDP Unicast (low latency) | -1 or 100000 |\n| UDP Unicast (balanced) | 10000-100000 |\n| UDP Unicast (CPU efficient) | 0-1000 |\n| Market data feed receiver | -1 |\n| Generic UDP server | 10000 |\n\n**Example Scenarios**\n\n*Scenario 1: Multicast Video Streaming Receiver*\nValue: 0 (default)\nRationale: Socket calls ADD_MEMBERSHIP immediately after creation. Transition phase is milliseconds. No benefit from polling during transition.\n\n*Scenario 2: Low-Latency UDP Unicast Trading Application*\nValue: -1\nRationale: Never calls ADD_MEMBERSHIP. Needs lowest possible latency. Dedicated cores available. CPU usage is acceptable tradeoff.\n\n*Scenario 3: UDP Logging/Telemetry Collector*\nValue: 1000\nRationale: Latency not critical. Many sockets, CPU efficiency matters. Brief polling catches ready packets, quick fallback otherwise.\n\n*Scenario 4: UDP Unicast with Shared Cores*\nValue: 10000-50000\nRationale: Needs good latency but can't monopolize CPU. Provides substantial polling window while allowing other threads to run."
                        },
                        "max_rx_poll_batch": {
                            "type": "integer",
                            "minimum": 0,
                            "default": 16,
                            "title": "Max RX buffers per poll",
                            "description": "Maps to XLIO_CQ_POLL_BATCH_MAX environment variable.\n\n**What This Does:**\nControls the maximum number of Completion Queue Entries (CQEs) processed in a single poll_and_process_element_rx() call. This directly limits how many received packets XLIO handles before returning control to the application.\n\n**How the Polling Mechanism Works:**\n1. Application calls recv()/recvfrom()/poll()/select()/epoll_wait()\n2. XLIO acquires the ring lock (via trylock) and enters poll_and_process_element_rx()\n3. First, any packets already in the software rx_queue are processed (up to max_rx_poll_batch)\n4. If the limit isn't reached, XLIO polls the hardware Completion Queue in a loop:\n   ```\n   while (rx_polled < max_rx_poll_batch) {\n       buff = poll(status);  // Read next CQE from hardware\n       if (!buff) break;     // CQ is empty\n       ++rx_polled;\n       process_recv_buffer(buff, fd_ready_array);\n   }\n   ```\n5. After polling, GRO aggregations are flushed via m_gro_mgr.flush_all()\n6. Control returns to the application\n\n**Return Value Semantics (for callers):**\n- `max_rx_poll_batch - rx_polled`: How many more packets could have been polled\n- `0`: Batch limit reached - CQ may have more packets (NOT fully drained)\n- `-1`: CQ was empty (no packets to poll)\n- Positive value: CQ drained before reaching limit\n\nThis return value drives the polling loop in UDP/TCP receive paths:\n- UDP rx_wait() loops while return is 0 (not drained) until packets are ready\n- TCP may break early if packets arrive before batch completes\n\n**Value Range:** 1 to 32768\n\n**Value Tradeoffs:**\n\n*Low Values (1-8) - Best for Latency Consistency:*\n\nBenefits:\n- Lower first-packet latency: When CQ has many pending packets, the first packet reaches the application faster since XLIO processes only a small batch before returning.\n- More predictable timing: Smaller processing batches mean more consistent per-call latency. Easier to reason about worst-case response time.\n- Better interleaving: Application code runs more frequently between batches, enabling faster reaction to application-level events (timeouts, signals, other sockets).\n- Tighter GRO aggregation: flush_all() is called more frequently with smaller batches. This limits TCP segment aggregation, which can reduce receive-side latency at the cost of more per-packet overhead.\n- Shorter ring lock hold time: Other threads waiting on the ring lock (if sharing rings) wait less per acquisition.\n\nDrawbacks:\n- Higher per-packet overhead: More poll_and_process_element_rx() calls are needed to drain a CQ with many pending packets. Each call has function call overhead, lock acquisition cost, and GRO flush overhead.\n- Lower maximum throughput: Under sustained high packet rates, the overhead of frequent returns to the application reduces packets-per-second capacity.\n- More lock acquisitions: If the application loops calling recv(), each iteration requires re-acquiring the ring lock.\n- Suboptimal GRO efficiency: TCP segment coalescing is limited since fewer packets accumulate before flush. More individual packets delivered = more application callbacks.\n\nRecommended for:\n- High-frequency trading and low-latency applications needing predictable timing\n- Real-time systems with strict worst-case latency requirements\n- Interactive protocols where responsiveness trumps throughput\n- Applications using select()/poll() that need to react to other FDs quickly\n\n*Medium Values (16-64) - Balanced (Default):*\n\nBenefits:\n- Good balance between latency and throughput for most workloads\n- Reasonable GRO aggregation efficiency\n- Manageable per-call processing time\n- Default value (16) chosen based on typical workload profiling\n\nDrawbacks:\n- May not be optimal for extreme latency or extreme throughput scenarios\n\nRecommended for:\n- General-purpose servers and applications\n- Mixed workloads with varying packet rates\n- When unsure, start with the default\n\n*High Values (128-32768) - Best for Throughput:*\n\nBenefits:\n- Higher sustained throughput: More packets processed per poll call means lower per-packet overhead. Amortizes function call, lock acquisition, and GRO flush costs across many packets.\n- Better CPU cache utilization: Processing many packets in sequence keeps CQ and buffer structures hot in L1/L2 cache. Sequential access patterns are more cache-friendly.\n- More effective GRO aggregation: With more packets processed before flush_all(), TCP Large Receive Offload (software GRO) can aggregate more segments. Larger coalesced packets reduce per-packet application overhead.\n- Fewer lock acquisitions under high load: CQ fully drained in fewer calls, reducing lock contention when multiple threads share rings.\n\nDrawbacks:\n- Higher latency variance (jitter): When CQ has many packets, the first packet must wait while the entire batch is processed. Tail latency increases.\n- Longer ring lock hold time: Other threads waiting for the ring lock must wait longer per acquisition. Can hurt multi-threaded scenarios with ring sharing.\n- Delayed application responsiveness: Application code doesn't run until the batch completes. Timeouts, signals, or other socket events wait longer.\n- No benefit under light load: If CQ drains before reaching the limit, the high value has no effect (CQ naturally empties).\n- Memory locality pressure: Processing many buffers in one pass touches more memory, potentially evicting other hot data from CPU cache.\n\nRecommended for:\n- Bulk data transfer applications (file transfer, streaming)\n- High-throughput servers processing millions of packets per second\n- Single-threaded applications where lock contention isn't a concern\n- Scenarios where maximizing PPS matters more than latency consistency\n\n**Performance Impact Summary:**\n| Metric                    | Low (1-8)      | Default (16)   | High (128+)     |\n|---------------------------|----------------|----------------|------------------|\n| First-packet latency      | Lowest         | Low            | Higher           |\n| Latency std-dev (jitter)  | Lowest         | Low            | Higher           |\n| Throughput (high load)    | Lower          | Good           | Highest          |\n| Per-packet CPU overhead   | Higher         | Balanced       | Lower            |\n| GRO aggregation           | Minimal        | Moderate       | Maximum          |\n| Ring lock hold time       | Shortest       | Medium         | Longest          |\n| CQ drain iterations       | Many           | Moderate       | Few              |\n\n**Interaction with Other Parameters:**\n\n- **performance.polling.blocking_rx_poll_usec:** When blocking on recv(), XLIO polls in a loop for up to this duration. Each iteration processes up to max_rx_poll_batch packets. Lower batch size = more iterations within the timeout.\n\n- **hardware_features.lro (Large Receive Offload):** Software GRO aggregation flushes after each poll batch. Higher max_rx_poll_batch allows more packets to be aggregated before flush, improving coalescing efficiency. Low values limit aggregation benefit.\n\n- **performance.rings.rx.allocation_logic:** With ring-per-thread/socket, lock contention is minimal, so higher batch values have less downside. With shared rings, longer lock hold times from high batch values can hurt other threads.\n\n- **performance.polling.skip_cq_on_rx:** When enabled, recv() may skip CQ polling entirely (relying on epoll to drive polling). This makes max_rx_poll_batch less relevant for direct recv() calls but still affects epoll-driven polling.\n\n- **performance.polling.select_poll.poll_usec:** During select()/poll()/epoll_wait(), XLIO polls rings. Batch size affects how many packets are processed per ring before moving to the next ring or returning.\n\n**Tuning Recommendations:**\n\n1. **Start with default (16)** - Optimal for most workloads\n\n2. **For latency-sensitive applications (HFT, real-time):**\n   - Reduce to 4-8\n   - Combine with ULTRA_LATENCY profile settings\n   - Accept throughput tradeoff for predictable timing\n\n3. **For high-throughput bulk transfer:**\n   - Increase to 64-256\n   - Monitor tail latency to ensure it's acceptable\n   - Combine with high blocking_rx_poll_usec for sustained polling\n\n4. **For multi-threaded with shared rings:**\n   - Keep lower (8-32) to limit lock hold time\n   - Or use per-thread ring allocation to avoid contention entirely\n\n5. **For diagnosing latency issues:**\n   - Try reducing max_rx_poll_batch to see if batch processing causes latency spikes\n   - Profile poll_and_process_element_rx() duration with perf\n\n**Profile-Specific Values:**\n- Default profile: 16\n- LATENCY profile: 16\n- ULTRA_LATENCY profile: May benefit from 4-8 (consider tuning)\n- NGINX profile: 16 (unchanged)\n\n**Monitoring:**\n- No direct xlio_stats counter for batch utilization\n- n_rx_drained_at_once_max in CQ stats shows peak packets processed in one drain_and_process() call (internal thread path)\n- Profile poll_and_process_element_rx() with perf to measure time per call\n- Latency histograms reveal impact of batch processing on tail latency"
                        },
                        "rx_kernel_fd_attention_level": {
                            "type": "integer",
                            "default": 100,
                            "minimum": 0,
                            "title": "RX kernel FD attention level",
                            "description": "Maps to XLIO_RX_UDP_POLL_OS_RATIO environment variable.\n\n**What This Does (UDP Only):**\nControls how often XLIO checks the OS kernel socket for pending data during UDP receive operations (recv, recvfrom, recvmsg, read). This creates a balance between hardware-accelerated CQ polling and attention to traffic that arrives via the OS kernel path.\n\n**The Counter Mechanism:**\nXLIO maintains a per-socket counter (m_rx_udp_poll_os_ratio_counter) that tracks CQ poll iterations:\n1. Each time XLIO polls the Completion Queue, the counter increments\n2. When counter >= rx_kernel_fd_attention_level, XLIO calls ioctl(FIONREAD) on the OS socket\n3. If the OS socket has pending data (poll_os returns 1), XLIO falls back to OS recv\n4. After checking OS, the counter resets to 0\n\nThe effective ratio is: 1 OS check for every N CQ polls, where N = rx_kernel_fd_attention_level.\n\n**When Does Traffic Go to the OS Socket?**\nEven with XLIO offloading active, some UDP traffic may arrive via the OS kernel:\n- Packets that don't match hardware flow steering rules\n- Traffic before multicast membership is established (before IP_ADD_MEMBERSHIP/IPV6_JOIN_GROUP)\n- ICMP-related responses (e.g., ICMP Port Unreachable triggering ECONNREFUSED)\n- Fragmented IP packets that the NIC cannot reassemble\n- Traffic during socket initialization before offload rules are installed\n- Packets when hardware flow table is full\n- Traffic matching XLIO_SPEC rules that force OS handling\n- Edge cases with certain socket options or protocol behaviors\n\n**Value Interpretation:**\n\n- **0 (Disable OS polling):**\n  OS socket is NEVER checked during the active polling phase. Only the hardware CQ is polled.\n  OS packets are only processed when:\n  - Blocking socket enters sleep mode and wakes via epoll\n  - Explicitly triggered by certain socket operations\n  \n  WARNING: Non-offloaded packets may be severely delayed or effectively starved.\n  Use only for pure-offloaded workloads where ALL traffic is guaranteed to be hardware-steered.\n\n- **Low Values (1-10) - Frequent OS Checking:**\n  OS socket checked very frequently relative to CQ polling.\n  - Value 1: OS checked every single CQ poll iteration (maximum OS attention)\n  - Value 10: OS checked every 10th CQ poll iteration\n  \n  Benefits:\n  - Fastest response to non-offloaded traffic\n  - Best for mixed workloads with significant non-offloaded UDP traffic\n  - Essential if application depends on OS-delivered packets (multicast before join, etc.)\n  \n  Drawbacks:\n  - Higher syscall overhead (ioctl call every N polls)\n  - Each OS check is a kernel transition adding ~100-500 nanoseconds\n  - Slightly higher latency variance for offloaded traffic\n  - CPU cycles spent checking often-empty OS socket\n\n- **Medium Values (50-100) - Balanced (Default = 100):**\n  OS socket checked occasionally, prioritizing CQ polling.\n  - Default value (100): OS checked once every 100 CQ poll iterations\n  \n  Benefits:\n  - Low syscall overhead (~1% of CQ poll iterations)\n  - Good latency for offloaded traffic\n  - Still catches non-offloaded traffic within reasonable time\n  \n  Drawbacks:\n  - Non-offloaded packets may wait up to 100 polling cycles before detection\n  - In high-frequency polling scenarios, this delay is typically negligible\n  - In low-traffic scenarios, may delay OS packets noticeably\n\n- **High Values (500+) - Rare OS Checking:**\n  OS socket checked very infrequently.\n  \n  Benefits:\n  - Minimal syscall overhead\n  - Maximum focus on offloaded fast path\n  - Best latency consistency for offloaded traffic\n  \n  Drawbacks:\n  - Significant delays for non-offloaded packets\n  - May appear as packet loss or high latency for OS-delivered traffic\n  - Only suitable when non-offloaded traffic is rare and non-critical\n\n**Performance Tradeoffs Summary:**\n\n| Value | OS Check Frequency | Syscall Overhead | Non-Offloaded Latency | Offloaded Latency |\n|-------|-------------------|------------------|----------------------|-------------------|\n| 0 | Never (disabled) | None | Very High/Infinite | Lowest |\n| 1-10 | Every 1-10 CQ polls | High (~10%+) | Lowest | Slightly Higher |\n| 50-100 | Every 50-100 CQ polls | Low (~1-2%) | Medium | Low |\n| 500+ | Every 500+ CQ polls | Minimal (<0.2%) | High | Lowest |\n\n**Counter Reset Behavior:**\nThe counter is managed to optimize common scenarios:\n- Reset to 0 after successful OS poll (ensures continued OS checking if OS traffic is flowing)\n- Set to threshold value after successful OS read (triggers immediate OS check on next recv)\n- Set to threshold when ring is added (skip initial OS checking until offload is active)\n- Set to threshold after rx_add_ring_cb (start offloaded operation)\n\n**Interaction with Related Parameters:**\n\n- **performance.polling.iomux.poll_os_ratio:** Similar parameter but for select()/poll()/epoll_wait() operations. This parameter (rx_kernel_fd_attention_level) affects direct recv() calls, while poll_os_ratio affects iomux operations. Tune both consistently for coherent behavior.\n\n- **performance.polling.blocking_rx_poll_usec:** Duration of CQ polling before sleeping. With longer polling durations, the OS check ratio has more iterations to work with. Example: 100ms polling at 1µs/iteration = ~100,000 iterations, so ratio of 100 means ~1000 OS checks.\n\n- **performance.polling.offload_transition_poll_count:** For UDP sockets before ADD_MEMBERSHIP, traffic typically goes to OS. This parameter controls polling behavior during that transition phase.\n\n**Recommended Configurations:**\n\n*Pure Offloaded UDP Workload (all traffic hardware-steered):*\n  rx_kernel_fd_attention_level = 0\n  - Disable OS checking entirely\n  - Maximum performance for offloaded path\n  - Only use when certain ALL traffic is offloaded\n\n*Latency-Sensitive Trading/HFT Applications:*\n  rx_kernel_fd_attention_level = 100-500\n  - Minimal OS overhead\n  - Focus on offloaded fast path\n  - Non-critical OS traffic (if any) can tolerate delays\n\n*Mixed UDP Workload (some multicast, some unicast, control traffic):*\n  rx_kernel_fd_attention_level = 10-50\n  - Balance between offloaded and OS traffic\n  - Reasonable response time for both paths\n  - Good for applications with diverse traffic patterns\n\n*Applications Depending on OS-Delivered Traffic:*\n  rx_kernel_fd_attention_level = 1-10\n  - Maximum attention to OS socket\n  - Essential if multicast setup/teardown is frequent\n  - Or if significant traffic doesn't match flow steering\n\n*Nginx/High-Connection Servers:*\n  rx_kernel_fd_attention_level = 0 (via NGINX profile)\n  - Focuses entirely on offloaded path\n  - OS traffic handled through epoll sleep/wake mechanism instead\n\n**Profile-Specific Defaults:**\n- Default profile: 100\n- LATENCY profile: 100\n- NGINX profile: 0 (disabled - relies on epoll for OS traffic)\n- 29WEST profile: 0 (disabled - pure offloaded focus)\n\n**Troubleshooting:**\n\n*Symptom: Non-offloaded UDP packets arrive with high latency or seem lost*\n- Cause: rx_kernel_fd_attention_level too high or 0\n- Solution: Lower the value (e.g., 10-50) to check OS more frequently\n\n*Symptom: Higher than expected syscall rate in perf/strace*\n- Cause: rx_kernel_fd_attention_level too low, causing frequent ioctl calls\n- Solution: Increase the value if OS traffic is rare\n\n*Symptom: Multicast traffic works after JOIN but initial packets are delayed*\n- Cause: Before flow steering is established, packets go to OS\n- Solution: Lower rx_kernel_fd_attention_level during multicast setup phase\n\n**Monitoring:**\n- xlio_stats shows n_rx_poll_os_hit: counts when OS poll found pending data\n- xlio_stats shows n_rx_os_errors: errors during OS polling (should be 0)\n- High n_rx_poll_os_hit with high ratio suggests OS traffic is flowing and may benefit from lower ratio\n- Low n_rx_poll_os_hit suggests ratio could be increased to reduce overhead"
                        }
                    },
                    "additionalProperties": false
                },
                "completion_queue": {
                    "type": "object",
                    "description": "Completion queue behavior settings.",
                    "properties": {
                        "keep_full": {
                            "type": "boolean",
                            "default": true,
                            "title": "Keep completion queue full",
                            "description": "Maps to XLIO_CQ_KEEP_QP_FULL environment variable.\n\n**What This Controls:**\nDetermines how XLIO handles buffer shortages when replenishing the hardware Receive Queue (RQ) after processing received packets.\n\n**Background - The Buffer Flow:**\n1. Hardware NIC receives packets into buffers posted to the Receive Queue (RQ)\n2. CQ Manager polls for completions and processes received packets\n3. After processing, XLIO must replenish the RQ with fresh buffers\n4. Buffers come from the local ring cache (rx_pool) or global buffer pool\n\n**The Debt Mechanism:**\nXLIO tracks 'm_debt' - the count of buffers consumed from RQ that haven't been replenished yet. When buffers become available, the debt is paid back by posting new buffers to the RQ.\n\n**When true (Aggressive Replenishment - DEFAULT):**\n- After EVERY packet completion, XLIO immediately tries to replenish the RQ\n- If no buffers are available from local pool OR global pool:\n  - Re-posts the SAME buffer back to RQ (the packet data is DROPPED)\n  - Increments n_rx_sw_pkt_drops counter (visible in xlio_stats)\n- The RQ is ALWAYS kept at full capacity (maximum WREs)\n\n**When false (Deferred Replenishment):**\n- XLIO accumulates debt rather than forcing immediate replenishment\n- Debt is paid back later when buffers become available or during poll_failed path\n- Packets are NOT dropped unless debt reaches maximum QP size (m_debt >= rx_num_wr)\n- Compensation is batched, reducing per-packet CPU overhead\n\n**Performance Tradeoffs:**\n\n*true (Aggressive):*\n- PROS:\n  - QP always has maximum capacity to receive packets\n  - Better for bursty workloads where max receive capacity is critical\n  - Minimizes risk of hardware-level RQ starvation\n- CONS:\n  - Drops packets when buffers are scarce (visible in xlio_stats)\n  - Higher CPU overhead per packet (compensation runs on every poll)\n  - Can cause application-visible packet loss under memory pressure\n\n*false (Deferred):*\n- PROS:\n  - Lower CPU overhead - compensation is batched\n  - No packet drops unless QP completely drains\n  - Lower latency jitter (less work per packet)\n  - Better for latency-sensitive applications\n- CONS:\n  - QP may temporarily have fewer available WREs\n  - Could lead to HW packet drops if sustained traffic exceeds replenishment rate\n  - Requires careful sizing of spare_buffers and rx_num_wr\n\n**Recommended Settings:**\n- High-frequency trading / ultra-low latency: false\n- Throughput-focused bulk transfer: true\n- Memory-constrained environments: false (avoids panic-mode drops)\n- Bursty traffic patterns: true (ensures maximum receive capacity)\n\n**Interaction with Related Parameters:**\n- performance.rings.rx.spare_buffers: Larger values reduce frequency of global pool access, mitigating keep_full=true drops\n- performance.rings.rx.post_batch_size: Controls batching threshold; larger values work better with keep_full=false\n- memory.buffers.rx_pool_size: Larger pools reduce buffer shortages overall\n\n**Monitoring:**\nWhen keep_full=true causes drops, monitor via xlio_stats:\n- 'SW RX Packets dropped' counter increments for each re-posted buffer\n- Use 'xlio_stats -p <pid>' to observe drop rates under load\n\n**Note:** Latency-optimized profiles (29WEST, LATENCY, NVME_BF3) automatically set this to false."
                        },
                        "periodic_drain_msec": {
                            "type": "integer",
                            "default": 10,
                            "minimum": 0,
                            "title": "Periodic drain interval (msec)",
                            "description": "Maps to XLIO_PROGRESS_ENGINE_INTERVAL environment variable.\n\n**What This Controls:**\nThe interval in milliseconds at which XLIO's internal thread periodically polls all Completion Queues (CQs) to drain received packets, independent of application socket API calls.\n\n**Background - Why This Matters:**\nXLIO operates as a user-space TCP/IP stack that requires execution context to make progress. Normally, the application provides this context by calling socket APIs (recv, send, poll, etc.). However, when the application is:\n- Busy with computation and not calling socket APIs\n- Blocked on non-socket operations\n- Processing other work in a single-threaded model\n\n...the TCP stack cannot progress without an external trigger. This parameter provides that trigger through XLIO's internal thread.\n\n**What Happens During Periodic Drain:**\n1. Internal thread wakes up every N milliseconds\n2. Iterates through ALL network devices and their rings\n3. For each ring, attempts to acquire the ring lock (trylock - non-blocking)\n4. If lock acquired, polls the CQ and processes completions:\n   - TCP packets: Processed immediately through the TCP stack (ACK handling, window updates, data delivery to socket buffers)\n   - UDP packets: Queued for later application retrieval\n5. Replenishes QP buffers if needed\n6. Releases lock and moves to next ring\n\n**TCP-Specific Impact:**\nThe periodic drain is CRITICAL for TCP because:\n- **ACK Processing**: Incoming ACKs must be processed to advance the send window and allow more data transmission\n- **Retransmission Timers**: Delayed ACK detection and retransmit triggers depend on stack progress\n- **Window Updates**: Receive window advertisements require processing incoming data\n- **Keepalive/Timers**: Various TCP timers need the stack to progress\n\nWithout periodic drain (and without app-driven polling), TCP connections can stall, exhibit poor throughput, or timeout.\n\n**Value Tradeoffs:**\n\n*Low Values (1-10 ms):*\n- PROS:\n  - Fast TCP progress even when app is busy\n  - Lower retransmission latency (problems detected sooner)\n  - Better throughput for apps with irregular socket access patterns\n  - More responsive ACK processing\n- CONS:\n  - Higher CPU overhead from internal thread\n  - More frequent lock contention with application threads\n  - Potential latency jitter if internal thread preempts app\n\n*High Values (50-100+ ms):*\n- PROS:\n  - Lower CPU utilization by internal thread\n  - Less lock contention with application\n  - Suitable when app provides regular execution context\n- CONS:\n  - TCP may stall if app doesn't call socket APIs frequently\n  - Longer time to detect and retransmit lost packets\n  - Throughput degradation for bursty/irregular access patterns\n  - Delayed ACK processing can hurt sender's throughput\n\n*Disabled (0):*\n- PROS:\n  - Zero CPU overhead from background CQ polling\n  - No lock contention from internal thread\n  - Maximum determinism for latency-critical apps\n- CONS:\n  - TCP ONLY progresses when app calls socket APIs\n  - App MUST poll sockets frequently (busy-poll or tight event loop)\n  - Unsuitable for apps with irregular socket access\n\n**Recommended Settings:**\n\n- **General purpose / Default**: 10 ms (balance of responsiveness and overhead)\n- **Latency-sensitive with frequent polling**: 50-100 ms (app already provides context)\n- **Throughput-focused bulk transfer**: 5-10 ms (ensures ACKs are processed promptly)\n- **Single-threaded ultra-low-latency**: 0 (disabled) - app guarantees frequent polling\n- **Event-driven servers (nginx, HAProxy)**: 0 (disabled) - workers have their own polling loops\n- **Background/batch processing apps**: 10-20 ms (ensures TCP progress during computation phases)\n\n**Profiles That Modify This:**\n- ultra_latency: Sets to 0 (disabled) - assumes app provides execution context\n- latency: Sets to 100 ms (reduced overhead, app expected to poll)\n- nginx/nginx_dpu: Sets to 0 (disabled) - workers poll in their event loops\n- nvme_bf3: Sets to 0 (disabled) - specialized storage workload\n- Worker Threads mode: Automatically disabled (XLIO's worker threads provide context)\n\n**Interaction with Related Parameters:**\n- performance.threading.internal_handler.timer_msec: Base wakeup interval of internal thread. periodic_drain_msec should be >= timer_msec for predictable behavior.\n- network.protocols.tcp.timer_msec: TCP timer resolution. Should be coordinated - if TCP timers are 100ms, drain interval > 100ms may delay timer processing.\n- performance.completion_queue.periodic_drain_max_cqes: Limits CQEs processed per drain. Lower values reduce time spent in drain but may leave work undone.\n- performance.completion_queue.keep_full: When true, periodic drain helps maintain QP fullness even when app is idle.\n\n**Monitoring:**\nUse xlio_stats to observe drain behavior:\n- 'CQ Drain count' shows how often periodic drain runs\n- If TCP throughput is poor with app idle periods, try lower values\n- If CPU utilization is high on internal thread, try higher values or disable\n\n**Important Note:**\nIf CQ was already drained by application socket API calls within the interval, the internal thread simply goes back to sleep without additional processing. This makes the mechanism safe and non-redundant - it only does work when the application hasn't."
                        },
                        "periodic_drain_max_cqes": {
                            "type": "integer",
                            "default": 10000,
                            "minimum": 0,
                            "title": "Max CQEs per periodic drain",
                            "description": "Maps to XLIO_PROGRESS_ENGINE_WCE_MAX environment variable.\n\n**What This Controls:**\nLimits the maximum number of Completion Queue Entries (CQEs) processed in a SINGLE periodic drain operation by XLIO's internal thread. This parameter works together with periodic_drain_msec to control background CQ draining behavior.\n\n**Important: Application Path is NOT Affected**\nThis limit ONLY applies to the internal thread's drain_and_proccess() function. Application socket API calls (recv, recvfrom, poll, select, epoll_wait) use poll_and_process_element_rx() which has its own separate limit controlled by performance.polling.max_rx_poll_batch (default 16). Applications are never restricted by this parameter.\n\n**How the Drain Mechanism Works:**\n1. Timer fires every periodic_drain_msec milliseconds\n2. Internal thread iterates through ALL network devices and their rings\n3. For each ring, acquires ring lock (trylock - non-blocking)\n4. Polls CQ in a loop until EITHER:\n   - CQ is empty (no more completions available), OR\n   - Number of processed CQEs reaches periodic_drain_max_cqes limit\n5. Releases ring lock, moves to next ring\n\nThe polling loop logic:\n```\nwhile (progress_engine_wce_max > ret_total) {\n    buff = poll(status);  // Get next CQE from hardware\n    if (!buff) break;     // CQ empty, exit loop\n    process_completion(buff);\n    ret_total++;\n}\n```\n\n**Value Tradeoffs:**\n\n*High Values (5000-10000+, Default is 10000):*\n- PROS:\n  - Drains large packet backlogs in a single timer fire\n  - Efficient for throughput workloads - fewer wakeups needed\n  - Better TCP progress when app is busy (ACKs, window updates processed)\n  - Reduces number of incomplete drains requiring another timer interval\n- CONS:\n  - Internal thread may hold ring lock for extended periods (milliseconds under heavy load)\n  - Can cause latency spikes for application threads waiting for ring lock\n  - Higher CPU burst when processing large batches\n  - Less predictable timing behavior\n\n*Moderate Values (500-2000):*\n- PROS:\n  - Balanced approach between throughput and latency\n  - Ring lock held for shorter periods\n  - More predictable CPU usage patterns\n- CONS:\n  - May require multiple timer fires to clear large backlogs\n  - Slightly reduced efficiency for high-throughput scenarios\n\n*Low Values (50-200):*\n- PROS:\n  - Internal thread releases ring lock quickly\n  - Minimal latency impact on application threads\n  - Very predictable, bounded CPU usage per drain\n  - Best for ultra-low-latency applications\n- CONS:\n  - Multiple timer intervals needed to clear any significant backlog\n  - TCP progress may be slower under heavy load\n  - More timer overhead (frequent wakeups with little work done)\n\n*Value of 0 (Disables Periodic Draining):*\n- When set to 0, the periodic drain timer is NOT registered at all\n- Same effect as setting periodic_drain_msec to 0\n- Application MUST provide all execution context via socket API calls\n- Suitable only for applications with guaranteed frequent polling\n\n**Recommended Settings by Workload:**\n\n- **General purpose / Default**: 10000\n  Works well for most scenarios, ensures backlogs are cleared efficiently.\n\n- **High-frequency trading / Ultra-low latency**: 100-500\n  Minimizes lock hold time, reduces jitter from internal thread.\n  Pair with periodic_drain_msec=0 if app already provides frequent context.\n\n- **Throughput-focused bulk transfer**: 10000-50000\n  Maximize work per wakeup, minimize timer overhead.\n\n- **Mixed workload with latency sensitivity**: 1000-2000\n  Balance between clearing backlogs and limiting lock contention.\n\n- **Event-driven servers (nginx, HAProxy)**: 0 or 500\n  Workers have their own polling loops; internal thread should be minimal.\n\n**Interaction with Related Parameters:**\n\n- **periodic_drain_msec**: These two work as a pair. High interval with low max_cqes means infrequent but limited drains. Low interval with high max_cqes means frequent drains that can process more. Setting either to 0 disables periodic draining entirely.\n\n- **performance.polling.max_rx_poll_batch**: Controls application-path CQ polling limit (default 16). This is the limit that matters for application latency, NOT periodic_drain_max_cqes.\n\n- **performance.threading.worker_threads**: In Worker Threads mode, periodic draining is automatically disabled - the worker threads provide continuous context.\n\n**Monitoring:**\nUse xlio_stats to observe drain behavior:\n- 'Drained max' shows the maximum CQEs processed in a single drain operation\n- If 'Drained max' consistently equals periodic_drain_max_cqes, consider increasing the limit\n- If 'Drained max' is always very low, the limit may be unnecessarily high\n\n**Performance Impact Analysis:**\n\nLock hold time calculation (approximate):\n- Processing one CQE takes roughly 100-500 nanoseconds\n- With periodic_drain_max_cqes=10000, worst-case lock hold: 1-5 milliseconds\n- With periodic_drain_max_cqes=100, worst-case lock hold: 10-50 microseconds\n\nFor applications where every microsecond matters, reducing this value trades throughput efficiency for latency predictability."
                        },
                        "rx_drain_rate_nsec": {
                            "type": "integer",
                            "default": 0,
                            "minimum": 0,
                            "title": "RX drain rate (nsec)",
                            "description": "Maps to XLIO_RX_CQ_DRAIN_RATE_NSEC environment variable.\n\n**IMPORTANT: This parameter applies ONLY to UDP sockets. TCP sockets are not affected.**\n\n**Understanding the Receive Path Architecture:**\n\nWhen packets arrive at the NIC, they flow through these stages:\n1. NIC hardware receives the packet and posts a Completion Queue Entry (CQE) to the hardware CQ\n2. XLIO polls the CQ and moves completed packets to the socket's Ready Packet Queue\n3. Application calls recv()/recvfrom() and retrieves packets from the Ready Packet Queue\n\nThe key question this parameter answers: When the application calls recv() and there are already packets waiting in the Ready Packet Queue, should XLIO first check the CQ for newer packets, or immediately return what's already available?\n\n**When Disabled (value = 0, Default):**\n\nIf packets exist in the Ready Packet Queue, XLIO returns them immediately WITHOUT polling the CQ.\nThe CQ is only polled when the Ready Packet Queue is empty.\n\n```\nis_readable() logic:\n  if (ready_packet_count > 0) {\n    return true;  // Immediate return, no CQ poll\n  }\n  poll_cq();  // Only poll when queue is empty\n```\n\n*Benefits:*\n- Fastest possible latency - immediate return to application\n- Minimal CPU overhead - no unnecessary CQ polling\n- Predictable, deterministic behavior\n\n*Drawbacks:*\n- Application sees a potentially stale view of available data\n- Newer packets may be waiting in CQ but not yet visible to the application\n- In multi-threaded scenarios, one thread may not see packets that arrived during another thread's processing\n\n**When Enabled (value > 0):**\n\nEven when packets exist in the Ready Packet Queue, XLIO will poll the CQ if enough time has passed since the last poll. The value specifies the MINIMUM time interval (in nanoseconds) between forced CQ polls.\n\n```\nis_readable() logic:\n  if (ready_packet_count > 0) {\n    if (time_since_last_poll < rx_drain_rate_nsec) {\n      return true;  // Too soon, skip CQ poll\n    }\n    // Enough time passed, poll CQ for fresh data\n    update_last_poll_timestamp();\n  }\n  poll_cq();\n```\n\n*Benefits:*\n- More accurate, 'real-time' view of available packets\n- Better behavior in multi-threaded applications\n- Catches packets that arrived during previous processing\n- Useful for applications that need to know the 'true' queue depth\n\n*Drawbacks:*\n- Additional CPU cycles spent polling CQ even when data is available\n- Slightly higher latency due to CQ polling overhead\n- More variable latency (jitter) depending on CQ state\n\n**Critical Implementation Detail: Global Timestamp**\n\nThe last-poll timestamp (g_si_tscv_last_poll) is a GLOBAL variable shared across ALL UDP sockets in the process. This means:\n- If Socket A polls the CQ, it resets the timer for ALL sockets including Socket B\n- With many active UDP sockets, this naturally rate-limits CQ polling across the process\n- The actual polling frequency depends on overall socket activity, not just one socket\n\n**Value Tradeoffs:**\n\n*Lower Values (100-500 nsec):*\n- More frequent CQ polling\n- More accurate queue state visibility\n- Higher CPU overhead\n- Better for: Real-time data feeds, market data applications, multi-producer scenarios\n\n*Moderate Values (500-2000 nsec):*\n- Balanced polling frequency\n- Good accuracy with reasonable CPU overhead\n- Better for: General-purpose UDP applications with moderate latency requirements\n\n*Higher Values (2000-5000 nsec):*\n- Less frequent CQ polling\n- Lower CPU overhead\n- May miss recently-arrived packets until next poll\n- Better for: Throughput-focused applications, less latency-sensitive workloads\n\n*Disabled (0, Default):*\n- No forced CQ polling when data is available\n- Lowest latency, lowest CPU overhead\n- Better for: Single-threaded applications, latency-critical paths, applications that drain sockets completely\n\n**Recommended Settings by Use Case:**\n\n- **Ultra-low latency trading (single-threaded)**: 0\n  Immediate return is more important than queue accuracy.\n\n- **Market data with multiple handlers**: 100-500\n  Multiple threads need accurate view of incoming data.\n\n- **Multi-threaded UDP server**: 500-1000\n  Balance between responsiveness and thread coordination.\n\n- **Streaming/media applications**: 1000-2000\n  Moderate accuracy needs, CPU efficiency important.\n\n- **Logging/monitoring collectors**: 2000-5000 or 0\n  Either don't care about real-time accuracy, or want absolute minimum latency.\n\n**Interaction with Related Parameters:**\n\n- **performance.polling.blocking_rx_poll_usec**: Controls how long to spin-poll before sleeping. rx_drain_rate_nsec affects EACH poll iteration within that spinning.\n\n- **performance.polling.rx_kernel_fd_attention_level**: Controls ratio of CQ polls to OS fd polls. rx_drain_rate_nsec is checked on every is_readable() call regardless of this ratio.\n\n- **performance.completion_queue.periodic_drain_msec**: The internal thread's drain interval. This is independent of rx_drain_rate_nsec which only affects application-thread polling.\n\n**Performance Impact Analysis:**\n\nTypical CQ poll overhead: 50-200 nanoseconds (when CQ is empty)\nWith rx_drain_rate_nsec=100, worst case: one extra 50-200ns poll per recv() call\nWith rx_drain_rate_nsec=5000, extra poll at most every 5 microseconds of activity\n\nFor applications making millions of recv() calls per second, even small values can add measurable CPU overhead. For applications with lower call rates, the impact is negligible.\n\n**When to Enable This Parameter:**\n\n1. Multi-threaded UDP applications where threads share socket access\n2. Applications that need accurate queue depth information\n3. Scenarios where packets arrive in bursts and you need to see all of them quickly\n4. Debugging/monitoring where you need to verify packet arrival timing\n\n**When to Keep Disabled (0):**\n\n1. Single-threaded applications with exclusive socket access\n2. Ultra-low latency requirements where every nanosecond counts\n3. Applications that always drain sockets completely before returning\n4. High-frequency recv() patterns where CQ will be naturally polled often\n\nRecommended value range: 100-5000 (nsec) when enabled."
                        },
                        "interrupt_moderation": {
                            "type": "object",
                            "description": "Interrupt moderation (also known as interrupt coalescing) controls when the NIC triggers CPU interrupts after packet completions.\n\n**How It Works:**\nWithout moderation, the NIC generates an interrupt for every packet - low latency but high CPU overhead.\nWith moderation, the NIC batches completions and interrupts only when:\n  - A specified number of packets accumulate (count threshold), OR\n  - A specified time period elapses (period threshold)\n\n**The Fundamental Tradeoff:**\n- More interrupts = lower latency, higher CPU usage\n- Fewer interrupts = higher latency, lower CPU usage\n\n**When to Enable (default):**\n- High-throughput bulk transfers (large files, streaming)\n- CPU-constrained environments\n- Applications that can tolerate some latency for better efficiency\n\n**When to Disable:**\n- Ultra-low-latency requirements (HFT, real-time systems)\n- Small packet, request/response workloads where latency directly impacts throughput\n- When CPU is not a bottleneck\n\n**Performance Impact:**\nBenchmarks show up to 18% throughput difference between enabled/disabled for latency-sensitive workloads (e.g., Nginx serving 0-1KB files).",
                            "properties": {
                                "enable": {
                                    "type": "boolean",
                                    "default": true,
                                    "title": "Enable interrupt moderation",
                                    "description": "Maps to XLIO_CQ_MODERATION_ENABLE environment variable.\nMaster switch for Completion Queue (CQ) interrupt moderation.\n\n**What This Controls:**\nWhen enabled, the NIC hardware batches completion notifications and generates\ninterrupts based on count/period thresholds rather than per-packet.\n\n**When true (default):**\n- Hardware coalesces interrupts using packet_count and period_usec thresholds\n- Adaptive algorithm (if enabled) dynamically adjusts these thresholds\n- Reduces CPU interrupt overhead at the cost of some latency\n- Best for: throughput-oriented workloads, large transfers, CPU-constrained systems\n\n**When false:**\n- Each packet completion immediately triggers an interrupt\n- Lowest possible latency - packets processed as soon as they arrive\n- Higher CPU utilization due to interrupt processing overhead\n- Best for: ultra-low-latency requirements, HFT, small request/response workloads\n\n**Performance Tradeoffs:**\n- Enabled: Better CPU efficiency, ~15-20% lower CPU usage, but 50-1000µs added latency\n- Disabled: Lowest latency, but up to 20% higher CPU usage\n\n**Note:** Setting either performance.polling.blocking_rx_poll_usec or performance.polling.iomux.poll_usec to infinite (-1) will automatically disable moderation, as the application is already polling continuously."
                                },
                                "packet_count": {
                                    "type": "integer",
                                    "default": 48,
                                    "title": "Packet count threshold",
                                    "description": "Maps to XLIO_CQ_MODERATION_COUNT environment variable.\nNumber of packet completions to accumulate before generating an interrupt.\nThis is the initial value used when moderation is first enabled, and the fallback value\nwhen adaptive moderation detects no traffic.\n\n**How It Works:**\nThe NIC generates an interrupt when EITHER this count is reached OR the period_usec expires,\nwhichever comes first.\n\n**Low Values (1-16):**\n- Interrupts trigger more frequently\n- Lower latency - packets processed sooner\n- Higher CPU overhead - more interrupt processing\n- Better for: latency-sensitive workloads, small packets, request/response patterns\n- Example: value=4 means interrupt after every 4 packets (or period timeout)\n\n**High Values (64-500):**\n- Interrupts trigger less frequently\n- Higher latency - packets wait longer before processing\n- Lower CPU overhead - better batching efficiency\n- Better for: high-throughput bulk transfers, large file downloads, streaming\n- Example: value=256 means batch up to 256 packets per interrupt\n\n**Default (48):**\nBalanced starting point, but adaptive algorithm will adjust based on traffic.\n\n**Interaction with period_usec:**\nInterrupt triggers on FIRST condition met:\n- If count reached before period: interrupt on count\n- If period expires before count: interrupt on period\n\n**Maximum Value:**\nAutomatically capped at half of RX work queue size to prevent queue overflow."
                                },
                                "period_usec": {
                                    "type": "integer",
                                    "default": 50,
                                    "title": "Moderation period (µsec)",
                                    "description": "Maps to XLIO_CQ_MODERATION_PERIOD_USEC environment variable.\nMaximum time in microseconds to hold a packet before generating an interrupt.\nThis is the initial value used when moderation is first enabled, and the fallback value\nwhen adaptive moderation detects no traffic.\n\n**How It Works:**\nThe NIC generates an interrupt when EITHER this period expires OR the packet_count threshold\nis reached, whichever comes first. This ensures packets are never held indefinitely.\n\n**Low Values (10-50 µsec):**\n- Maximum wait time of 10-50 microseconds\n- Ensures timely processing even under light traffic\n- Higher interrupt rate under sparse traffic\n- Better for: latency-sensitive applications, trading, gaming\n- Example: value=25 means packets wait at most 25µs\n\n**High Values (100-1000 µsec):**\n- Allows packets to accumulate for up to 1ms\n- Better batching under moderate traffic\n- Lower interrupt rate, better CPU efficiency\n- Risk: noticeable latency under light/bursty traffic\n- Better for: bulk transfers, streaming, high-throughput scenarios\n- Example: value=500 means packets may wait up to 500µs (0.5ms)\n\n**Default (50 µsec):**\nProvides sub-100µs worst-case latency while allowing some batching.\n\n**Relationship to Latency:**\nThis value represents the MAXIMUM additional latency introduced by moderation\nunder light traffic conditions. Under heavy traffic, packet_count is usually\nreached first, so actual latency is lower.\n\n**Interaction with packet_count:**\nThe period acts as a safety net - even if traffic is too light to reach packet_count,\nan interrupt is guaranteed within this period.\n\n**For Ultra-Low-Latency:**\nConsider disabling moderation entirely (enable=false) rather than using very low values,\nas even small periods add measurable latency."
                                },
                                "adaptive_count": {
                                    "type": "integer",
                                    "default": 500,
                                    "title": "Adaptive moderation count threshold",
                                    "description": "Maps to XLIO_CQ_AIM_MAX_COUNT environment variable.\nMaximum packet count threshold the adaptive algorithm can set.\n\n**What This Controls:**\nThe adaptive interrupt moderation (AIM) algorithm dynamically adjusts moderation\nparameters based on observed traffic. This value caps how high the count can go.\n\n**How Adaptive Moderation Works:**\nEvery adaptive_change_frequency_msec (default: 1000ms), XLIO:\n1. Measures packets received in the interval\n2. Calculates the packet rate (packets/second)\n3. Computes count = packet_rate / adaptive_interrupt_per_sec\n4. Clamps count to [1, adaptive_count] range\n5. Updates hardware CQ moderation settings\n\n**Low Values (50-200):**\n- Limits maximum batching\n- More frequent interrupts even under heavy traffic\n- Better latency consistency at high traffic rates\n- Higher CPU usage under load\n- Better for: mixed workloads, latency-sensitive applications\n\n**High Values (500-1000):**\n- Allows aggressive batching under heavy traffic\n- Fewer interrupts during traffic bursts\n- Better CPU efficiency for bulk transfers\n- Risk: higher latency spikes during traffic bursts\n- Better for: throughput-oriented workloads, streaming, bulk transfers\n\n**Default (500):**\nAllows substantial batching while keeping burst latency bounded.\n\n**Maximum Value:**\nAutomatically capped at half of RX work queue size.\n\n**Interaction with adaptive_interrupt_per_sec:**\ncount = packet_rate / target_interrupt_rate\nHigher adaptive_interrupt_per_sec → lower count values\nHigher adaptive_count → allows count to grow during traffic bursts"
                                },
                                "adaptive_period_usec": {
                                    "type": "integer",
                                    "default": 1000,
                                    "title": "Adaptive moderation period (µsec)",
                                    "description": "Maps to XLIO_CQ_AIM_MAX_PERIOD_USEC environment variable.\nMaximum period (in microseconds) the adaptive algorithm can set.\n\n**What This Controls:**\nCaps the maximum hold time the adaptive algorithm will configure.\nPrevents the algorithm from setting periods so long that latency becomes unacceptable.\n\n**How It's Used:**\nThe adaptive algorithm calculates:\nperiod = (1,000,000 / target_interrupt_rate) - (1,000,000 / packet_rate)\nThis is then clamped to [0, adaptive_period_usec].\n\n**Low Values (100-500 µsec):**\n- Maximum latency bounded to sub-millisecond\n- More predictable latency under varying traffic\n- May cause unnecessary interrupts under moderate traffic\n- Better for: latency-sensitive applications, real-time systems\n\n**High Values (1000-5000 µsec):**\n- Allows up to 1-5ms batching window\n- Better CPU efficiency for bursty workloads\n- Risk: occasional latency spikes of several milliseconds\n- Better for: batch processing, non-real-time throughput workloads\n\n**Default (1000 µsec = 1ms):**\nAllows adaptive algorithm flexibility while keeping worst-case latency under 1ms.\n\n**Latency Implications:**\nThis value represents the worst-case additional latency the adaptive algorithm\ncan introduce. Under steady traffic, actual latency is typically much lower\nas count thresholds are reached before period expiry."
                                },
                                "adaptive_change_frequency_msec": {
                                    "type": "integer",
                                    "default": 1000,
                                    "title": "Adaptive change frequency (msec)",
                                    "description": "Maps to XLIO_CQ_AIM_INTERVAL_MSEC environment variable.\nInterval in milliseconds between adaptive moderation recalculations.\n\n**What This Controls:**\nHow frequently XLIO analyzes traffic patterns and adjusts moderation parameters.\nA periodic timer triggers the adapt_cq_moderation() function at this interval.\n\n**Low Values (100-500 ms):**\n- Faster adaptation to traffic changes\n- Better response to bursty/variable workloads\n- Slightly higher overhead from frequent calculations\n- Better for: dynamic workloads, traffic with varying patterns\n- Example: value=250 adapts 4 times per second\n\n**High Values (1000-5000 ms):**\n- Slower adaptation, more stable settings\n- Less overhead, but may miss short traffic pattern changes\n- Better for: steady-state workloads, predictable traffic\n- Example: value=2000 adapts every 2 seconds\n\n**Default (1000 ms = 1 second):**\nBalanced frequency for most workloads.\n\n**Value of 0:**\nCompletely disables adaptive interrupt moderation.\nModeration uses static packet_count and period_usec values.\nUse this when you want fully manual control.\n\n**Interaction with Traffic Patterns:**\n- Bursty traffic: Use lower values (250-500ms) for faster response\n- Steady traffic: Higher values (1000-2000ms) reduce overhead\n- Short-lived connections: May complete before adaptation takes effect\n\n**Note on Warmup:**\nNew connections start with packet_count/period_usec defaults.\nIf connections are shorter than this interval, they never benefit from adaptation.\nFor short-lived connections, consider tuning the static defaults instead."
                                },
                                "adaptive_interrupt_per_sec": {
                                    "type": "integer",
                                    "default": 10000,
                                    "title": "Target interrupts per second",
                                    "description": "Maps to XLIO_CQ_AIM_INTERRUPTS_RATE_PER_SEC environment variable.\nTarget interrupt rate per second that the adaptive algorithm tries to achieve.\nThis is the primary tuning knob for balancing latency vs CPU efficiency.\n\n**How It Works:**\nThe adaptive algorithm calculates: count = packet_rate / this_value\nExample: At 1M packets/sec with target 10000 interrupts/sec → count = 100 packets/interrupt\n\n**Low Values (1000-5000 interrupts/sec):**\n- Fewer interrupts per second\n- Higher latency (packets batched longer)\n- Better CPU efficiency (15-25% lower CPU usage)\n- Better for: throughput-oriented workloads, bulk transfers, streaming\n- Tradeoff: May cause 100-500µs additional latency\n- Example: value=2000 targets one interrupt every 500µs\n\n**High Values (10000-50000 interrupts/sec):**\n- More interrupts per second\n- Lower latency (smaller batches, faster processing)\n- Higher CPU usage (more interrupt overhead)\n- Better for: latency-sensitive workloads, request/response patterns\n- Example: value=20000 targets one interrupt every 50µs\n\n**Default (10000 interrupts/sec):**\nTargets ~100µs between interrupts, balancing latency and efficiency.\n\n**Performance Impact (from benchmarks):**\n| Target Rate | Throughput | CPU Usage | Use Case |\n|-------------|------------|-----------|----------|\n| 1000 | Baseline | Lowest | Bulk transfers |\n| 5000 | +5% | Low | Balanced |\n| 10000 | +10% | Moderate | General purpose |\n| 20000 | +15% | Higher | Latency-sensitive |\n| Disabled | +18% | Highest | Ultra-low-latency |\n\n**Workload-Specific Recommendations:**\n- **Nginx/HAProxy (small files <1KB):** 15000-20000 or disable\n- **Streaming/CDN (large files):** 2000-5000\n- **Database (mixed):** 10000 (default)\n- **HFT/Trading:** Disable moderation entirely\n\n**Interaction with adaptive_count:**\nThe calculated count is clamped to adaptive_count maximum.\nIf packet_rate / target_rate > adaptive_count, actual interrupt rate will be lower than target.\n\n**Formula:**\nactual_count = min(packet_rate / target_rate, adaptive_count)\nactual_interrupt_rate ≈ packet_rate / actual_count"
                                }
                            },
                            "additionalProperties": false
                        }
                    },
                    "additionalProperties": false
                },
                "buffers": {
                    "type": "object",
                    "description": "Buffer management and batching settings.",
                    "properties": {
                        "batching_mode": {
                            "oneOf": [
                                {
                                    "type": "integer",
                                    "enum": [
                                        0,
                                        1,
                                        2
                                    ],
                                    "default": 1
                                },
                                {
                                    "type": "string",
                                    "enum": [
                                        "disable",
                                        "enable_and_reuse",
                                        "enable"
                                    ],
                                    "default": "enable_and_reuse"
                                }
                            ],
                            "title": "Buffer batching mode",
                            "description": "Maps to XLIO_BUFFER_BATCHING_MODE environment variable.\n\n**What This Does:**\nControls how sockets manage RX and TX buffers - whether they cache buffers locally\nfor reuse or return them immediately to the shared buffer pool. This is a master\nswitch that affects both receive and transmit buffer lifecycle management.\n\n**Understanding the Buffer Flow:**\n\n*Without Batching (immediate return):*\n```\nPacket arrives → Buffer allocated from ring pool → App processes data → Buffer returned to ring pool immediately\n```\nEach operation potentially requires acquiring a lock on the shared pool.\n\n*With Batching (cached locally):*\n```\nPacket arrives → Buffer from ring pool → App processes → Buffer cached in socket's local queue\n                                                           ↓\n                                          When cache reaches threshold → Batch return to ring pool\n```\nLock acquisition is amortized across many buffers.\n\n**The Three Modes Explained:**\n\n---\n\n**\"disable\" or 0 - No Buffer Batching**\n\nForces all buffer batch sizes to 1:\n- performance.rings.tx.tcp_buffer_batch → 1\n- performance.rings.tx.udp_buffer_batch → 1\n- performance.buffers.rx.batch_size → 1\n\n*Behavior:*\n- Every RX buffer is returned to the ring/pool immediately after the application\n  consumes the data (after recv/read completes)\n- Every TX buffer is fetched one at a time from the pool for each send operation\n- No local caching of buffers at the socket level\n\n*Performance Characteristics:*\n\n| Aspect              | Impact                                              |\n|---------------------|-----------------------------------------------------|\n| Memory usage        | LOWEST - no per-socket buffer caching               |\n| Memory predictability | BEST - buffers never held by idle sockets         |\n| Pool lock contention | HIGHEST - every buffer operation acquires lock    |\n| Latency average     | Higher - pool access overhead on every operation    |\n| Latency variance    | Higher - susceptible to lock contention spikes      |\n| Throughput (PPS)    | Lower - serialization at pool locks                 |\n| CPU overhead        | Higher - more frequent lock acquire/release         |\n\n*When to Use:*\n- Memory-constrained environments where buffer memory must be strictly controlled\n- Systems with thousands of mostly-idle sockets where cached buffers would waste memory\n- NGINX DPU deployments (this mode is auto-selected for NGINX_DPU profile)\n- Debugging scenarios to simplify buffer lifecycle tracking\n- When strict buffer accounting is required\n\n---\n\n**\"enable_and_reuse\" or 1 - Batching with Periodic Reclaim (DEFAULT)**\n\nEnables local buffer caching with automatic cleanup of unused buffers.\n\n*RX Buffer Behavior:*\n- Processed buffers are queued in the socket's local reuse list (m_rx_reuse_buff)\n- When queue size reaches threshold (rx_bufs_batch, default ~64), batch return is triggered\n- At 2× threshold, forced immediate return occurs\n- During TCP timer callbacks (~100ms intervals), `return_pending_rx_buffs()` checks\n  for idle cached buffers and returns them if unused since last check\n- Uses a two-phase \"pending\" mechanism: first call marks buffers pending, second call\n  returns them if still unused - this prevents returning actively-used buffers\n\n*TX Buffer Behavior:*\n- Sockets fetch multiple TX buffers at once (tcp_buffer_batch/udp_buffer_batch)\n- Unused fetched buffers stay in socket's local cache (dst_entry.m_p_tx_mem_buf_desc_list)\n- During TCP timer callbacks, `return_pending_tx_buffs()` returns excess cached buffers\n  to the ring pool via return_buffers_pool()\n\n*The Reclaim Mechanism:*\nEvery TCP timer tick (~100ms, controlled by network.protocols.tcp.timer_msec),\nXLIO calls return_pending_rx_buffs() and return_pending_tx_buffs(). This ensures:\n- Idle sockets don't hoard buffers indefinitely\n- Buffer pool doesn't get starved by inactive connections\n- Critical for avoiding deadlocks where all buffers are held by idle sockets,\n  preventing new FIN packets from being received to close connections\n\n*Performance Characteristics:*\n\n| Aspect              | Impact                                              |\n|---------------------|-----------------------------------------------------|\n| Memory usage        | MODERATE - caching with periodic cleanup            |\n| Memory predictability | GOOD - idle buffers eventually returned           |\n| Pool lock contention | LOW - batched operations amortize locking         |\n| Latency average     | Better - most operations use local cache            |\n| Latency variance    | Lower - fewer pool access interruptions             |\n| Throughput (PPS)    | Good - efficient batch processing                   |\n| CPU overhead        | Lower - amortized locking overhead                  |\n| Deadlock safety     | BEST - reclaim prevents buffer starvation           |\n\n*When to Use:*\n- Most production deployments (this is the default for good reason)\n- Servers with mixed workloads (active and idle connections)\n- When you need balance between performance and resource efficiency\n- Applications where connection lifetimes vary significantly\n- Any scenario where buffer starvation/deadlock must be avoided\n\n---\n\n**\"enable\" or 2 - Batching without Reclaim**\n\nEnables local buffer caching but DISABLES the periodic reclaim mechanism.\n\n*Key Difference from Mode 1:*\n- `return_pending_rx_buffs()` returns immediately without doing anything\n- `return_pending_tx_buffs()` returns immediately without doing anything\n- Cached buffers STAY in socket's local cache until the socket is destroyed\n  or the cache naturally fills and triggers a batch return\n\n*Behavior:*\n- Same batching/caching as mode 1 during active send/receive\n- BUT: idle sockets keep their cached buffers indefinitely\n- Buffers only return to pool when:\n  a) Cache exceeds threshold and batch return triggers, OR\n  b) Socket is closed/destroyed\n\n*Performance Characteristics:*\n\n| Aspect              | Impact                                              |\n|---------------------|-----------------------------------------------------|\n| Memory usage        | HIGHEST - buffers held even when idle               |\n| Memory predictability | LOWEST - hard to predict buffer distribution      |\n| Pool lock contention | LOWEST - minimal pool interaction                 |\n| Latency average     | Best - buffers always ready in local cache          |\n| Latency variance    | LOWEST - no reclaim overhead during operation       |\n| Throughput (PPS)    | Highest - maximum caching efficiency                |\n| CPU overhead        | Lowest - no periodic reclaim processing             |\n| Deadlock risk       | ELEVATED - possible buffer starvation               |\n\n*When to Use:*\n- High-frequency trading or ultra-low-latency applications where every\n  microsecond of variance matters\n- Applications with predictable, steady traffic patterns on all sockets\n- Systems with abundant memory where buffer efficiency is less important\n- Workloads where all sockets are continuously active (no idle connections)\n- When you're certain total buffer demand won't exceed pool capacity\n\n*CAUTION - Buffer Starvation Risk:*\nWithout reclaim, if many sockets cache buffers and then go idle, the global\nbuffer pool can be depleted. New connections may fail to allocate buffers,\nand critically, FIN packets for connection teardown may not be processable,\npotentially causing connection leaks or hangs.\n\n---\n\n**Interaction with Other Parameters:**\n\n- **performance.rings.tx.tcp_buffer_batch:** When batching_mode=0 (disable),\n  this is forced to 1 regardless of configured value.\n- **performance.rings.tx.udp_buffer_batch:** Same as above - forced to 1 when disabled.\n- **performance.buffers.rx.batch_size:** Controls the reuse threshold for RX buffers.\n  With batching enabled, buffers accumulate until this threshold before batch return.\n- **network.protocols.tcp.timer_msec:** Controls how often the reclaim mechanism runs\n  in mode 1 (enable_and_reuse). Lower values = more frequent reclaim checks.\n\n**Tuning Recommendations:**\n\n1. **Start with default (enable_and_reuse / 1)** - safe for most workloads\n\n2. **Switch to disable (0) if:**\n   - Running thousands of connections with sporadic traffic\n   - Memory is severely constrained\n   - Seeing \"unable to allocate buffer\" errors with many idle sockets\n   - Need deterministic memory usage for capacity planning\n\n3. **Switch to enable (2) only if:**\n   - All sockets are continuously active (no idle connections)\n   - Latency consistency is paramount (HFT, real-time control)\n   - You have profiled and confirmed no buffer starvation risk\n   - Memory is abundant and not a concern\n   - You've tested under peak load and verified buffer availability\n\n**Monitoring:**\n- xlio_stats shows buffer pool levels and allocation failures\n- Watch for n_rx_ready_byte_drop and buffer allocation errors\n- Monitor connection close failures that might indicate buffer starvation"
                        },
                        "tx": {
                            "type": "object",
                            "description": "Transmit buffer settings.",
                            "properties": {
                                "buf_size": {
                                    "oneOf": [
                                        {
                                            "type": "integer",
                                            "default": 0,
                                            "minimum": 0,
                                            "maximum": 262144
                                        },
                                        {
                                            "type": "string",
                                            "default": "0B",
                                            "pattern": "^[0-9]+[KMGkmg]?[B]?$"
                                        }
                                    ],
                                    "title": "TX buffer size",
                                    "description": "Maps to XLIO_TX_BUF_SIZE environment variable.\n\n**What This Controls:**\nSize of individual transmit data buffer elements in the global TX buffer pool (g_buffer_pool_tx).\nEach buffer holds outgoing packet payload data. The actual allocated size per buffer is\n`buf_size + 92 bytes` (92 bytes reserved for L2/L3/L4 headers: Ethernet, IP, TCP/UDP).\n\n**Value Range:**\n- Minimum: Must be greater than TCP MSS (Maximum Segment Size), otherwise reset to 0 (auto)\n- Maximum: 256KB (262144 bytes)\n- Default: 0 (auto-calculated)\n\n**Auto-Calculation (value = 0):**\nWhen set to 0, XLIO calculates the optimal buffer size as:\n`buf_size = TCP_MSS` (derived from MTU - 40 bytes for IP+TCP headers)\n\nFor standard 1500-byte MTU: buf_size ≈ 1460 bytes\nFor jumbo frames (9000-byte MTU): buf_size ≈ 8960 bytes\n\n**Memory Consumption:**\nTotal TX buffer pool memory ≈ `num_buffers × (buf_size + 92 + ~128 bytes descriptor overhead)`\n\nInitial pool size = `performance.rings.tx.tcp_buffer_batch × 1024` buffers\n\nExamples:\n- Default (1460 bytes), batch=16: ~27 MB initial pool\n- 64KB buffers, batch=16: ~1.1 GB initial pool\n- 256KB buffers, batch=1: ~262 MB initial pool (worker threads mode)\n\n**TSO (TCP Segmentation Offload) Interaction:**\nThis parameter directly affects TSO efficiency. When TSO is enabled:\n- `tso.max_buf_sz = min(buf_size, max_tso_size)`\n- LWIP uses `max_buf_sz` to determine how much data to pack into a single TCP segment\n- Larger buffers allow TSO to create larger segments (up to the hardware's max_payload_sz)\n- The hardware NIC then splits these large segments into MTU-sized packets\n\n**Performance Impact - LOW Values (0/auto, ~1.5KB for standard MTU):**\n\n| Aspect | Impact |\n|--------|--------|\n| Memory footprint | LOWEST - minimal per-buffer overhead |\n| TSO efficiency | LIMITED - segments capped at ~MSS size |\n| Buffer allocations | MORE FREQUENT - many small buffers needed for large sends |\n| CPU overhead | HIGHER - more segments to create, more syscalls |\n| Latency (small msgs) | GOOD - buffers match typical packet sizes |\n| Throughput (bulk) | LOWER - TSO cannot aggregate effectively |\n| Best for | Latency-sensitive apps with small messages, memory-constrained environments |\n\n**Performance Impact - HIGH Values (64KB - 256KB):**\n\n| Aspect | Impact |\n|--------|--------|\n| Memory footprint | HIGHER - large per-buffer allocation |\n| TSO efficiency | MAXIMUM - full hardware TSO utilization |\n| Buffer allocations | FEWER - one buffer can hold large payload |\n| CPU overhead | LOWER - fewer segments, less processing |\n| Latency (small msgs) | SLIGHTLY HIGHER - potential memory waste for small data |\n| Throughput (bulk) | HIGHEST - TSO creates optimal large segments |\n| Best for | Streaming, bulk transfers, high-bandwidth applications |\n\n**Special Behaviors:**\n\n1. **Worker Threads Mode:** When `performance.threading.worker_threads > 0`,\n   XLIO automatically sets `buf_size = 256KB` for optimal throughput in\n   the asynchronous processing model.\n\n2. **TSO Disabled:** When TSO is disabled, large buffer sizes provide less benefit\n   since segments are still limited to MSS. However, fewer buffer allocations\n   may still reduce overhead for applications sending large payloads.\n\n3. **Zero-Copy TX:** For zero-copy transmissions (PBUF_ZEROCOPY), this parameter\n   does not affect the data buffers since data is sent directly from application memory.\n   It only affects the descriptor pool sizing.\n\n**Tuning Recommendations:**\n\n*For latency-sensitive applications (trading, real-time):*\n- Use default (0) or small values matching your typical message sizes\n- Smaller buffers = faster allocation, less memory waste\n\n*For throughput-oriented applications (streaming, file transfer):*\n- Use 64KB-256KB with TSO enabled\n- Enables hardware to batch many packets efficiently\n- Reduces CPU overhead significantly at high bandwidth\n\n*For mixed workloads:*\n- Consider 16KB-32KB as a balanced middle ground\n- Provides reasonable TSO benefit without excessive memory\n\n*For memory-constrained environments:*\n- Keep default (0) to minimize buffer pool memory\n- Ensure core.resources.memory_limit is appropriately sized\n\n**Interaction with Other Parameters:**\n- `hardware_features.offloads.tso.enable`: TSO must be enabled to benefit from large buffers\n- `hardware_features.offloads.tso.max_size`: Upper bound on TSO segment size\n- `performance.rings.tx.tcp_buffer_batch`: Affects how many buffers are fetched at once\n- `core.resources.memory_limit`: Large buf_size increases memory consumption\n- `performance.threading.worker_threads`: Automatically overrides to 256KB when > 0\n\nSupports suffixes: B, KB, MB, GB.",
                                    "x-memory-size": true
                                },
                                "prefetch_size": {
                                    "type": "integer",
                                    "default": 256,
                                    "minimum": 0,
                                    "title": "TX prefetch size",
                                    "description": "Maps to XLIO_TX_PREFETCH_BYTES environment variable.\n\n**What This Controls:**\nNumber of bytes to prefetch into CPU L1 cache before writing packet data to TX buffers.\nThis optimization targets the UDP fast path where packet headers and payload are copied\nto transmit buffers allocated from XLIO's buffer pool.\n\n**How It Works:**\nWhen XLIO prepares a UDP packet for transmission:\n1. A TX buffer is obtained from the buffer pool (may be \"cold\" - not in CPU cache)\n2. If prefetch_size > 0, XLIO issues CPU prefetch instructions for the buffer\n3. Then headers (L2/IP/UDP) are copied, followed by user payload (memcpy_fromiovec)\n4. The prefetch brings buffer memory into L1 cache BEFORE the write operations\n\n**CPU Cache Mechanics:**\n- Uses hardware prefetch instructions: `prefetcht0` (x86), `dcbt` (PPC64), `prfm` (ARM64)\n- Issues one prefetch instruction per L1 cache line (64 bytes on x86/ARM64, 128 bytes on PPC64)\n- Example: 256 bytes = 4 prefetch instructions on x86 (256 ÷ 64 = 4 cache lines)\n- Actual prefetch size = min(prefetch_size, payload_size)\n\n**Value Range:** 0 to MTU size (typically 0-1500 bytes, or up to 9000 for jumbo frames)\n\n**Performance Impact - Value of 0 (Disabled):**\n\n| Aspect | Impact |\n|--------|--------|\n| Prefetch overhead | NONE - no prefetch instructions issued |\n| Cache miss risk | HIGHER - buffer may not be in cache when writes begin |\n| Write stalls | POSSIBLE - CPU may stall waiting for memory during copy |\n| CPU cycles | Fewer for prefetch, but potentially more stall cycles |\n| Best for | Systems where prefetch is counterproductive, benchmarking baseline |\n\n**Performance Impact - LOW Values (64-256 bytes, default=256):**\n\n| Aspect | Impact |\n|--------|--------|\n| Prefetch overhead | MINIMAL - 1-4 prefetch instructions per packet |\n| Cache warming | PARTIAL - covers headers + small payload portion |\n| Instruction count | LOW - minimal impact on instruction cache |\n| Cache pollution | MINIMAL - prefetches only what's likely to be used soon |\n| Best for | Small packets, latency-sensitive applications, mixed workloads |\n\n**Performance Impact - HIGH Values (512 bytes to MTU):**\n\n| Aspect | Impact |\n|--------|--------|\n| Prefetch overhead | HIGHER - 8-24 prefetch instructions per packet (for 512-1500 bytes) |\n| Cache warming | MAXIMUM - entire packet buffer pre-warmed in cache |\n| Instruction count | HIGHER - more CPU cycles spent on prefetch loop |\n| Cache pollution | POSSIBLE - may evict other useful data from L1 cache |\n| Write efficiency | BEST - all writes hit L1 cache, no stalls |\n| Best for | Large packets, throughput-focused applications, bulk transfers |\n\n**When Prefetch Helps Most:**\n- TX buffers are \"cold\" (recently allocated from pool, not in cache)\n- Large payloads where multiple cache lines need to be written\n- Systems with high memory latency (multi-socket NUMA)\n- Applications sending bursts after idle periods\n\n**When Prefetch May Hurt:**\n- Very small packets (< 64 bytes payload) - overhead exceeds benefit\n- Extremely high PPS where prefetch instruction overhead accumulates\n- L1 cache is heavily contested by other operations\n- TX buffers are already \"warm\" from recent reuse\n\n**Architecture-Specific Considerations:**\n\n*x86/x86_64:*\n- L1 cache line: 64 bytes\n- prefetcht0 instruction brings data to all cache levels\n- Default 256 bytes = 4 prefetch instructions\n- Modern CPUs have good hardware prefetchers that may reduce benefit\n\n*ARM64 (aarch64):*\n- L1 cache line: 64 bytes (typical)\n- Uses prfm pldl1keep instruction\n- Similar behavior to x86\n\n*PPC64:*\n- L1 cache line: 128 bytes\n- Uses dcbt instruction\n- Default 256 bytes = 2 prefetch instructions\n- Larger cache lines mean fewer instructions needed\n\n**Tuning Recommendations:**\n\n*For latency-sensitive applications (trading, gaming, real-time):*\n- Start with default (256 bytes)\n- If sending mostly small messages (< 256 bytes), try reducing to 64-128\n- If latency variance is high, try increasing to match typical message size\n\n*For throughput-oriented applications (streaming, bulk transfer):*\n- Try values matching your typical payload size (512-1500 bytes)\n- For jumbo frames, consider values up to 4096-8192 bytes\n- Monitor CPU utilization - if prefetch overhead is visible, reduce\n\n*For high PPS applications (millions of packets/sec):*\n- Consider reducing to 64-128 bytes or disabling (0)\n- At very high PPS, prefetch instruction overhead can accumulate\n- Profile with perf to measure actual cache miss rates\n\n*For NUMA systems:*\n- Prefetching is more valuable when buffers may be on remote NUMA node\n- Higher values (512+) may show more benefit\n- Combine with proper NUMA pinning for best results\n\n**Interaction with Other Parameters:**\n- Only affects non-inline sends (payload > ~200 bytes for UDP)\n- Inline sends bypass TX buffer copy entirely, making prefetch irrelevant\n- Used in conjunction with `performance.rings.tx.max_inline_size`\n- Does not affect TCP path (TCP uses different buffer management)\n\n**Profiling Tips:**\n- Use `perf stat -e cache-misses,cache-references` to measure L1 miss rate\n- Compare with prefetch disabled (0) vs various values\n- Look for reduced cache-miss percentage with appropriate prefetch size\n- Monitor overall CPU utilization for prefetch overhead"
                                }
                            }
                        },
                        "rx": {
                            "type": "object",
                            "description": "Receive buffer settings.",
                            "properties": {
                                "buf_size": {
                                    "oneOf": [
                                        {
                                            "type": "integer",
                                            "default": 0,
                                            "minimum": 0,
                                            "maximum": 65280
                                        },
                                        {
                                            "type": "string",
                                            "default": "0B",
                                            "pattern": "^[0-9]+[KMGkmg]?[B]?$"
                                        }
                                    ],
                                    "title": "RX buffer size",
                                    "description": "Maps to XLIO_RX_BUF_SIZE environment variable.\n\n**What This Does:**\nControls the size of each receive buffer element in the RX buffer pool (g_buffer_pool_rx_rwqe). These buffers hold incoming packet data received from the network interface. The buffer size determines the maximum packet payload that can be received without fragmentation or truncation.\n\n**IMPORTANT: Striding RQ Interaction (Default Mode)**\nWhen Striding RQ is enabled (hardware_features.striding_rq.enable=true, the DEFAULT), this parameter has LIMITED effect:\n- Buffer size is calculated from stride configuration: `strides_num × stride_size` (default: 2048 × 64 = 128KB)\n- The rx_buf_size value is used ONLY for LRO max_payload_sz calculation (see below)\n- To change actual buffer sizes with Striding RQ, adjust hardware_features.striding_rq.strides_num and stride_size instead\n\nThis parameter primarily affects scenarios where **Striding RQ is DISABLED**.\n\n**Value Semantics:**\n- **0 (Default):** Auto-calculate based on maximum MTU across all network interfaces\n  - With Striding RQ enabled: uses stride configuration for buffers\n  - With Striding RQ disabled: uses max_mtu directly (e.g., 1500 bytes for standard Ethernet, ~9000 for jumbo frames)\n- **Non-zero value:** Force specific buffer size (must be > TCP MSS, otherwise reset to 0)\n- **Maximum:** 65280 bytes (0xFF00)\n- **Alignment:** Automatically aligned to 64-byte boundary for cache efficiency\n\n**How Buffer Size Affects LRO (Large Receive Offload):**\nWhen LRO is enabled, multiple incoming TCP segments are aggregated into larger buffers before delivery to the application. The rx_buf_size limits this aggregation:\n```\nLRO max_payload_sz = min(actual_buf_size, 64KB) rounded down to 256-byte boundary\n```\n- With default (0) and Striding RQ: max_payload ≈ 64KB (limited by XLIO_MLX5_PARAMS_LRO_PAYLOAD_SIZE)\n- With explicit rx_buf_size=8192: max_payload = 8192 bytes\n- Larger max_payload = more segments aggregated = higher throughput, fewer packets to process\n- Smaller max_payload = less aggregation = lower latency per delivery, more frequent packet events\n\n**Memory Impact:**\nTotal RX buffer pool memory = buf_size × number_of_buffers\n- Initial pool: 2 × ring_elements_count buffers per ring\n- Pool expands dynamically as needed (up to XLIO_MEMORY_LIMIT)\n\nWith Striding RQ disabled:\n- Standard MTU (1500): ~3MB per ring (2 × 1024 buffers × 1500 bytes)\n- Jumbo frames (9000): ~18MB per ring\n- Large rx_buf_size (32KB): ~64MB per ring\n\n**Performance Tradeoffs:**\n\n*High Values (8KB - 64KB) - Best for Throughput (when Striding RQ disabled):*\n\nBenefits:\n- Larger LRO aggregation: More TCP segments coalesced into fewer packets\n- Fewer buffer descriptor operations: Each buffer holds more data\n- Reduced per-packet overhead: Fewer completions to process for same data volume\n- Better for bulk data transfer, streaming, and high-bandwidth applications\n- Enables receiving jumbo frames when MTU > 1500\n\nDrawbacks:\n- Higher memory consumption: Each buffer consumes more memory\n- Memory waste for small packets: UDP packets or small TCP segments leave buffer mostly empty\n- Longer time-to-first-byte: LRO holds packets longer waiting for more segments\n- May increase latency variance due to larger aggregation batches\n\nRecommended for:\n- Bulk data transfer (file transfer, backup, replication)\n- Streaming media servers\n- High-throughput applications where latency variance is acceptable\n- Jumbo frame environments (MTU 9000)\n\n*Low Values (MTU-sized, ~1500-2048) - Best for Latency (when Striding RQ disabled):*\n\nBenefits:\n- Lower memory footprint: Efficient memory utilization\n- Minimal LRO aggregation delay: Packets delivered sooner\n- Better memory efficiency: Buffers sized appropriately for typical packets\n- More predictable latency: Less time spent in aggregation\n- Suitable for mixed workloads with varying packet sizes\n\nDrawbacks:\n- Limited LRO benefit: Less segment aggregation reduces throughput optimization\n- Higher per-packet overhead: More completions to process\n- More buffer management operations: Smaller buffers cycle more frequently\n\nRecommended for:\n- Latency-sensitive applications (trading, gaming, real-time control)\n- Request-response protocols (HTTP, database queries)\n- Applications with small message sizes\n- Memory-constrained environments\n\n*Value of 0 (Auto-calculation) - Recommended for Most Users:*\n\nBenefits:\n- Automatically adapts to network configuration\n- Optimal for the configured MTU\n- No manual tuning required\n- Works correctly with both standard and jumbo frames\n\n**Interaction with Other Parameters:**\n\n- **hardware_features.striding_rq.enable:** When true (default), stride configuration overrides rx_buf_size for buffer allocation. rx_buf_size only affects LRO max_payload calculation.\n\n- **hardware_features.striding_rq.strides_num/stride_size:** With Striding RQ enabled, these control actual buffer sizes. Effective buffer = strides_num × stride_size.\n\n- **hardware_features.lro:** LRO aggregation is limited by rx_buf_size (or stride buffer size). Larger buffers allow more aggressive aggregation.\n\n- **network.protocols.ip.mtu:** rx_buf_size should be >= MTU to receive full packets without fragmentation. Auto-calculation (value 0) ensures this.\n\n- **performance.rings.rx.ring_elements_count:** Determines how many buffers are allocated per ring. Total memory = buf_size × ring_elements_count × 2.\n\n- **performance.rings.rx.spare_buffers:** Local buffer cache per ring. Must be compatible with buffer size for efficient recycling.\n\n**Validation Rules:**\n- If rx_buf_size <= TCP MSS (Maximum Segment Size), it's automatically reset to 0 (auto-calculate)\n- TCP MSS = MTU - 40 (IP header + TCP header)\n- Value is clamped to maximum 65280 bytes (0xFF00)\n\n**Tuning Recommendations:**\n\n1. **For most deployments:** Use default (0) - automatically optimal for your MTU\n\n2. **For maximum throughput (Striding RQ disabled):**\n   - Set to 32KB-64KB\n   - Ensure XLIO_MEMORY_LIMIT is sufficient for larger buffer pool\n   - Combine with hardware_features.lro enabled\n\n3. **For latency-sensitive applications (Striding RQ disabled):**\n   - Use default (0) or MTU-sized value\n   - Consider disabling LRO for minimum latency\n\n4. **For jumbo frames:**\n   - Use default (0) - auto-detects MTU\n   - Or explicitly set to match your jumbo MTU (e.g., 9000)\n\n5. **For Striding RQ environments (default):**\n   - This parameter has minimal effect on buffer allocation\n   - Tune hardware_features.striding_rq.strides_num and stride_size instead\n   - rx_buf_size only affects LRO max_payload calculation\n\n**Monitoring:**\n- Check xlio_stats for buffer pool statistics (n_buffer_pool_len, n_buffer_pool_created)\n- Monitor n_rx_sw_pkt_drops for buffer exhaustion issues\n- If seeing drops, consider increasing XLIO_MEMORY_LIMIT rather than buffer size\n\nSupports suffixes: B, KB, MB, GB (e.g., \"8KB\", \"32KB\").",
                                    "x-memory-size": true
                                },
                                "prefetch_size": {
                                    "type": "integer",
                                    "default": 256,
                                    "minimum": 32,
                                    "title": "RX prefetch size",
                                    "description": "Maps to XLIO_RX_PREFETCH_BYTES environment variable.\n\n**What This Controls:**\nNumber of bytes to prefetch into CPU L1 cache when processing received packets. This optimization targets the receive path where packet data is copied from XLIO's RX buffers to the application's user-space buffers during recv()/recvfrom()/recvmsg() calls.\n\n**How It Works - The Complete Flow:**\n\n1. **Packet arrives at NIC** → Hardware posts a Completion Queue Entry (CQE)\n2. **XLIO polls CQ** → Retrieves the CQE indicating packet is ready\n3. **CQE Processing (prefetch happens here):**\n   - `prefetch_range()` is called on the packet buffer\n   - Prefetch starts AFTER the L2 (Ethernet) header (~14-18 bytes into buffer)\n   - Issues CPU prefetch instructions to bring data into L1 cache\n4. **Packet added to socket's Ready Queue** → Awaits application retrieval\n5. **Application calls recv()** → Data is memcpy'd from buffer to user space\n   - If prefetch was effective, memcpy hits warm L1 cache!\n\n**Critical Detail: What Actually Gets Prefetched**\n\nThe prefetch starts after the L2 header (m_sz_transport_header ≈ 14-18 bytes), so the prefetched region includes:\n\n```\nBuffer Layout:\n|-- L2 Header (14-18B) --|-- IP Header (20B) --|-- UDP/TCP (8-20B) --|-- PAYLOAD --|\n                         ^                                            ^\n                         |-- Prefetch starts here                     |-- App reads from here\n```\n\n- **Prefetched but NOT read by app:** IP + UDP/TCP headers (~28-40 bytes)\n- **Effective payload prefetch:** prefetch_size minus ~30-40 bytes of headers\n\nWith default value (256 bytes):\n- Total prefetched: 256 bytes starting at offset ~14\n- Headers within prefetch: ~30-40 bytes (not useful to app)\n- **Actual payload prefetched: ~216-226 bytes**\n\nWith minimum value (32 bytes):\n- Total prefetched: 32 bytes starting at offset ~14\n- Almost entirely IP+UDP headers!\n- **Actual payload prefetched: ~0-4 bytes** (nearly useless)\n\n**CPU Cache Mechanics:**\n\n- Uses hardware prefetch instructions: `prefetcht0` (x86), `dcbt` (PPC64), `prfm pldl1keep` (ARM64)\n- One prefetch instruction per L1 cache line:\n  - x86/ARM64: 64-byte cache lines\n  - PPC64: 128-byte cache lines\n- Example (x86): 256 bytes = 4 prefetch instructions (256 ÷ 64)\n- Actual bytes prefetched = min(prefetch_size, packet_size - L2_header_size)\n\n**Value Range:** 32 to 2044 bytes (MCE_MIN_RX_PREFETCH_BYTES to MCE_MAX_RX_PREFETCH_BYTES)\n\n**Performance Impact - LOW Values (32-128 bytes):**\n\n| Aspect | Impact |\n|--------|--------|\n| Prefetch overhead | MINIMAL - 1-2 prefetch instructions |\n| Effective payload prefetch | POOR - mostly prefetches IP/UDP headers, minimal payload |\n| Cache pollution risk | LOWEST - minimal data brought into L1 |\n| Best for | Extremely small payloads (<64 bytes), or when prefetch overhead must be minimized |\n\nWith 32-64 bytes, you're mostly prefetching protocol headers that the application doesn't read. This setting provides little benefit for typical workloads.\n\n**Performance Impact - DEFAULT Value (256 bytes):**\n\n| Aspect | Impact |\n|--------|--------|\n| Prefetch overhead | LOW - 4 prefetch instructions (x86) |\n| Effective payload prefetch | GOOD - ~216 bytes of actual payload |\n| Cache pollution risk | LOW - fits comfortably in L1 (32KB typical) |\n| Best for | Small to medium packets, mixed workloads, general-purpose applications |\n\nThe default 256 bytes is well-balanced: 4 cache lines cover typical small message payloads while keeping overhead minimal.\n\n**Performance Impact - HIGH Values (512-1024 bytes):**\n\n| Aspect | Impact |\n|--------|--------|\n| Prefetch overhead | MODERATE - 8-16 prefetch instructions |\n| Effective payload prefetch | EXCELLENT - ~470-990 bytes of payload |\n| Cache pollution risk | MODERATE - consumes 8-16 cache lines |\n| Write efficiency | HIGH - large portion of payload pre-warmed |\n| Best for | Medium to large packets, throughput-focused applications |\n\n**Performance Impact - MAXIMUM Values (1500-2044 bytes, near MTU):**\n\n| Aspect | Impact |\n|--------|--------|\n| Prefetch overhead | HIGH - 24-32 prefetch instructions |\n| Effective payload prefetch | MAXIMUM - entire payload pre-warmed |\n| Cache pollution risk | HIGHER - may evict other useful data from 32KB L1 |\n| Instruction overhead | SIGNIFICANT - prefetch loop adds CPU cycles per packet |\n| Best for | Large packets near MTU size, applications that always read full payloads |\n\n**When Prefetch Helps Most:**\n\n1. **Cold buffers:** Buffers recently allocated from pool (not in cache from previous use)\n2. **Large payloads:** Where multiple cache lines need to be read by application\n3. **High memory latency systems:** Multi-socket NUMA where buffer may be on remote node\n4. **Bursty traffic patterns:** After idle periods when L1 cache has been repurposed\n5. **Run-to-completion mode:** Tight timing between prefetch and application read\n\n**When Prefetch May Hurt:**\n\n1. **Very small packets (<64 bytes payload):** Overhead exceeds benefit; headers dominate prefetch\n2. **Extremely high PPS (millions/sec):** Prefetch instruction overhead accumulates significantly\n3. **Partial reads:** Application uses MSG_PEEK or reads only first few bytes\n4. **Long ready queue:** If many packets queue before app reads, prefetched data evicts from L1\n5. **Worker threads mode:** Timing gap between prefetch (worker thread) and read (app thread) may cause L1 eviction\n6. **L1 cache contention:** Other hot data being accessed may evict prefetched packet data\n\n**Timing Window Consideration:**\n\nPrefetch effectiveness depends on the time between prefetch (CQ processing) and consumption (app recv()):\n\n- **Ideal:** < 1-10 microseconds - data stays warm in L1\n- **Acceptable:** 10-100 microseconds - data likely still in L2/L3\n- **Poor:** > 100 microseconds - data may be evicted, prefetch wasted\n\nIn run-to-completion mode (default), timing is usually tight. With worker threads or if application is slow to call recv(), the gap widens and prefetch benefit decreases.\n\n**Architecture-Specific Behavior:**\n\n*x86/x86_64:*\n- L1 cache: typically 32KB per core\n- Cache line: 64 bytes\n- prefetcht0 brings data to all cache levels (L1, L2, L3)\n- Modern CPUs have hardware prefetchers that may already optimize sequential access\n\n*ARM64 (aarch64):*\n- L1 cache: typically 32-64KB per core\n- Cache line: 64 bytes (most implementations)\n- prfm pldl1keep instruction with L1 keep hint\n- Explicit prefetch often more beneficial than x86 due to less aggressive hardware prefetch\n\n*PPC64:*\n- L1 cache: typically 32-64KB per core\n- Cache line: 128 bytes (larger than x86/ARM)\n- dcbt instruction for data cache block touch\n- Fewer prefetch instructions needed: 256 bytes = only 2 instructions\n\n**Tuning Recommendations:**\n\n*For latency-sensitive applications (trading, real-time, gaming):*\n- Start with default (256 bytes) - proven good balance\n- If typical message size < 200 bytes, consider reducing to 128\n- If typical message size > 512 bytes, consider increasing to match payload size + 40 (for headers)\n- Profile with `perf stat -e cache-misses` to measure actual L1 miss rates\n\n*For throughput-oriented applications (streaming, bulk transfer):*\n- Match prefetch_size to your typical payload size + ~40 bytes for headers\n- For MTU-sized packets (1500B): use 1024-1500 bytes\n- For jumbo frames (9000B MTU): use 2044 (maximum) - covers first 2KB of payload\n- Monitor CPU utilization for prefetch overhead at very high PPS\n\n*For high PPS applications (packet processing, forwarding):*\n- Consider reducing to 128-256 bytes or using minimum (32)\n- At millions of PPS, even small per-packet overhead multiplies significantly\n- Profile to determine if prefetch actually improves throughput\n\n*For mixed workloads:*\n- Default (256 bytes) is usually optimal\n- Covers headers + ~216 bytes payload - good for most common packet sizes\n- Low overhead with meaningful benefit\n\n*For NUMA systems:*\n- Prefetch more valuable when buffers may reside on remote NUMA node\n- Consider higher values (512-1024 bytes)\n- Combine with proper NUMA pinning (numactl) for best results\n\n**Interaction with Other Parameters:**\n\n- **performance.buffers.rx.prefetch_before_poll:** Alternative prefetch timing - prefetches BEFORE polling CQ. Use one or the other based on your latency needs.\n\n- **performance.polling.max_rx_poll_batch:** If batch size is large, prefetch is done per-packet within the batch. Higher batch = more prefetch instructions per poll cycle.\n\n- **hardware_features.striding_rq:** With Striding RQ enabled, buffer layout differs slightly but prefetch mechanics remain the same.\n\n- **network.protocols.ip.mtu:** Maximum useful prefetch_size is approximately MTU - 14 (L2 header). Prefetching beyond packet size is harmless but wasteful.\n\n**Profiling Tips:**\n\n1. Measure L1 cache miss rate:\n   ```bash\n   perf stat -e L1-dcache-load-misses,L1-dcache-loads ./your_app\n   ```\n\n2. Compare different values:\n   - Run benchmark with prefetch_size=32 (baseline, minimal prefetch)\n   - Run with 256 (default)\n   - Run with value matching your payload size\n   - Measure both latency and throughput\n\n3. Look for:\n   - Reduced L1 cache miss percentage with appropriate prefetch size\n   - But watch for increased CPU utilization from prefetch overhead\n   - Sweet spot is where miss rate improves without significant CPU increase"
                                },
                                "prefetch_before_poll": {
                                    "type": "integer",
                                    "default": 0,
                                    "title": "Prefetch before polling",
                                    "description": "Maps to XLIO_RX_PREFETCH_BYTES_BEFORE_POLL environment variable.\n\n**What This Controls:**\nNumber of bytes to speculatively prefetch into CPU L1 cache BEFORE polling the Completion Queue (CQ) for new packets. Unlike the regular `prefetch_size` parameter (which prefetches AFTER a packet is received), this parameter enables predictive prefetching of the NEXT expected packet's buffer location.\n\n**How It Works - The Mechanism:**\n\nXLIO maintains knowledge of where the next packet will arrive based on the order buffers were posted to the receive queue:\n\n1. **Buffer Posting Phase (Background):**\n   - When XLIO posts RX buffers to the hardware queue, it chains them together via an internal linked list (`p_prev_desc`)\n   - This creates a predictable order: buffer posted first will receive a packet first (FIFO)\n\n2. **Prefetch Before Poll (when enabled):**\n   - At the START of each `poll_and_process_element_rx()` call, BEFORE checking for completions\n   - XLIO prefetches the predicted next buffer location into L1 cache\n   - For Regular RQ: prefetches `m_p_next_rx_desc_poll->p_buffer`\n   - For Striding RQ: prefetches `m_rx_hot_buffer->p_buffer + consumed_offset`\n\n3. **Regular Prefetch (happens later, uses `prefetch_size`):**\n   - AFTER a CQE is polled and validated\n   - Prefetches the CURRENT packet's data (already in hand)\n\n**The Key Difference: Timing**\n\n```\n                     Regular prefetch (prefetch_size)\n                              |\n                              v\n[Poll CQ] --> [Get CQE] --> [Prefetch packet data] --> [Process packet]\n    ^\n    |\nprefetch_before_poll happens HERE, speculatively\n```\n\nWith `prefetch_before_poll`, the prefetch happens BEFORE we even know if a packet is ready. This is speculative optimization.\n\n**Why This Specifically Benefits LOW PPS (Packets Per Second) Traffic:**\n\nAt low packet rates (e.g., <100K PPS), there are significant idle gaps between packets:\n\n*Without prefetch_before_poll (default):*\n```\nPacket 1 arrives --> [Cache warm] --> processed\n   |--- 10ms idle gap (CPU does other work, cache contents change) ---|\nPacket 2 arrives --> [Buffer is COLD in cache] --> L1 cache miss --> higher latency\n```\n\nDuring the idle gap:\n- CPU executes application code, other threads, OS tasks\n- L1 cache contents are evicted/replaced with other data\n- The next RX buffer becomes \"cold\" (not in L1 cache)\n- When packet 2 arrives, reading its buffer causes expensive L1 cache misses (~4-10 cycles penalty per miss)\n\n*With prefetch_before_poll enabled:*\n```\nApplication calls recv() --> poll starts\n  ^\n  |-- Speculatively prefetch next RX buffer location (even if no packet yet)\n  |-- Returns with no data (EAGAIN or timeout)\n\n[Buffer now warm in L1 cache]\n\nNext recv() call --> poll starts\n  ^\n  |-- Packet 2 is now ready\n  |-- Buffer already in L1 cache from previous speculative prefetch!\n  |-- Lower latency to process and copy to user space\n```\n\nEven if the poll returns empty, the prefetch keeps the expected buffer location warm. When a packet does arrive, cache hits replace cache misses.\n\n**Why This Does NOT Benefit HIGH PPS Traffic:**\n\nAt high packet rates (e.g., >1M PPS), packets arrive back-to-back with minimal gaps:\n\n```\nPacket 1 --> Packet 2 --> Packet 3 --> Packet 4 (continuous stream)\n        ~1µs      ~1µs      ~1µs\n```\n\n- Regular prefetch (after poll) is already effective because:\n  - Prefetch for packet N happens while processing packet N\n  - By the time packet N+1 is polled, the prefetched data is still warm\n  - Very short inter-packet gaps mean L1 cache hasn't been evicted\n\n- The \"before poll\" prefetch becomes redundant:\n  - We're already about to poll and find a packet immediately\n  - Extra prefetch instruction overhead accumulates at millions of PPS\n  - Each poll iteration adds unnecessary prefetch CPU cycles\n\n**Value Range:** 0 to 2044 bytes (0 = disabled, 32-2044 when enabled)\n\n**Performance Impact - VALUE OF 0 (Disabled, Default):**\n\n| Aspect | Impact |\n|--------|--------|\n| CPU overhead per poll | LOWEST - no speculative prefetch |\n| Low PPS latency | Higher (buffer may be cold when packet arrives) |\n| High PPS throughput | OPTIMAL - no redundant prefetch overhead |\n| Recommended for | High throughput applications, >500K PPS workloads |\n\n**Performance Impact - LOW Values (32-128 bytes):**\n\n| Aspect | Impact |\n|--------|--------|\n| Prefetch overhead | Minimal - 1-2 cache lines per poll |\n| Cache warming | Partial - warms headers + small payload portion |\n| L1 pollution risk | Very low |\n| Best for | Small packet workloads, minimal latency improvement needed |\n\nWith 64 bytes, you prefetch one cache line - enough to warm the IP/UDP headers and ~20-30 bytes of payload.\n\n**Performance Impact - MEDIUM Values (256-512 bytes):**\n\n| Aspect | Impact |\n|--------|--------|\n| Prefetch overhead | Moderate - 4-8 cache lines per poll |\n| Cache warming | Good - headers + substantial payload |\n| L1 pollution risk | Low to moderate |\n| Best for | General low-latency applications, medium packet sizes |\n\n256 bytes (matching the default `prefetch_size`) provides a good balance for typical packet sizes.\n\n**Performance Impact - HIGH Values (1024-2044 bytes):**\n\n| Aspect | Impact |\n|--------|--------|\n| Prefetch overhead | Higher - 16-32 cache lines per poll |\n| Cache warming | Excellent - nearly entire MTU-sized packet |\n| L1 pollution risk | Significant - ~1-2KB of L1 per poll (L1 is typically 32KB) |\n| Best for | Large packets near MTU, latency-critical applications that can afford the overhead |\n\n**Critical Tradeoff: Prefetch Overhead vs. Cache Warming**\n\nEvery poll operation pays the prefetch cost, even if:\n- No packet is waiting (poll returns empty)\n- Application is polling frequently in a tight loop\n- Multiple sockets share the same ring (prefetch executed once per ring poll)\n\n```\nCost per poll = (prefetch_before_poll / 64) prefetch instructions\n\nAt 1M polls/sec with 256 bytes:\n  = 4 prefetch instructions × 1M = 4M extra instructions/sec\n  = Negligible on modern CPUs\n\nAt 10M polls/sec with 1024 bytes:\n  = 16 prefetch instructions × 10M = 160M extra instructions/sec\n  = Potentially measurable overhead\n```\n\n**When to Enable (Non-zero Value):**\n\n1. **Latency-sensitive, low PPS applications:**\n   - Financial trading systems\n   - Real-time control systems\n   - Interactive gaming servers\n   - Any application where sub-microsecond latency improvements matter\n\n2. **Bursty traffic patterns:**\n   - Request-response with idle gaps (HTTP, database queries)\n   - Sporadic sensor data\n   - Event-driven systems with variable inter-arrival times\n\n3. **When you've profiled and see L1 cache misses:**\n   - Use `perf stat -e L1-dcache-load-misses` to measure\n   - If misses are high in RX path, this parameter may help\n\n**When to Keep Disabled (Value 0, Default):**\n\n1. **High throughput, high PPS workloads:**\n   - Streaming applications\n   - Bulk data transfer\n   - Packet forwarding/routing\n\n2. **When regular prefetch (`prefetch_size`) is sufficient:**\n   - Continuous traffic with minimal idle gaps\n   - Multi-threaded applications where cache is always warm\n\n3. **CPU-constrained environments:**\n   - Where every instruction matters\n   - When CPU utilization is already near 100%\n\n**Tuning Recommendations:**\n\n*For ultra-low latency (trading, HFT):*\n- Start with 256 bytes\n- Profile baseline latency\n- Try 128 and 512, measure p99 latency\n- Choose value that minimizes latency variance (std-dev)\n\n*For mixed workloads (web servers, databases):*\n- Try 256 bytes\n- Monitor both latency and throughput\n- If throughput degrades, disable or reduce value\n\n*For request-response patterns:*\n- Enable with value matching typical response size + 40 bytes (headers)\n- Example: 512-byte responses → set to 512 or 576\n\n**Interaction with Other Parameters:**\n\n- **performance.buffers.rx.prefetch_size:** Complementary, not exclusive. `prefetch_size` handles the current packet; `prefetch_before_poll` speculatively warms the next. Both can be enabled simultaneously for maximum cache warming.\n\n- **performance.polling.blocking_rx_poll_usec:** Long blocking polls (high values) benefit more from `prefetch_before_poll` since the idle wait time means buffers go cold.\n\n- **hardware_features.striding_rq:** With Striding RQ, the prefetch target is the next stride position within the current WQE buffer, calculated as `current_buffer + consumed_bytes`. Value is automatically clamped to stride_size.\n\n- **performance.polling.max_rx_poll_batch:** With large batch sizes, prefetch happens once per poll call, not per packet. This reduces overhead for high PPS.\n\n**Profiling Tips:**\n\n1. Measure baseline without prefetch_before_poll:\n   ```bash\n   XLIO_RX_PREFETCH_BYTES_BEFORE_POLL=0 ./your_app\n   ```\n\n2. Enable with default prefetch_size value:\n   ```bash\n   XLIO_RX_PREFETCH_BYTES_BEFORE_POLL=256 ./your_app\n   ```\n\n3. Compare latency distributions:\n   - Focus on p50, p99, p99.9 latencies\n   - Low PPS workloads should show improvement in tail latencies\n   - High PPS workloads may show no improvement or slight degradation\n\n4. Monitor cache effectiveness:\n   ```bash\n   perf stat -e L1-dcache-load-misses,L1-dcache-loads ./your_app\n   ```\n   Look for reduced L1 miss rate with prefetch enabled.\n\nDisable with 0."
                                }
                            }
                        },
                        "tcp_segments": {
                            "type": "object",
                            "description": "TCP segmentation settings.",
                            "properties": {
                                "socket_batch_size": {
                                    "type": "integer",
                                    "default": 64,
                                    "minimum": 1,
                                    "title": "Socket segment batch size",
                                    "description": "Maps to XLIO_TX_SEGS_BATCH_TCP environment variable.\n\n**What This Does:**\nControls how many TCP segment structures (tcp_seg) a socket fetches at once\nfrom the ring's segment cache. Each TCP connection maintains a local cache\nof pre-allocated segment structures to reduce lock contention when sending data.\n\n**What is a tcp_seg?**\nA tcp_seg is a lightweight control structure (~48 bytes) used by the TCP stack\nto track outgoing data segments. Each tcp_seg contains:\n- Pointers to the TCP header and packet buffer (pbuf)\n- Sequence number and segment length\n- Flags for TCP options (MSS, timestamps, window scaling, TSO, zerocopy)\nNote: tcp_seg is NOT the data buffer itself - it's metadata that describes\na TCP segment for transmission, retransmission tracking, and acknowledgment handling.\n\n**Three-Level Caching Architecture:**\nXLIO uses a hierarchical caching system for tcp_seg allocation:\n1. **Global Pool** (g_tcp_seg_pool): Central pool protected by a spinlock.\n   Allocates new segments in large batches (pool_batch_size, default 16384).\n2. **Ring Cache** (per-ring): Each ring maintains a local cache, fetching\n   from the global pool in ring_batch_size batches (default 1024).\n   Protected by a per-ring lock.\n3. **Socket Cache** (per-connection): Each TCP connection has its own cache,\n   fetching from the ring in socket_batch_size batches (this parameter).\n   NO LOCK NEEDED - single-threaded access within a connection.\n\n**How Socket-Level Batching Works:**\n1. When sending TCP data, the socket's tcp_write() needs a tcp_seg structure\n2. Socket first checks its local cache (m_tcp_seg_list)\n3. If empty, it acquires the ring lock and fetches 'socket_batch_size' segments\n4. One segment is used immediately, the rest remain in the socket's local cache\n5. After transmission completes, segments are returned to the socket's cache\n6. Excess segments are returned to the ring when:\n   - Total cached > 2x socket_batch_size, AND\n   - Less than half are in active use\n7. When connection closes, all cached segments return to the ring\n\n**Special Mode: Value of 1 (Direct Allocation)**\nWhen socket_batch_size=1, XLIO uses a different code path:\n- Segments are fetched one at a time directly from the ring\n- No socket-level caching occurs\n- Creates 'hot' segments that stay in CPU cache\n- Each allocation/free touches the ring lock\n- Beneficial for latency-sensitive single-stream scenarios\n\n**Value Tradeoffs:**\n\n*High Values (64, 128, 256) - Optimize for throughput and multi-stream:*\n+ Dramatically fewer ring lock acquisitions per connection\n+ Better sustained throughput for streaming workloads\n+ Lower CPU overhead from lock contention with many concurrent connections\n+ Amortized allocation cost over many send operations\n+ Segments stay 'warm' in the socket's working set\n- Higher memory consumption per connection (N segments × ~48 bytes)\n- Segments stay in connection's cache even during idle periods\n- May delay segment availability to other connections\n- Excess segment return logic adds minor overhead\n\n*Low Values (1, 2, 4) - Optimize for memory and CPU cache efficiency:*\n+ Lower memory footprint per connection\n+ Better segment sharing across connections\n+ Segments more likely to be CPU cache-hot (recently used)\n+ More predictable memory usage\n+ Better for servers with thousands of mostly-idle connections\n- More frequent ring lock acquisitions\n- Higher latency variance due to more frequent fetching\n- Lock contention becomes significant with many active connections\n\n*Value of 1 - Direct allocation mode:*\nFetches one segment per operation directly from ring.\nBenefits:\n+ Maximum CPU cache efficiency - same segments reused rapidly\n+ Minimal per-connection memory overhead\n+ Predictable allocation timing\nDrawbacks:\n- Every send/free operation acquires ring lock\n- Higher lock contention with multiple connections on same ring\n- Not suitable for high-throughput multi-connection scenarios\n\n**Default Value (64) - Balanced for typical workloads:**\nWith 64 segments cached per socket:\n- Memory overhead: ~3KB per connection (64 × 48 bytes)\n- Ring lock acquired roughly once per 64 send operations\n- Good balance between throughput and memory efficiency\n- Handles burst traffic without frequent refills\n\n**Tuning Recommendations:**\n- High-throughput streaming (file transfer, video): Consider 128-256\n- Web servers (many connections, bursty traffic): 32-64 works well\n- High-connection servers (10K+ connections): Use 8-16 to save memory\n- Single high-rate stream (trading, telemetry): Try 1 for cache efficiency\n- Many concurrent streams on same ring: Higher values reduce lock contention\n- Memory-constrained systems: Lower values (8-32)\n- Worker thread mode: Lower values may help since threads share rings\n\n**Interaction with Ring Allocation Logic:**\n- With per_thread ring allocation: each thread's connections share a ring,\n  so higher socket_batch_size reduces ring lock contention between connections\n- With per_socket ring allocation: each connection has its own ring,\n  so socket_batch_size mainly affects memory usage\n- With per_interface ring allocation: all connections share one ring,\n  so higher values significantly reduce lock contention\n\n**Related Parameters:**\n- tcp_segments.ring_batch_size: How many segments ring fetches from global pool\n- tcp_segments.pool_batch_size: Allocation granularity in global pool\n- performance.rings.rx.allocation_logic: Determines ring sharing patterns"
                                },
                                "ring_batch_size": {
                                    "type": "integer",
                                    "default": 1024,
                                    "minimum": 1,
                                    "title": "Ring segment batch size",
                                    "description": "Maps to XLIO_TX_SEGS_RING_BATCH_TCP environment variable.\n\n**What This Does:**\nControls how many TCP segment structures (tcp_seg) a ring fetches at once\nfrom the global segment pool when its local cache is empty. Each ring\nmaintains a local cache to reduce contention on the global pool's spinlock.\n\n**Three-Level Caching Architecture Position:**\nThis parameter controls the middle tier of XLIO's hierarchical caching:\n1. **Global Pool** (g_tcp_seg_pool): Central pool protected by a spinlock.\n   Allocates new segments in large batches (pool_batch_size, default 16384).\n2. **Ring Cache** (per-ring): THIS LEVEL - Each ring maintains a local cache,\n   fetching from the global pool in ring_batch_size batches.\n   Protected by a per-ring lock (m_tcp_seg_lock).\n3. **Socket Cache** (per-connection): Each TCP connection has its own cache,\n   fetching from the ring in socket_batch_size batches (default 64).\n\n**How Ring-Level Caching Works:**\n1. When a socket's cache is empty, it calls ring->get_tcp_segs(socket_batch_size)\n2. Ring acquires its own lock (m_tcp_seg_lock) - separate from global pool lock\n3. If ring cache has enough segments, they're returned immediately\n4. If ring cache is empty or insufficient, ring acquires global pool spinlock\n   and fetches 'ring_batch_size' segments in one operation\n5. Requested segments go to socket, remainder stays in ring cache\n6. When sockets return segments, they go back to ring cache first\n7. Ring returns segments to global pool when:\n   - Ring cache size exceeds 2 × ring_batch_size (the return threshold)\n   - At that point, half of the cached segments are returned to global pool\n\n**The Return Threshold Mechanism:**\nThe return threshold = 2 × ring_batch_size (e.g., 2048 with default 1024).\nThis creates a hysteresis effect that prevents thrashing:\n- Ring cache grows as sockets return segments\n- Once it exceeds 2 × ring_batch_size, half are returned to global pool\n- This keeps ring cache between ring_batch_size and 2 × ring_batch_size\n- Prevents constant fetching/returning cycles\n\n**Value Tradeoffs:**\n\n*High Values (1024, 2048, 4096) - Optimize for reduced global lock contention:*\n+ Fewer global pool lock acquisitions per ring\n+ Better performance when many rings are active simultaneously\n+ More segments stay 'local' to each ring\n+ Higher return threshold means more buffer against spikes\n+ Good for per_thread or per_socket ring allocation with many threads\n- Higher memory consumption per ring (N segments × ~48 bytes per ring)\n- More segments 'stuck' in ring caches during idle periods\n- May starve other rings if one ring accumulates many segments\n- With per_interface allocation (single ring), benefits are minimal\n\n*Low Values (128, 256, 512) - Optimize for memory and segment distribution:*\n+ Lower memory overhead per ring\n+ Better segment sharing across rings via global pool\n+ Lower return threshold - segments return to global pool sooner\n+ More responsive to workload changes across rings\n+ Better for memory-constrained systems\n- More frequent global pool lock acquisitions\n- Higher latency variance during segment fetch operations\n- Global pool spinlock becomes bottleneck with many active rings\n\n**Interaction with Ring Allocation Logic:**\nThe impact of this parameter depends heavily on how rings are allocated:\n\n- **per_interface** (all connections share one ring):\n  Ring batch size matters less - single ring rarely needs global pool refills\n  after initial warmup. Consider lower values to save memory.\n\n- **per_thread** (default - ring per thread):\n  Moderate impact. With N threads, you have N rings each maintaining up to\n  2 × ring_batch_size segments. Total memory: N × 2 × 1024 × 48 ≈ 100KB × N.\n  Higher values reduce global lock contention between threads.\n\n- **per_socket** (ring per socket):\n  Significant impact! With thousands of sockets, you could have thousands\n  of rings. Consider lower values (256-512) to avoid memory explosion.\n  Example: 1000 sockets × 2048 segments × 48 bytes = ~94MB just for segments.\n\n- **per_cpuid/per_core**:\n  Similar to per_thread. Ring count bounded by CPU count.\n\n**Default Value (1024) - Balanced for typical deployments:**\n- Return threshold: 2048 segments per ring\n- Memory overhead: ~98KB per ring (2048 × 48 bytes)\n- Fetches from global pool roughly every 16 socket refills (1024/64)\n- Good balance between lock reduction and memory efficiency\n\n**Tuning Recommendations:**\n\n- **High-throughput, few threads (1-4):** Default 1024 works well.\n  Low ring count means memory isn't a concern; focus on throughput.\n\n- **Many threads (32+) with per_thread allocation:**\n  Consider reducing to 512 to limit total memory.\n  32 threads × 2 × 1024 × 48 = ~3MB vs ~1.5MB with 512.\n\n- **Per-socket allocation with many connections:**\n  Use 256-512. Memory scales linearly with connection count.\n  With 10K connections: 10K × 2 × 256 × 48 = ~235MB.\n\n- **Per-interface allocation:**\n  Can use lower values (256-512) since there's only one ring.\n  Memory savings are minimal but no downside.\n\n- **Worker thread mode:**\n  Rings are shared more broadly; higher values may help reduce\n  global pool contention between worker threads.\n\n- **Memory-constrained systems:**\n  Lower to 256-512. Tradeoff is more global lock contention.\n\n**Related Parameters:**\n- tcp_segments.socket_batch_size: How many segments socket fetches from ring\n- tcp_segments.pool_batch_size: Allocation granularity in global pool\n- performance.rings.rx.allocation_logic: Determines how many rings exist"
                                },
                                "pool_batch_size": {
                                    "type": "integer",
                                    "default": 16384,
                                    "minimum": 1,
                                    "title": "Pool segment batch size",
                                    "description": "Maps to XLIO_TX_SEGS_POOL_BATCH_TCP environment variable.\n\n**What This Does:**\nControls the allocation granularity of the global TCP segment pool (g_tcp_seg_pool).\nWhen the pool needs more segments, it allocates this many tcp_seg structures\nin a single memory allocation from the heap. This is the bottom tier of XLIO's\nthree-level segment caching architecture.\n\n**Three-Level Caching Architecture Position:**\nThis parameter controls the bottom tier - the global pool:\n1. **Global Pool** (g_tcp_seg_pool): THIS LEVEL - Central pool protected by a spinlock.\n   When exhausted, allocates 'pool_batch_size' new segments from heap memory.\n   Segments are pre-linked into a free list for O(1) allocation.\n2. **Ring Cache** (per-ring): Middle tier, fetches from global pool in\n   ring_batch_size batches (default 1024).\n3. **Socket Cache** (per-connection): Top tier, fetches from ring in\n   socket_batch_size batches (default 64).\n\n**How Global Pool Allocation Works:**\n1. Application starts, global pool is initialized with 'pool_batch_size' segments\n2. Rings fetch segments from pool as connections are established\n3. When pool is empty and a ring needs segments, pool calls expand():\n   - Allocates pool_batch_size × sizeof(tcp_seg) bytes (~1.6MB with default)\n   - Creates a linked list of tcp_seg structures\n   - Prepends to existing free list\n   - Statistics tracked: total_objs, allocations, expands\n4. Segments return to pool when rings' caches overflow\n5. Segments are NEVER deallocated - pool only grows (no shrinking)\n\n**Memory Calculation:**\nEach tcp_seg structure is approximately 100 bytes (including the l2_l3_tcphdr_zc\narray for zerocopy header storage). With default pool_batch_size of 16384:\n- Initial allocation: 16384 × ~100 bytes ≈ 1.6MB\n- Each expansion adds another ~1.6MB\n- Memory is never returned to OS until process exit\n\n**Value Tradeoffs:**\n\n*High Values (16384, 32768, 65536) - Optimize for fewer allocations:*\n+ Fewer heap allocations over application lifetime\n+ Less memory fragmentation (larger contiguous blocks)\n+ Lower allocation overhead during traffic spikes\n+ Better for long-running servers that reach steady state\n+ Pre-allocates enough for thousands of concurrent segments\n+ Segments are contiguous in memory - better for CPU prefetching\n- Higher initial memory footprint (even if not all segments are used)\n- Memory committed upfront that might not be needed\n- Each expansion is a larger memory commitment\n- Overkill for applications with few connections\n\n*Low Values (256, 512, 1024) - Optimize for memory efficiency:*\n+ Lower initial memory footprint\n+ Memory grows more gradually with actual demand\n+ Better for memory-constrained environments\n+ More appropriate for applications with predictable, low connection counts\n+ Segments still contiguous within each allocation batch\n- More frequent heap allocations during warmup phase\n- More memory fragmentation (many small allocations)\n- Brief latency spikes during allocation events\n- With many concurrent segments, triggers many expand() calls\n\n**Impact on Performance:**\n\n*During Warmup Phase:*\nWhen application starts and connections are established:\n- Lower values: More expand() calls, each holding global pool spinlock\n- Higher values: Fewer expand() calls, but each is larger\nThe expand() operation includes heap allocation and linked list setup,\nso minimizing calls reduces warmup latency variance.\n\n*During Steady State:*\nOnce enough segments are allocated, pool_batch_size has minimal impact:\n- Segments cycle between socket → ring → pool → ring → socket\n- expand() is rarely called\n- Pool acts as a reservoir for bursty traffic across rings\n\n*During Traffic Spikes:*\nIf traffic spike exceeds pooled segments:\n- Lower values: Multiple expand() calls, lock contention during each\n- Higher values: Single larger expand(), one-time latency hit\n\n**Relationship with Other Batch Parameters:**\nThe three batch sizes form a hierarchy:\n- socket_batch_size (64): Fastest path, no lock\n- ring_batch_size (1024): Per-ring lock, 16× socket batches\n- pool_batch_size (16384): Global spinlock, 16× ring batches\n\nThe 16× ratios mean:\n- Pool can satisfy ~16 ring refills before needing to expand\n- Ring can satisfy ~16 socket refills before needing pool access\n- This reduces frequency of more expensive (locked) operations\n\n**Tuning Recommendations:**\n\n- **High-performance servers (long-running, high traffic):**\n  Default 16384 or higher. Pre-allocates generously for sustained load.\n  Memory cost (~1.6MB per batch) is negligible for such deployments.\n\n- **Latency-sensitive applications:**\n  Consider 32768 or 65536 to minimize expand() calls during operation.\n  Trading memory for predictability - avoid allocation during trading hours.\n\n- **Memory-constrained systems:**\n  Use 2048-4096. More expand() calls but lower memory ceiling.\n  Each batch is ~200-400KB instead of ~1.6MB.\n\n- **Applications with few connections (<100):**\n  Lower to 1024-2048. Don't pre-allocate for scale you won't need.\n  100 connections × ~10 segments each = ~1000 segments typical max.\n\n- **Containerized/serverless workloads:**\n  Lower values (1024-4096) match resource limits.\n  Short-lived instances don't benefit from large pools.\n\n- **High connection count servers (10K+):**\n  Default 16384 or higher. Many connections = many concurrent segments.\n  With 10K connections, peak might need 50K+ segments across all queues.\n\n**Monitoring and Debugging:**\nUse xlio_stats to monitor pool behavior:\n- n_tcp_seg_pool_size: Current number of segments in global pool\n- n_tcp_seg_pool_no_segs: Count of allocation failures (pool exhausted)\n  High values indicate pool_batch_size or total memory is insufficient.\n\nAt shutdown, XLIO logs pool statistics:\n  \"TCP segments pool statistics:\"\n  \"  allocations=N expands=M total_segs=P\"\nMany 'expands' suggests pool_batch_size could be increased.\n\n**Related Parameters:**\n- tcp_segments.socket_batch_size: How many segments socket fetches from ring\n- tcp_segments.ring_batch_size: How many segments ring fetches from global pool\n- performance.rings.rx.allocation_logic: Determines ring count (affects total segment demand)"
                                }
                            }
                        }
                    },
                    "additionalProperties": false
                },
                "max_gro_streams": {
                    "type": "integer",
                    "default": 32,
                    "minimum": 0,
                    "title": "Maximum GRO streams",
                    "description": "Maps to XLIO_GRO_STREAMS_MAX environment variable.\nControls the maximum number of TCP flows that can perform Generic Receive Offload (GRO)\nsimultaneously per ring. GRO aggregates multiple consecutive TCP segments from the\nsame flow into larger packets before delivering them to the application.\n\n**What is GRO (Generic Receive Offload)?**\n\nGRO is a software technique that coalesces multiple small TCP packets from the same\nconnection into fewer, larger packets. When packets arrive with consecutive sequence\nnumbers, identical TCP flags, and compatible timestamps, XLIO chains their payloads\ntogether and delivers them as a single aggregated packet to the application.\n\nThis reduces:\n- Per-packet processing overhead (fewer recv() callbacks)\n- TCP/IP stack traversal costs\n- Application context switches\n- Cache pollution from processing many small packets\n\n**How It Works Internally:**\n\n1. Each ring maintains a gro_mgr with slots for max_gro_streams simultaneous flows\n2. When a TCP flow receives its first packet, it attempts to reserve a GRO slot\n3. If a slot is available, subsequent packets are aggregated (up to 32 packets or 64KB)\n4. Aggregation flushes when:\n   - Sequence number gap detected (out-of-order packet)\n   - TCP flags change (PSH, FIN, RST, etc.)\n   - Aggregation limit reached (32 packets or ~64KB)\n   - Poll batch completes (end of CQ processing)\n5. If no slot is available (max_gro_streams reached), the flow bypasses GRO entirely\n   and packets are delivered individually (still works, just without coalescing)\n\n**Aggregation Limits (per flow):**\n- Maximum packets per aggregation: 32 (MAX_GRO_BUFS)\n- Maximum bytes per aggregation: ~64KB (MAX_AGGR_BYTE_PER_STREAM minus MTU)\n\n**Memory Overhead:**\nVery low: ~8 bytes per slot (pointer) plus ~100 bytes per active flow's state.\nTotal per ring: max_gro_streams × ~8 bytes for the pointer array.\nExample: 32 streams × 8 bytes = 256 bytes per ring (negligible).\n\n**Value of 0 - GRO Disabled:**\n\nWhen set to 0, GRO is completely disabled:\n- XLIO uses rfs_uc (regular flow steering) instead of rfs_uc_tcp_gro\n- Every TCP packet is delivered individually to the application\n- No aggregation delay - lowest per-packet latency\n- Higher CPU overhead at high packet rates (more recv() calls)\n- No coalescing statistics tracked\n\nBest for:\n- Ultra-low-latency applications where every microsecond matters\n- Workloads with small, latency-sensitive messages\n- Debugging scenarios where packet-level visibility is needed\n- Applications that process each packet immediately anyway\n\n**Low Values (1-16) - Limited GRO:**\n\nBenefits:\n- Only a few flows benefit from aggregation\n- Lower memory allocation per ring\n- Suitable for applications with few concurrent TCP connections\n- Most flows bypass GRO (no aggregation delay)\n\nDrawbacks:\n- High-connection-count workloads see minimal GRO benefit\n- Flows that exceed the limit process packets individually\n- May not justify the GRO overhead for few connections\n\nBest for:\n- Single-connection or few-connection applications\n- Latency-sensitive apps with occasional bulk transfers\n- Testing/benchmarking GRO impact on specific flows\n\n**Medium Values (32-64) - Balanced (Default: 32):**\n\nBenefits:\n- Good coverage for typical server workloads (web servers, proxies)\n- Most active TCP connections can benefit from GRO\n- Balanced between throughput gains and resource usage\n- Suitable for connection counts typical in most deployments\n\nDrawbacks:\n- Very high connection counts may still exceed the limit\n- Some flows may miss GRO during connection bursts\n\nBest for:\n- Web servers handling moderate concurrent connections\n- Proxy applications (NGINX, HAProxy)\n- General-purpose TCP servers\n- Most typical deployment scenarios\n\n**High Values (128-512) - Maximum GRO Coverage:**\n\nBenefits:\n- More concurrent TCP flows can benefit from GRO\n- Better throughput for high-connection-count workloads\n- Fewer packets delivered individually\n- Higher CPU efficiency at scale\n\nDrawbacks:\n- Larger pointer array allocation per ring (still small: 512 × 8 = 4KB)\n- Marginal benefit if actual concurrent flows are fewer than the limit\n- No benefit for UDP traffic (GRO is TCP-only)\n\nBest for:\n- High-connection-count servers (1000s of connections)\n- CDN edge servers, load balancers\n- Applications with many simultaneous bulk transfers\n- Scenarios where connection count regularly exceeds default (32)\n\n**Performance Impact Summary:**\n\n| Metric                    | 0 (Disabled) | Low (1-16) | Default (32) | High (128+) |\n|---------------------------|--------------|------------|--------------|-------------|\n| Latency per packet        | Lowest       | Low        | Slightly higher | Slightly higher |\n| Throughput (high PPS)     | Lower        | Medium     | Good         | Best        |\n| CPU efficiency (high load)| Lower        | Medium     | Good         | Best        |\n| recv() calls per MB       | Many         | Many       | Fewer        | Fewest      |\n| Memory per ring           | None         | ~128B      | ~256B        | ~4KB        |\n| Connections benefiting    | None         | Few        | Moderate     | Many        |\n\n**GRO vs LRO (Large Receive Offload):**\n\n- LRO (hardware_features.lro): Hardware-based aggregation performed by the NIC\n- GRO (max_gro_streams): Software-based aggregation performed by XLIO\n- Both can work together: LRO aggregates at hardware level, GRO provides\n  additional software coalescing if hardware aggregation was incomplete\n- GRO is always available; LRO depends on NIC capabilities and settings\n\n**Interaction with Other Parameters:**\n\n- **hardware_features.lro:** Hardware LRO reduces packets before GRO sees them.\n  With strong LRO, GRO may have fewer packets to aggregate.\n- **performance.polling.max_rx_poll_batch:** GRO flushes at end of each poll batch.\n  Larger batches allow more packets to accumulate for aggregation.\n- **Ring allocation logic:** GRO slots are per-ring. With per-thread rings,\n  each thread has its own max_gro_streams budget.\n\n**Monitoring GRO Effectiveness:**\n\nUse xlio_stats to monitor:\n- n_rx_gro_packets: Count of aggregated (coalesced) packets delivered\n- n_rx_gro_frags: Total individual packets that were aggregated\n- n_rx_gro_bytes: Total bytes processed through GRO\n- Avg GRO packet size: n_rx_gro_bytes / n_rx_gro_packets (higher = better coalescing)\n- GRO frags per packet: n_rx_gro_frags / n_rx_gro_packets (higher = more aggregation)\n\nIf n_rx_gro_frags/n_rx_gro_packets is close to 1.0, packets aren't being aggregated\n(possibly out-of-order arrival, small messages, or max_gro_streams too low).\n\n**Tuning Recommendations:**\n\n1. **Start with default (32)** - optimal for most workloads\n\n2. **For ultra-low-latency (HFT, real-time):**\n   - Set to 0 to disable GRO entirely\n   - Eliminates any aggregation-induced latency\n\n3. **For high-throughput bulk transfer:**\n   - Keep default or increase to 64-128\n   - Monitor GRO statistics to verify aggregation is occurring\n\n4. **For high connection counts (1000+):**\n   - Increase to 128-256 to ensure most flows benefit\n   - Check if n_rx_gro_frags >> n_rx_gro_packets (good aggregation)\n\n5. **For memory-constrained environments:**\n   - Default (32) uses only ~256 bytes per ring\n   - Even high values (256) use only ~2KB per ring\n   - Memory is not a significant concern for this parameter"
                },
                "override_rcvbuf_limit": {
                    "type": "integer",
                    "default": 65536,
                    "minimum": 0,
                    "title": "Override OS receive buffer limit",
                    "description": "Maps to XLIO_RX_BYTES_MIN environment variable.\n\n**What This Does:**\nSets the minimum socket receive buffer limit (in bytes) for **UDP sockets only**.\nWhen an application calls setsockopt(SO_RCVBUF) with a value smaller than this parameter,\nXLIO overrides the requested value and uses this minimum instead.\n\nThis parameter controls the maximum total bytes of received UDP datagrams that XLIO\nwill hold in a socket's ready queue waiting for the application to read them.\nWhen this limit is reached, new incoming datagrams are silently dropped.\n\n**Note:** This parameter does NOT affect TCP sockets. TCP uses flow control via\nthe receive window mechanism (controlled by SO_RCVBUF directly and system tcp_rmem settings),\nwhich prevents packet loss by signaling the sender to slow down.\n\n**How the Receive Buffer Works:**\n\n1. UDP packets arrive from the network and are placed in the socket's ready queue\n2. XLIO tracks total bytes in queue (m_rx_ready_byte_count)\n3. When bytes >= limit, new packets are dropped (returned false from rx_input_cb)\n4. Dropped packets are counted in statistics (n_rx_ready_byte_drop)\n5. When application calls recv()/recvfrom(), packets are removed from queue\n6. Applications can increase the limit via setsockopt(SO_RCVBUF), but cannot\n   decrease it below this minimum parameter value\n\n**Relationship with SO_RCVBUF:**\n\nWhen a UDP socket is created:\n1. XLIO reads the kernel's default SO_RCVBUF value (typically from net.core.rmem_default)\n2. The kernel value is doubled (Linux convention: kernel doubles the requested value)\n3. XLIO enforces: effective_limit = max(kernel_value, override_rcvbuf_limit)\n\nWhen application calls setsockopt(SO_RCVBUF, value):\n1. XLIO receives value×2 (kernel doubling)\n2. XLIO enforces: effective_limit = max(value×2, override_rcvbuf_limit)\n3. If current queue exceeds new limit, oldest packets are dropped to fit\n\n**Memory Impact:**\n\nThis is a per-socket limit on packet payload bytes held in the ready queue.\nActual memory consumption includes packet descriptors and headers, roughly:\n  Memory per socket ≈ limit × 1.1 (10% overhead for descriptors)\n\nWith many UDP sockets, total memory = num_sockets × limit × 1.1\nExample: 1000 UDP sockets × 64KB × 1.1 ≈ 70MB potential memory\n\n**High Values (256KB - 16MB) - Better Burst Tolerance:**\n\nBenefits:\n- Absorbs traffic bursts when application temporarily cannot keep up\n- Tolerates application processing jitter (GC pauses, context switches)\n- Handles high-bandwidth multicast streams with variable consumption rates\n- Reduces packet loss during brief application stalls\n- Suitable for applications doing batch processing of received data\n\nDrawbacks:\n- Higher per-socket memory consumption\n- Risk of buffering stale data that becomes irrelevant by read time\n- Increases end-to-end latency (packets wait longer in buffer)\n- Can mask application performance problems (appears to work, but with delay)\n- Memory tied up even during idle periods\n\nBest for:\n- Multicast video/audio streaming (buffering helps smooth playback)\n- Batch data collection applications\n- Applications with variable processing rates\n- High-bandwidth bulk UDP transfers\n- Scenarios where some latency is acceptable to avoid data loss\n\n**Low Values (4KB - 32KB) - Better for Real-Time:**\n\nBenefits:\n- Forces applications to consume data promptly or lose it\n- Minimizes latency - no stale data sitting in buffers\n- Lower per-socket memory footprint\n- Better for real-time applications where old data is worthless\n- Dropped packets signal application is too slow (useful feedback)\n- Encourages application to keep up with wire speed\n\nDrawbacks:\n- Higher packet drop rate during any processing hiccup\n- No tolerance for application jitter or brief stalls\n- Requires application to be consistently fast\n- May lose packets during legitimate short delays\n\nBest for:\n- Real-time trading/financial data (stale quotes are harmful)\n- Live sensor data where only latest reading matters\n- Gaming and interactive applications\n- Low-latency signaling protocols\n- Applications that can afford to miss some updates\n\n**Default Value (65536 = 64KB):**\n\nThe default provides a reasonable balance:\n- Handles typical network jitter and small bursts (~40-50 full-size packets)\n- ~64KB per socket is modest memory overhead\n- Sufficient for most general-purpose UDP applications\n- Not so large that it masks performance issues\n- Matches common system defaults for socket buffers\n\n**Value of 0 - No Minimum Enforcement:**\n\nWhen set to 0, XLIO does not enforce a minimum:\n- Application's setsockopt(SO_RCVBUF) is used directly\n- Application can set arbitrarily small buffers\n- Useful when application explicitly wants minimal buffering\n- Risk: Application might accidentally set too small a value\n\n**Performance Tradeoff Summary:**\n\n| Value Range  | Packet Loss  | Latency      | Memory    | Best Use Case            |\n|--------------|--------------|--------------|-----------|--------------------------||\n| 0            | App decides  | App decides  | Variable  | Advanced tuning          |\n| 4KB-32KB     | Higher       | Lowest       | Low       | Real-time, latency-critical |\n| 64KB (default)| Moderate    | Low-moderate | Moderate  | General purpose          |\n| 256KB-1MB    | Lower        | Higher       | High      | Burst tolerance, streaming |\n| 2MB-16MB     | Lowest       | Highest      | Very high | Bulk transfer, buffering |\n\n**Monitoring and Diagnostics:**\n\nUse xlio_stats to monitor per-socket buffer usage:\n\n- n_rx_ready_byte_count: Current bytes in receive ready queue\n- n_rx_ready_byte_max: High-water mark (maximum bytes seen)\n- n_rx_ready_byte_drop: Bytes dropped due to buffer limit\n- n_rx_ready_pkt_drop: Packets dropped due to buffer limit\n\nWarning signs:\n- n_rx_ready_byte_drop > 0: Packets are being lost, consider increasing limit\n  or improving application consumption rate\n- n_rx_ready_byte_max ≈ limit: Buffer is filling up regularly\n- n_rx_ready_byte_count consistently high: Application not keeping up\n\nOn socket close, XLIO logs drop statistics if packets were lost:\n  \"Rx byte : max X / dropped Y (Z%)\"\n\n**Interaction with Other Parameters:**\n\n- **performance.rings.rx.ring_elements_count:** Hardware queue depth is separate\n  from this socket-level buffer. Packets first arrive in hardware RQ, then are\n  moved to the socket's ready queue (subject to this limit).\n\n- **performance.completion_queue.keep_full:** Controls behavior when hardware\n  buffers are exhausted - different from this socket-level limit.\n\n- **System sysctl net.core.rmem_max:** Maximum value applications can request\n  via setsockopt(SO_RCVBUF). This XLIO parameter enforces a minimum, while\n  rmem_max enforces a maximum.\n\n- **System sysctl net.core.rmem_default:** Default SO_RCVBUF for new sockets.\n  XLIO uses max(rmem_default×2, override_rcvbuf_limit) as the initial limit.\n\n**Tuning Recommendations:**\n\n1. **Start with default (65536)** - suitable for most applications\n\n2. **For real-time/low-latency applications:**\n   - Reduce to 8192-16384 bytes\n   - Application must be able to keep up with arrival rate\n   - Monitor n_rx_ready_byte_drop to understand loss rate\n\n3. **For high-bandwidth streaming/multicast:**\n   - Increase to 256KB-1MB\n   - Allows buffering during processing spikes\n   - Watch memory consumption with many sockets\n\n4. **For financial trading/market data:**\n   - Low values (4KB-16KB) preferred - stale data is worse than no data\n   - Application should process faster than data arrives\n   - Drops indicate application is too slow (useful signal)\n\n5. **For bulk UDP data transfer:**\n   - Higher values (512KB-4MB) improve throughput\n   - Reduces drops during application processing variations\n   - Trade-off: Increased memory and latency\n\n6. **If seeing packet drops (n_rx_ready_byte_drop > 0):**\n   - First: Check if application is reading fast enough\n   - If application is optimal: Increase this limit\n   - If limit is already high: Application fundamentally cannot keep up\n\n**Example Calculations:**\n\nFor a 10 Gbps multicast stream with 1500-byte packets:\n- Packets per second: ~833,333 pps\n- At 64KB buffer: can hold ~43 packets = ~52 microseconds of data\n- At 1MB buffer: can hold ~683 packets = ~820 microseconds of data\n- At 16MB buffer: can hold ~10,922 packets = ~13 milliseconds of data\n\nChoose based on how long your application might stall during processing."
                }
            },
            "additionalProperties": false
        },
        "applications": {
            "type": "object",
            "title": "Application Integration",
            "description": "Configurations and optimizations for specific applications or use cases.",
            "properties": {
                "nginx": {
                    "type": "object",
                    "description": "NGINX-specific optimizations.",
                    "properties": {
                        "src_port_stride": {
                            "type": "integer",
                            "default": 2,
                            "minimum": 2,
                            "title": "Source port stride",
                            "description": "Maps to XLIO_NGINX_SRC_PORT_STRIDE environment variable.\nControls how incoming connections are distributed across Nginx worker processes using hardware flow steering rules based on source port patterns.\n\n**How It Works:**\nXLIO creates hardware flow steering rules that match incoming packets based on their source port.\nEach Nginx worker receives a unique \"slot\" in the source port space, determined by:\n- mask = (workers_pow2 * src_port_stride) - 2\n- value = worker_id * src_port_stride\n\nWhere workers_pow2 is the number of workers rounded up to the next power of 2.\nPackets are routed to a worker when: (packet_src_port & mask) == value\n\n**Why Minimum is 2:**\nThe minimum value of 2 is critical because the formula subtracts 2 from the mask,\nwhich excludes bit 0 (the least significant bit) from flow steering decisions.\nThis is essential because client ephemeral source ports have no predictable pattern\nin their LSB - including it would cause uneven distribution.\n\n**Example with 4 workers and stride=2 (default):**\n- workers_pow2 = 4\n- mask = (4 * 2) - 2 = 6 (binary: 110, checking bits 1-2)\n- Worker 0: value=0, receives packets where (src_port & 6) == 0\n- Worker 1: value=2, receives packets where (src_port & 6) == 2\n- Worker 2: value=4, receives packets where (src_port & 6) == 4\n- Worker 3: value=6, receives packets where (src_port & 6) == 6\n\n**Low Values (e.g., 2 - default):**\nBenefits:\n- Supports more workers with the same hardware flow steering capacity\n- Compact match values use fewer bits in the 16-bit port mask\n- Works well with high worker counts (8, 16, 32+ Nginx workers)\n- Lower hardware resource usage per steering rule\n\nDrawbacks:\n- Connection distribution depends more heavily on client source port patterns\n- Adjacent workers may receive similar traffic volumes if client ports cluster\n\n**High Values (e.g., 4, 8, 16):**\nBenefits:\n- May provide more uniform distribution for certain client traffic patterns\n- Greater separation between worker match values reduces steering conflicts\n- Can help when clients generate ports with specific bit patterns\n\nDrawbacks:\n- Limits maximum supported workers: (workers_pow2 * stride) must fit reasonably\n  in the 16-bit port space to allow meaningful distribution\n- Uses more bits in the port mask, potentially overlapping with meaningful port ranges\n- Diminishing returns for most real-world traffic patterns\n\n**Non-Power-of-2 Workers:**\nWhen workers_num is not a power of 2 (e.g., 3, 5, 6, 7 workers), XLIO automatically:\n1. Rounds up to the next power of 2 for flow steering rules\n2. Creates secondary rules for some workers to balance the load\n3. Workers with lower IDs receive additional connection slots\n\nFor example, with 3 workers (rounded to 4 slots):\n- Worker 0: receives 2 slots (primary + secondary rule)\n- Worker 1: receives 1 slot\n- Worker 2: receives 1 slot\n\n**Recommended Settings:**\n- For most deployments: Use default value of 2\n- For power-of-2 worker counts (2, 4, 8, 16): Default works optimally\n- For high worker counts (32+): Stick with 2 to maximize supported workers\n- Only increase if you observe uneven distribution with default settings\n  and have analyzed your client source port patterns"
                        },
                        "workers_num": {
                            "type": "integer",
                            "default": 0,
                            "minimum": 0,
                            "title": "Number of Nginx workers",
                            "description": "Maps to XLIO_NGINX_WORKERS_NUM environment variable.\nNumber of Nginx worker processes to optimize for.\n**This parameter must be set to a value > 0 to enable XLIO offloading for Nginx.**\n\n**How It Works:**\nWhen workers_num > 0, XLIO:\n1. Enables the nginx profile with optimized settings (ring per interface, TSO, 3-tuple rules, etc.)\n2. Creates hardware flow steering rules to distribute incoming connections across workers\n3. Allocates memory resources scaled to the number of workers\n4. Assigns each worker a unique ID (0 to workers_num-1) for flow steering\n\n**Flow Steering Mechanism:**\nEach Nginx worker receives its own hardware flow steering rule based on source port masking.\nThe formula uses:\n- workers_pow2 = workers_num rounded up to the next power of 2\n- mask = (workers_pow2 * src_port_stride) - 2\n- value = worker_id * src_port_stride\n\nIncoming packets are routed to a worker when: (packet_src_port & mask) == value\n\n**Value = 0 (Disabled - Default):**\n- XLIO does NOT offload Nginx traffic\n- Nginx uses standard kernel networking\n- No hardware flow steering rules are created\n- Use this when running non-Nginx applications or when XLIO offloading is not desired\n\n**Low Values (1-4 workers):**\n- Suitable for light workloads or testing\n- Each worker gets dedicated flow steering resources\n- Minimal memory footprint\n- 4GB per-worker memory allocation baseline\n- Best with power-of-2 values (1, 2, 4) for optimal flow steering efficiency\n\n**Medium Values (4-16 workers):**\n- Optimal for most production deployments\n- Good balance between parallelism and resource utilization\n- 4GB per-worker memory allocation baseline\n- Power-of-2 values (4, 8, 16) provide the most efficient flow steering\n  because hardware masks naturally align with worker IDs\n\n**High Values (17-32+ workers):**\n- For high-throughput, high-concurrency workloads\n- Memory baseline reduced to 3GB per worker (to manage total memory consumption)\n- More hardware flow steering rules are created\n- Non-power-of-2 values create additional secondary steering rules to balance load\n- Higher memory and hardware resource consumption\n\n**Power-of-2 vs Non-Power-of-2 Values:**\n\nPower-of-2 values (2, 4, 8, 16, 32, ...) are most efficient because:\n- All workers receive exactly equal traffic distribution\n- No secondary steering rules are needed\n- Simpler hardware flow table usage\n\nNon-power-of-2 values (3, 5, 6, 7, 9, ...) work but have overhead:\n- XLIO rounds up to the next power of 2 (workers_pow2) for mask calculation\n- Some workers receive secondary rules to capture \"orphan\" traffic slots\n- Workers with lower IDs (ID < workers_pow2 % workers_num) get additional steering rules\n- Example with 3 workers (workers_pow2=4):\n  - Worker 0: handles 2 port slots (primary + secondary rule)\n  - Worker 1: handles 1 port slot\n  - Worker 2: handles 1 port slot\n  This creates slightly uneven distribution until traffic volume normalizes it.\n\n**Memory Allocation:**\nTotal memory_limit calculation:\n- workers_num <= 16: 4GB × workers_num total (4GB per worker base)\n- workers_num > 16: 3GB × workers_num total (3GB per worker base)\n\nThe total is then divided among workers, so each Nginx worker process receives:\nmemory_limit / workers_num\n\nThe master Nginx process (before forking workers) uses minimal memory.\n\n**Performance Tradeoffs Summary:**\n\n| workers_num | Flow Steering | Memory/Worker | Best For |\n|-------------|---------------|---------------|----------|\n| 0           | None (kernel) | N/A           | Non-Nginx apps |\n| 1-4         | Simple        | 4GB base      | Light workloads, testing |\n| 4-16        | Optimal       | 4GB base      | Production deployments |\n| 17-32       | Complex       | 3GB base      | High-throughput servers |\n| 33+         | Very Complex  | 3GB base      | Extreme scale (use power-of-2) |\n\n**Recommendations:**\n- Match workers_num exactly to your Nginx worker_processes configuration\n- Prefer power-of-2 values when possible for optimal hardware efficiency\n- For most deployments, 4-16 workers provides the best balance\n- Monitor xlio_stats to verify even distribution across workers\n- When in doubt, start with a power-of-2 value and benchmark"
                        },
                        "udp_pool_size": {
                            "type": "integer",
                            "default": 0,
                            "minimum": 0,
                            "title": "UDP socket pool size",
                            "description": "Maps to XLIO_NGINX_UDP_POOL_SIZE environment variable.\nMaximum number of UDP sockets to keep in a per-worker pool for reuse.\n\n**What This Does:**\nWhen a UDP socket is closed, instead of destroying it and releasing all resources,\nXLIO places the socket into a pool. When a new UDP socket is requested,\nXLIO first checks if a socket is available in the pool and reuses it,\navoiding the expensive socket creation overhead.\n\n**Socket Lifecycle with Pooling:**\n1. On close(): Socket state set to SOCKINFO_DESTROYING, RX buffers dropped,\n   socket pushed to pool stack (no close() syscall)\n2. On socket(): If pool not empty, pop socket, reset state to SOCKINFO_OPENED,\n   return immediately (no socket() syscall, no object construction)\n3. Bonus: connect() calls to the same destination are skipped entirely for pooled sockets\n\n**What Gets Saved Per Reuse:**\n- socket() system call (kernel FD allocation)\n- sockinfo_udp object construction (~400+ bytes + internal structures)\n- Internal epoll_create() and epoll_ctl() calls\n- Socket statistics allocation\n- Ring allocation logic setup\n\n**Value Tradeoffs:**\n\n*Value of 0 (Disabled - Default):*\nStandard behavior. Each close() destroys the socket, each socket() creates new.\nNo additional memory held. Use when UDP socket churn is low.\n\n*Low Values (1-10):*\n+ Minimal memory overhead\n+ Still provides optimization for short-lived connections\n- May exhaust pool quickly under high load, falling back to normal creation\nRecommended for: Light UDP load balancing, moderate connection rates\n\n*Medium Values (20-50):*\n+ Good balance between memory usage and hit rate\n+ Handles typical NGINX worker load patterns well\n- Memory for ~20-50 sockinfo objects per worker\nRecommended for: Standard NGINX UDP deployments\n\n*High Values (100+):*\n+ Near 100% pool hit rate for burst workloads\n+ Eliminates syscall overhead almost entirely\n- Significant memory consumption per worker\n- Each pooled socket holds: kernel FD, sockinfo structure, internal epoll FD\n- Total memory per worker: pool_size × ~2-4KB (depending on configuration)\n- With N workers: total = N × pool_size × 2-4KB\nRecommended for: High-throughput UDP proxying with very high connection churn\n\n**Memory Impact Example:**\nWith 8 NGINX workers and udp_pool_size=100:\n- ~800 pre-allocated sockets system-wide\n- ~1.6-3.2 MB of additional memory held\n- ~800 kernel file descriptors reserved\n\n**Monitoring:**\nUse xlio_stats to monitor pool effectiveness through socket creation/destruction counters.\n\n**Related Parameter:**\nUse with applications.nginx.udp_socket_pool_reuse to control RX buffer recycling\nthresholds for pooled sockets, further optimizing memory behavior."
                        },
                        "udp_socket_pool_reuse": {
                            "type": "integer",
                            "default": 0,
                            "minimum": 0,
                            "title": "RX buffer reclaim threshold for pooled sockets",
                            "description": "Maps to XLIO_NGINX_UDP_POOL_RX_NUM_BUFFS_REUSE environment variable.\nControls when RX buffers are reclaimed for sockets in the UDP socket pool.\n\n**What This Does:**\nWhen a socket is marked for the pool (via applications.nginx.udp_pool_size > 0),\nthis parameter sets the m_rx_num_buffs_reuse threshold - controlling how many\nRX buffers accumulate before being returned to the ring's buffer pool.\n\n**How Buffer Reclamation Works:**\n1. As packets are received, used RX buffers are held in a local reuse queue\n2. When the queue reaches this threshold, buffers are batch-returned to the ring\n3. Lower thresholds = more frequent reclamation, less memory held per socket\n4. Higher thresholds = less frequent reclamation, more memory held per socket\n\n**Value Tradeoffs:**\n\n*Value of 0 (Default):*\nUses the global default (performance.memory.rx.batch_size, typically 64).\nNo special behavior for pooled sockets.\n\n*Low Values (16-32):*\n+ Lower memory footprint per pooled socket\n+ Faster buffer recycling when socket enters pool\n- More frequent lock acquisitions for buffer reclamation\n- Slightly higher CPU overhead per packet\nRecommended for: Memory-constrained environments, many pooled sockets\n\n*Higher Values (128-256+):*\n+ Fewer lock acquisitions for buffer reclamation\n+ Better batching efficiency\n- More memory held per socket while active\n- Longer delay before buffers return to global pool\nRecommended for: High-throughput scenarios, fewer pooled sockets\n\n**Interaction with udp_pool_size:**\nThis parameter only takes effect when applications.nginx.udp_pool_size > 0.\nThe combination affects total memory footprint:\n- Total pooled buffer memory ≈ udp_pool_size × udp_socket_pool_reuse × buffer_size\n\n**Example:**\nWith udp_pool_size=50 and udp_socket_pool_reuse=64:\n- Up to 50 × 64 = 3200 RX buffers may be held across pooled sockets\n- At ~2KB per buffer, this is ~6.4MB per worker"
                        },
                        "distribute_cq": {
                            "type": "boolean",
                            "default": false,
                            "title": "Distribute completion queue interrupts across workers",
                            "description": "Maps to XLIO_DISTRIBUTE_CQ environment variable.\nControls whether Completion Queue (CQ) interrupts are distributed across different CPU cores based on worker ID.\n\n**What is a Completion Vector?**\nIn RDMA, each CQ can be associated with a \"completion vector\" - an index that maps to a specific MSI-X interrupt line on the NIC. Each MSI-X interrupt is typically serviced by a specific CPU core. The number of available completion vectors (context->num_comp_vectors) usually equals the number of CPU cores or MSI-X vectors supported by the NIC.\n\n**How This Parameter Works:**\nWhen enabled and running as a worker process (not master), CQs are created with:\n   comp_vector = worker_id % num_comp_vectors\n\nWhen disabled (default), all CQs use comp_vector=0, meaning all CQ completion interrupts are handled by the same CPU (typically CPU 0).\n\n**When Does This Matter?**\nThis parameter affects interrupt-driven I/O paths. When applications use epoll/blocking I/O and wait for CQ completion events (via completion channels), the kernel delivers interrupts to wake up the waiting process. The completion vector determines which CPU handles these interrupts.\n\nIn pure polling mode (where applications continuously poll CQs without waiting for interrupts), this parameter has no effect since interrupts are not used.\n\n**Value Tradeoffs:**\n\n*false (disabled, default):*\n- All CQ interrupts go to completion vector 0 (same CPU)\n- Simple, predictable interrupt handling\n- May create CPU hotspotting under high connection rates (CPS)\n- CPU 0 becomes a bottleneck for interrupt processing in multi-worker setups\n- Best for: Single-worker applications, polling-only workloads, low CPS scenarios\n\n*true (enabled):*\n- CQ interrupts distributed across CPUs based on worker ID\n- Spreads interrupt processing load across multiple CPU cores\n- Improves CPS (Connections Per Second) in multi-process scenarios\n- Each worker's CQs use a different completion vector, avoiding interrupt contention\n- Reduces CPU hotspotting on the default interrupt handler CPU\n- Best for: Multi-worker Nginx/Envoy, interrupt-driven I/O, high CPS workloads\n\n**Profile Behavior:**\nThis parameter is automatically set to true when using the nginx profile (via applications.nginx.workers_num > 0). The nginx profile assumes multi-process operation where interrupt distribution is beneficial.\n\n**NUMA Considerations:**\nFor optimal performance with distribute_cq enabled, consider:\n- Aligning worker CPU affinity with NIC NUMA node\n- Ensuring completion vectors map to CPUs on the same NUMA node as the NIC\n- Use irqbalance or manual smp_affinity configuration to control MSI-X to CPU mapping\n\n**Example Scenario:**\nWith 8 Nginx workers and a NIC with 16 completion vectors:\n- Worker 0: CQs use comp_vector=0 (interrupts on CPU 0)\n- Worker 1: CQs use comp_vector=1 (interrupts on CPU 1)\n- ...\n- Worker 7: CQs use comp_vector=7 (interrupts on CPU 7)\n\nThis spreads interrupt handling across 8 CPUs instead of concentrating all on CPU 0.\n\n**When to Enable:**\n- Running Nginx/Envoy with multiple workers\n- High CPS (connections per second) workloads\n- Observing CPU hotspotting on a single core during interrupt-heavy operations\n- Using interrupt-driven receive paths (epoll waiting on CQ completion channels)\n\n**When to Keep Disabled:**\n- Single-worker applications\n- Pure polling mode (applications never wait for CQ interrupts)\n- Simple deployments where interrupt distribution adds no benefit\n- Low connection rate workloads where interrupt overhead is negligible"
                        }
                    },
                    "additionalProperties": false
                }
            },
            "additionalProperties": false
        },
        "acceleration_control": {
            "type": "object",
            "description": "Socket acceleration control settings.",
            "properties": {
                "default_acceleration": {
                    "type": "boolean",
                    "default": true,
                    "title": "Enable acceleration by default for all sockets",
                    "description": "Maps to XLIO_OFFLOADED_SOCKETS environment variable.\nCreate all sockets as offloaded/not-offloaded by default.\nValue of true is for offloaded, false for not-offloaded."
                },
                "app_id": {
                    "type": "string",
                    "default": "XLIO_DEFAULT_APPLICATION_ID",
                    "title": "Application ID",
                    "description": "Maps to XLIO_APPLICATION_ID environment variable.\nSpecify a group of rules from libxlio.conf for XLIO to apply.\nExample: 'XLIO_APPLICATION_ID=iperf_server'.\nDefault value is \"XLIO_DEFAULT_APPLICATION_ID\" (match only the '*' group rule)"
                },
                "rules": {
                    "type": "array",
                    "default": [],
                    "title": "Acceleration control rules",
                    "description": "Maps to configuration in libxlio.conf file.\nRules defining transport protocol and offload settings for\nspecific applications or processes.",
                    "items": {
                        "type": "object",
                        "properties": {
                            "id": {
                                "type": "string",
                                "title": "Rule identifier",
                                "description": "Unique identifier for this transport control rule."
                            },
                            "name": {
                                "type": "string",
                                "title": "Application name",
                                "description": "Name of the application this rule applies to."
                            },
                            "actions": {
                                "type": "array",
                                "description": "Actions to apply for this rule",
                                "items": {
                                    "type": "string",
                                    "title": "Action directive",
                                    "description": "Directive that modifies transport layer behavior."
                                }
                            }
                        },
                        "additionalProperties": false
                    }
                }
            },
            "additionalProperties": false
        },
        "monitor": {
            "type": "object",
            "title": "Observability",
            "description": "Settings for logging, statistics, and monitoring functionality.",
            "properties": {
                "log": {
                    "type": "object",
                    "description": "Logging configuration.",
                    "properties": {
                        "level": {
                            "oneOf": [
                                {
                                    "type": "integer",
                                    "enum": [
                                        -2,
                                        -1,
                                        0,
                                        1,
                                        2,
                                        3,
                                        4,
                                        5,
                                        6,
                                        7,
                                        8
                                    ],
                                    "default": 3
                                },
                                {
                                    "type": "string",
                                    "enum": [
                                        "init",
                                        "none",
                                        "panic",
                                        "error",
                                        "warn",
                                        "info",
                                        "details",
                                        "debug",
                                        "fine",
                                        "finer",
                                        "all"
                                    ],
                                    "default": "info"
                                }
                            ],
                            "title": "Log level",
                            "description": "Maps to XLIO_TRACELEVEL environment variable.\nLogging level the library will be using.\n   - \"none\" or -2\n      Print no log at all\n   - \"panic\" or -1\n      Panic level logging, this would generally cause fatal behavior and an exception\n      will be thrown by the library. Typically, this is caused by memory\n      allocation problems. This level is rarely used.\n   - \"error\" or 0\n      Runtime ERRORs in the library.\n      Typically, these can provide insight for the developer of wrong internal\n      logic like: Errors from underlying OS or Infiniband verbs calls. internal\n      double mapping/unmapping of objects.\n   - \"warn\" or 2\n      Runtime warning that do not disrupt the workflow of the application but\n      might warn of a problem in the setup or the overall setup configuration.\n      Typically, these can be address resolution failure (due to wrong routing\n      setup configuration), corrupted ip packets in the receive path or\n      unsupported functions requested by the user application\n   - \"info\" or 3\n      General information passed to the user of the application. Bring up\n      configuration logging or some general info to help the user better\n      use the library\n   - \"details\" or 4\n      Complete XLIO configuration information.\n      Very high level insight of some of the critical decisions done in library.\n   - \"debug\" or 5\n      High level insight to the operations done in the library. All socket API calls\n      are logged and internal high level control channels log there activity.\n   - \"fine\" or 6\n      Low level run time logging of activity. This logging level includes basic\n      Tx and Rx logging in the fast path and it will lower application\n      performance.\n      It is recommended to use this level with monitor.log.file_path parameter.\n   - \"finer\" or 7\n      Very low level run time logging of activity!\n      This logging level will DRASTICALLY lower application performance.\n      It is recommended to use this level with monitor.log.file_path parameter.\n   - \"all\" or 8\n      today this level is identical to finer.\nExample: monitor.log.level=\"debug\""
                        },
                        "file_path": {
                            "type": "string",
                            "default": "",
                            "title": "Log file path",
                            "description": "Maps to XLIO_LOG_FILE environment variable.\nRedirect all logging to a specific user defined file.\nThis is very useful when raising the monitor.log.level.\nLibrary will replace a single '%d' appearing in the log file name\nwith the pid of the process loaded with XLIO.\nThis can help in running multiple instances of XLIO each with its own log file name.\nExample: \"/tmp/xlio.log\""
                        },
                        "details": {
                            "type": "integer",
                            "minimum": 0,
                            "maximum": 3,
                            "default": 0,
                            "title": "Log details level",
                            "description": "Maps to XLIO_LOG_DETAILS environment variable.\nAdd details on each log line:\n   - 0=Basic log line\n   - 1=ThreadId\n   - 2=ProcessId+ThreadId\n   - 3=Time + ProcessId + ThreadId [Time is in milli-seconds from start of process]."
                        },
                        "colors": {
                            "type": "boolean",
                            "default": true,
                            "title": "Colored log output",
                            "description": "Maps to XLIO_LOG_COLORS environment variable.\nUse color scheme when logging.\nRed for errors, purple for warnings and dim for low level debugs.\nmonitor.log.colors is automatically disabled when logging is directed\nto a non terminal device (e.g. monitor.log.file_path is configured)."
                        }
                    },
                    "additionalProperties": false
                },
                "stats": {
                    "type": "object",
                    "description": "Statistics collection settings.",
                    "properties": {
                        "file_path": {
                            "type": "string",
                            "default": "",
                            "title": "Statistics file path",
                            "description": "Maps to XLIO_STATS_FILE environment variable.\nRedirect socket statistics to a specific user defined file.\nLibrary will dump each socket statistics into a file when closing the socket.\nExample: \"/tmp/xlio_stats.log\""
                        },
                        "fd_num": {
                            "type": "integer",
                            "minimum": 0,
                            "maximum": 1024,
                            "default": 0,
                            "title": "Max tracked file descriptors",
                            "description": "Maps to XLIO_STATS_FD_NUM environment variable.\nMaximum number of sockets monitored by XLIO statistic mechanism.\nThis affects the number of sockets that xlio_stats and\nmonitor.stats.file_path can report simultaneously.\nxlio_stats tool is additionally limited by 1024 sockets."
                        },
                        "shmem_dir": {
                            "type": "string",
                            "default": "/tmp/xlio",
                            "title": "Shared memory directory",
                            "description": "Maps to XLIO_STATS_SHMEM_DIR environment variable.\nSet the directory path for the library to create the shared memory files for xlio_stats.\nNo files will be created when setting this value to empty string \"\"."
                        },
                        "cpu_usage": {
                            "type": "boolean",
                            "default": false,
                            "title": "Enable CPU usage statistics",
                            "description": "Maps to XLIO_CPU_USAGE_STATS environment variable.\nCalculate XLIO CPU usage during polling HW loops.\nThis information is available through XLIO stats utility."
                        }
                    },
                    "additionalProperties": false
                },
                "exit_report": {
                    "oneOf": [
                                {
                                    "type": "integer",
                                    "enum": [
                                        -1,
                                        0,
                                        1
                                    ],
                                    "default": -1
                                },
                                {
                                    "type": "string",
                                    "enum": [
                                        "auto",
                                        "disable",
                                        "enable"
                                    ],
                                    "default": "auto"
                                }
                    ],
                    "title": "Enable exit report",
                    "description": "Maps to XLIO_PRINT_REPORT environment variable.\nPrint a human readable report of resources usage at exit.\nThe report is printed during termination phase.\nTherefore, it can be missed if the process is killed with the SIGKILL signal.\nUse:\n   - \"auto\" or -1\n      Print report only if anomaly is detected on process exit.\n   - \"disable\" or 0\n      Never print report.\n   - \"enable\" or 1\n      Always print report."
                }
            },
            "additionalProperties": false
        },
        "profiles": {
            "type": "object",
            "description": "Predefined application profiles",
            "properties": {
                "spec": {
                    "oneOf": [
                        {
                            "type": "integer",
                            "enum": [
                                0,
                                1,
                                2,
                                3,
                                4,
                                5
                            ],
                            "default": 0
                        },
                        {
                            "type": "string",
                            "enum": [
                                "none",
                                "ultra_latency",
                                "latency",
                                "nginx",
                                "nginx_dpu",
                                "nvme_bf3"
                            ],
                            "default": "none"
                        }
                    ],
                    "title": "Application spec profile",
                    "description": "Maps to XLIO_SPEC environment variable.\nXLIO predefined specification profiles that optimize for different workload characteristics.\nEach profile pre-configures dozens of internal parameters for a specific use case.\n\n**IMPORTANT**: The integer values correspond to enum order, NOT latency ranking.\n\n═══════════════════════════════════════════════════════════════════════════════\n\"none\" or 0 - DEFAULT BALANCED CONFIGURATION\n═══════════════════════════════════════════════════════════════════════════════\nNo profile optimizations applied. Uses XLIO defaults which provide:\n- Moderate latency (100ms polling budget before sleep)\n- Good throughput (TSO auto-detect, GRO enabled with 32 streams)\n- Reasonable CPU usage (internal thread runs every 10ms)\n- Ring per thread allocation (good parallelism)\n\nBest for: General-purpose applications, initial testing, or when you want\nfull control over individual parameters.\n\n═══════════════════════════════════════════════════════════════════════════════\n\"ultra_latency\" or 1 - EXTREME LOW LATENCY (SINGLE-THREADED)\n═══════════════════════════════════════════════════════════════════════════════\nAggressive optimizations for absolute minimum latency at the cost of CPU and throughput.\n\n**Key Parameter Changes:**\n- rx_poll_num = -1 (INFINITE busy polling - never sleeps)\n- select_poll_num = -1 (INFINITE polling on select/poll/epoll)\n- select_poll_os_ratio = 0 (NEVER polls OS file descriptors)\n- rx_udp_poll_os_ratio = 0 (NEVER polls OS for UDP)\n- progress_engine_interval_msec = 0 (DISABLES internal thread entirely)\n- enable_tso = OFF (disables TCP Segmentation Offload)\n- gro_streams_max = 0 (DISABLES Generic Receive Offload)\n- tcp_nodelay = true (disables Nagle's algorithm)\n- tx_bufs_batch_udp = 1, tx_bufs_batch_tcp = 1 (no TX batching)\n- rx_bufs_batch = 4 (minimal RX batching)\n- tx_num_wr_to_signal = 4 (frequent TX completions = lower jitter)\n- cq_keep_qp_full = false (reduces completion overhead)\n- ring_dev_mem_tx = 16KB (uses on-device memory for TX)\n- memory_limit = 128MB (constrained memory footprint)\n- internal_thread_affinity = CPU 0 (pinned)\n\n**Performance Characteristics:**\n+ Lowest possible latency (sub-microsecond improvements)\n+ Lowest latency variance/jitter\n+ Immediate packet transmission (no batching delays)\n+ No context switches to internal thread\n- 100% CPU utilization (busy polling never yields)\n- Lower throughput (no TSO/GRO, small batches)\n- Single-threaded model assumption\n- Cannot handle non-offloaded sockets (OS polling disabled)\n- Reduced memory efficiency\n\n**When to Use:**\n- High-frequency trading (HFT) applications\n- Financial market data systems\n- Real-time control systems\n- Latency-critical request-response workloads\n- Applications that can dedicate CPU cores to networking\n\n**When NOT to Use:**\n- Applications using non-offloaded sockets (they will starve)\n- Throughput-sensitive workloads (bulk data transfer)\n- Memory-constrained environments needing >128MB buffers\n- Multi-threaded applications sharing sockets across threads\n\nExample: profiles.spec=ultra_latency\n\n═══════════════════════════════════════════════════════════════════════════════\n\"latency\" or 2 - BALANCED LOW LATENCY\n═══════════════════════════════════════════════════════════════════════════════\nLow latency optimizations while maintaining system compatibility.\n\n**Key Parameter Changes (vs ultra_latency):**\n- rx_poll_num = -1 (still infinite polling)\n- select_poll_num = -1 (still infinite polling)\n- select_poll_os_ratio = 100 (STILL POLLS OS every 100 iterations)\n- progress_engine_interval_msec = 100 (internal thread runs every 100ms)\n- Other settings same as ultra_latency\n\n**Key Differences from ultra_latency:**\n- Maintains OS file descriptor polling (non-offloaded sockets work)\n- Keeps internal thread for TCP state management (slower but safer)\n- No memory_limit override (uses default, typically larger)\n- Better compatibility with mixed socket workloads\n\n**Performance Characteristics:**\n+ Very low latency (slightly higher than ultra_latency)\n+ Non-offloaded sockets still functional\n+ TCP timers and state managed by internal thread\n+ Better system integration\n- Still high CPU usage (busy polling)\n- Lower throughput than default (no TSO/GRO)\n\n**When to Use:**\n- Latency-sensitive apps that also use non-offloaded sockets\n- Applications that cannot dedicate 100% CPU to networking\n- Mixed workloads with some latency-critical paths\n- When ultra_latency causes socket starvation issues\n\nExample: profiles.spec=latency\n\n═══════════════════════════════════════════════════════════════════════════════\n\"nginx\" or 3 - HTTP PROXY / WEB SERVER THROUGHPUT\n═══════════════════════════════════════════════════════════════════════════════\nOptimized for nginx and similar HTTP proxy/reverse proxy workloads.\nAutomatically enabled when applications.nginx.workers_num > 0.\n\n**Key Parameter Changes:**\n- ring_allocation_logic = PER_INTERFACE (shared rings across workers)\n- enable_tso = ON (TCP Segmentation Offload ENABLED)\n- tcp_send_buffer_size = 2MB (large TCP send buffers)\n- cq_poll_batch_max = 128 (large completion batches)\n- tcp_push_flag = false (better TCP batching)\n- tcp_3t_rules = true (3-tuple steering for efficiency)\n- select_poll_num = 0 (poll once, then sleep)\n- select_skip_os_fd_check = 1000 (rarely check OS)\n- timer_resolution_msec = 32, tcp_timer_resolution_msec = 256 (slower timers)\n- progress_engine_interval_msec = 0 (disabled)\n- rx_cq_wait_ctrl = true (efficient epoll integration)\n- rx_poll_on_tx_tcp = true (poll RX during TX for TCP ACKs)\n- distribute_cq_interrupts = true (spread interrupts across workers)\n- memory_limit = ~3-4GB per worker (scaled by worker count)\n\n**Performance Characteristics:**\n+ Maximum HTTP throughput (connections/sec, requests/sec)\n+ Efficient resource sharing across nginx workers\n+ Large TCP buffers for high bandwidth\n+ TSO reduces CPU overhead for large responses\n+ Optimized for epoll-based event loops\n+ Lower CPU utilization (sleeps when idle)\n- Higher latency than latency profiles\n- Not suitable for latency-critical workloads\n- Requires nginx worker configuration\n\n**When to Use:**\n- nginx reverse proxy / load balancer\n- HAProxy and similar HTTP proxies\n- Web servers serving large responses\n- CDN edge servers\n- Any epoll-based HTTP server\n\n**REQUIRED**: Set applications.nginx.workers_num=<N> to enable.\nExample: profiles.spec=nginx applications.nginx.workers_num=4\n\n═══════════════════════════════════════════════════════════════════════════════\n\"nginx_dpu\" or 4 - NGINX ON NVIDIA DPU (BlueField)\n═══════════════════════════════════════════════════════════════════════════════\nSame as nginx profile but tuned for DPU's constrained resources.\n\n**Key Differences from nginx:**\n- memory_limit = 512MB-1GB per worker (reduced for DPU)\n- buffer_batching_mode = NONE (simpler buffer management)\n- No rx_poll_on_tx_tcp (reduced complexity)\n\n**When to Use:**\n- nginx running inside NVIDIA BlueField DPU\n- DPU-offloaded network proxy scenarios\n\nExample: profiles.spec=nginx_dpu applications.nginx.workers_num=4\n\n═══════════════════════════════════════════════════════════════════════════════\n\"nvme_bf3\" or 5 - NVMe-oF / SPDK ON BLUEFIELD-3\n═══════════════════════════════════════════════════════════════════════════════\nOptimized for storage workloads (NVMe over Fabrics, SPDK) on BlueField-3 DPU.\n\n**Key Parameter Changes:**\n- strq_stride_num_per_rwqe = 8192 (large strides for big I/O)\n- enable_lro = ON (Large Receive Offload ENABLED)\n- enable_tso = ON (TSO ENABLED)\n- tx_num_wr = 1024, tx_num_wr_to_signal = 128 (large TX batches)\n- rx_num_wr = 32 (smaller RX queue)\n- tcp_abort_on_close = true (RST on close for fast cleanup)\n- handle_fork = false (no fork support needed)\n- cq_aim_interval_msec = DISABLED (no adaptive moderation)\n- progress_engine_interval_msec = 0 (disabled)\n- memory_limit = 256MB, memory_limit_user = 2GB\n- gro_streams_max = 0 (GRO disabled, using LRO instead)\n- ring_dev_mem_tx = 1KB\n\n**Performance Characteristics:**\n+ Optimized for large sequential I/O patterns\n+ LRO aggregates incoming storage data efficiently\n+ Large TX batches for high IOPS\n+ Fast connection cleanup (RST on close)\n+ Tuned for BlueField-3 hardware capabilities\n- Not suitable for general-purpose networking\n- Requires BlueField-3 DPU\n\n**When to Use:**\n- SPDK NVMe-oF target on BlueField-3\n- Storage disaggregation solutions\n- High-performance storage networking\n\nExample: profiles.spec=nvme_bf3\n\n═══════════════════════════════════════════════════════════════════════════════\nPROFILE SELECTION GUIDE\n═══════════════════════════════════════════════════════════════════════════════\n\n| Workload Type              | Recommended Profile | CPU Usage | Latency | Throughput |\n|---------------------------|--------------------:|----------:|--------:|-----------:|\n| HFT / Market Data          | ultra_latency (1)   | 100%      | Lowest  | Lower      |\n| Real-time Control          | ultra_latency (1)   | 100%      | Lowest  | Lower      |\n| Mixed Latency-Sensitive    | latency (2)         | High      | Low     | Moderate   |\n| HTTP Proxy / Load Balancer | nginx (3)           | Moderate  | Higher  | Highest    |\n| nginx on BlueField DPU     | nginx_dpu (4)       | Moderate  | Higher  | High       |\n| NVMe-oF / SPDK on BF3      | nvme_bf3 (5)        | Moderate  | Moderate| High       |\n| General Purpose            | none (0)            | Moderate  | Moderate| Good       |\n\n**Note**: Individual parameters can still be overridden after profile selection.\nThe profile sets initial values which can be fine-tuned as needed."
                }
            },
            "additionalProperties": false
        }
    }
}
